{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c2f14a-8433-47ef-a093-8ea05a722d26",
   "metadata": {},
   "source": [
    "**Company** : <br>\n",
    "Tech Firm\n",
    "\n",
    "**Notebook Function** : <br>\n",
    "    This notebook generates the local clustering measures\n",
    "    \n",
    "**Output File(s)** : <br>\n",
    "    tech_network_embed.csv - The final output file containing the network measure\n",
    "\n",
    "**Author(s)** : <br>\n",
    "Lara Yang, Sarayu Anshuman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fab87-eb0b-4466-87db-19c830787f81",
   "metadata": {},
   "source": [
    "Install packages and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b6523-a167-42fe-86c6-fa11ec2b3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cdlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da528c15-99cb-49b3-a2d3-53ad738e1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install igraph leidenalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf0b0b-6973-4883-b2ad-ed1962ff520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mittens import Mittens\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import ujson as json\n",
    "import re\n",
    "from gensim.matutils import cossim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from statistics import mean \n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from datetime import timedelta\n",
    "import multiprocessing\n",
    "import random\n",
    "from ast import literal_eval\n",
    "import networkx.algorithms.community as nx_comm\n",
    "from cdlib import algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a213646-e235-4319-9227-94b43638d6ba",
   "metadata": {},
   "source": [
    "Run helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34beb8f4-1fd7-4abd-9af1-a92b1b9b46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "########### Helper Functions for Generating Mittens Embeddings ##########\n",
    "#########################################################################\n",
    "def _window_based_iterator(toks, window_size, weighting_function):\n",
    "    for i, w in enumerate(toks):\n",
    "        yield w, w, 1\n",
    "        left = max([0, i-window_size])\n",
    "        for x in range(left, i):\n",
    "            yield w, toks[x],weighting_function(x)\n",
    "        right = min([i+1+window_size, len(toks)])\n",
    "        for x in range(i+1, right):\n",
    "            yield w, toks[x], weighting_function(x)\n",
    "    return\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    \"\"\"\n",
    "    Reads word vectors into a dictionary\n",
    "    Parameters\n",
    "    ----------\n",
    "    glove_filename : str\n",
    "        Name of file that contains vectors\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        A dictionary matching words to their vectors\n",
    "    \"\"\"\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        data = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader}\n",
    "    return data\n",
    "\n",
    "# Inspired by original build_weighted_matrix in utils.py in the Mittens paper source codebase\n",
    "def build_weighted_matrix(emails,\n",
    "        mincount=300, vocab_size=None, window_size=10,\n",
    "        weighting_function=lambda x: 1 / (x + 1)):\n",
    "    \"\"\"\n",
    "    Builds a count matrix based on a co-occurrence window of\n",
    "    `window_size` elements before and `window_size` elements after the\n",
    "    focal word, where the counts are weighted based on proximity to the\n",
    "    focal word.\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dicts\n",
    "        Emails converted from JSON formats\n",
    "    mincount : int\n",
    "        Only words with at least this many tokens will be included.\n",
    "    vocab_size : int or None\n",
    "        If this is an int above 0, then, the top `vocab_size` words\n",
    "        by frequency are included in the matrix, and `mincount`\n",
    "        is ignored.\n",
    "    window_size : int\n",
    "        Size of the window before and after. (So the total window size\n",
    "        is 2 times this value, with the focal word at the center.)\n",
    "    weighting_function : function from ints to floats\n",
    "        How to weight counts based on distance. The default is 1/d\n",
    "        where d is the distance in words.\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Cooccurence matrix guaranteed to be symmetric because of the way the counts are collected.\n",
    "    \"\"\"\n",
    "    wc = defaultdict(int)\n",
    "    corpus = read_corpus(emails, sentence_delim=False)\n",
    "    #---print(\"corpus------\")\n",
    "    #---print(corpus)\n",
    "    if corpus is None:\n",
    "        print(\"These emails are empty\\t{}.\\n\".format(str(emails)))\n",
    "        return pd.DataFrame()\n",
    "    for toks in corpus:\n",
    "        for tok in toks:\n",
    "            wc[tok] += 1\n",
    "    #---print(\"wc------\")\n",
    "    #---print(wc)\n",
    "    if vocab_size:\n",
    "        srt = sorted(wc.items(), key=itemgetter(1), reverse=True)\n",
    "        vocab_set = {w for w, c in srt[: vocab_size]}\n",
    "        #---print('using vocabsize')\n",
    "    else:\n",
    "        vocab_set = {w for w, c in wc.items() if c >= mincount} #this is being printed\n",
    "        #---print('using mincount')\n",
    "    vocab = sorted(vocab_set)\n",
    "    n_words = len(vocab)\n",
    "    #---print(f\"n_words: {n_words}\")\n",
    "    # Weighted counts:\n",
    "    counts = defaultdict(float)\n",
    "    for toks in corpus:\n",
    "        window_iter = _window_based_iterator(toks, window_size, weighting_function)\n",
    "        for w, w_c, val in window_iter:\n",
    "            if w in vocab_set and w_c in vocab_set:\n",
    "                counts[(w, w_c)] += val\n",
    "    X = np.zeros((n_words, n_words))\n",
    "    for i, w1 in enumerate(vocab):\n",
    "        for j, w2 in enumerate(vocab):\n",
    "            X[i, j] = counts[(w1, w2)]\n",
    "    X = pd.DataFrame(X, columns=vocab, index=pd.Index(vocab))\n",
    "    return X\n",
    "\n",
    "def read_corpus(emails, sentence_delim=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dict\n",
    "        A list of emails converted from JSON formats\n",
    "    sentence_delim : bool, optional\n",
    "        If true, co-occurrences across sentence boundaries are ignored.\n",
    "    Returns\n",
    "    -------\n",
    "    list of list of str\n",
    "        Corpus converted from emails.\n",
    "        If sentence_delim is false, returns a list of emails, which are represented as lists of tokens\n",
    "        If sentence_delim is true, returns a list of sentences, which are represented as lists of tokens\n",
    "    \"\"\"\n",
    "    if not sentence_delim:\n",
    "        return [email['body'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails]\n",
    "    else:\n",
    "        return [sent.strip().split() for email in emails for line in email['body'].split('\\n') for sent in line.split('SENT_END') if len(sent) > 0]\n",
    "\n",
    "def output_embeddings(mittens_df, filename, compress=False):\n",
    "    if compress:\n",
    "        mittens_df.to_csv(filename + '.gz', quoting=csv.QUOTE_NONE, header=False, sep=\" \", compression='gzip')\n",
    "    else:\n",
    "        mittens_df.to_csv(filename, quoting=csv.QUOTE_NONE, header=False, sep=\" \")\n",
    "    return\n",
    "\n",
    "def isnull_wrapper(x):\n",
    "    r = pd.isnull(x)\n",
    "    if type(r) == bool:\n",
    "        return r\n",
    "    return r.any()\n",
    "\n",
    "def cossim_with_none(vec1, vec2, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Auxiliary function that calls cossim function to test if vectors are None to prevent erroring out.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : list of (int, float), gensim sparse vector format\n",
    "    vec2 : list of (int, float), gensim sparse vector format\n",
    "    format : str, optional\n",
    "        Either sparse or dense. If sparse, vec1 and vec2 are in gensim sparse vector format; use cossim function from gensim.\n",
    "        Otherwise, vec1 and vec2 are numpy arrays and cosine similarity is hand calculated\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between vec1 and vec2\n",
    "    \"\"\"\n",
    "    if not (isnull_wrapper(vec1) or isnull_wrapper(vec2)):\n",
    "        if vec_format == 'sparse':\n",
    "            return cossim(vec1, vec2)\n",
    "        elif vec_format == 'dense':\n",
    "            if len(vec1) == 0 or len(vec2) == 0:\n",
    "                return None\n",
    "            return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        else:\n",
    "            raise ValueError()\n",
    "    return None\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with JSON Emails #############\n",
    "#########################################################################\n",
    "\n",
    "def slice_user_corpus(emails, train_mode):\n",
    "    \"\"\"\n",
    "    Similar to slice_user_corpus in the Coco, modified to work with tech firm data structure\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dict\n",
    "        A list of emails converted from JSON formats to dictionaries \n",
    "    train_mode : str\n",
    "        One of 'annual', 'quarterly', 'all'\n",
    "        Indicates how to chunk up emails - into quarters, years, or both\n",
    "    Returns\n",
    "    -------\n",
    "    timekey2emails : dict\n",
    "        Matches quarters or years to respective emails\n",
    "    \"\"\"\n",
    "    timekey2emails = defaultdict(list)\n",
    "    for email in emails:\n",
    "        if train_mode == 'annual':\n",
    "            timekey2emails[email['year']].append(email)\n",
    "        elif train_mode == 'quarterly':\n",
    "            timekey2emails[email['quarter']].append(email)\n",
    "        elif train_mode == 'halfyearly':\n",
    "            timekey2emails[email['halfyear']].append(email)\n",
    "        elif train_mode == 'thirdyearly': #1/3 or a year or 4 months\n",
    "            timekey2emails[email['thirdyear']].append(email)\n",
    "        elif train_mode == 'all':\n",
    "            timekey2emails[email['year']].append(email)\n",
    "            timekey2emails[email['quarter']].append(email)\n",
    "    return timekey2emails\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with Date Objects ############\n",
    "#########################################################################\n",
    "def str_to_datetime(date):\n",
    "    if date is None:\n",
    "        return None\n",
    "    date = re.sub(r\"(\\([A-Z]{3,}\\))\", \"\", date)\n",
    "    date = date.strip()\n",
    "    dt = None\n",
    "    for fmt in ('%a, %d %b %Y %H:%M:%S %z', '%d %b %Y %H:%M:%S %z', '%a %d, %b %Y %H:%M:%S %z', '%a, %d %b %Y %H:%M:%S %Z', '%a, %d %b %Y %H:%M:%S (%Z)', '%Y-%m-%d %H:%M:%S%z'):\n",
    "        try:\n",
    "            dt = datetime.strptime(date, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return dt\n",
    "\n",
    "def to_quarter(date, format):\n",
    "    \"\"\"\n",
    "    Returns quarter of date as str using date formats used in tech firm.\n",
    "    Support for string format is provided, but if both to_quarter and to_year are needed,\n",
    "    it is computationally more efficient to first convert string to datetime and then call\n",
    "    both to_quarter and to_year on the datetime object to avoid duplicated datetime conversion.\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        return None\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        dt = str_to_datetime(date)\n",
    "        if dt is None:\n",
    "            return None\n",
    "        year = dt.year\n",
    "        month = dt.month\n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    quarter = ((int(month)-1) // 3) + 1\n",
    "    timekey = str(year) + 'Q' + str(quarter)\n",
    "    return timekey\n",
    "\n",
    "def to_year(date, date_type):\n",
    "    \"\"\"\n",
    "    Returns year of date as str using date formats used in tech firm.\n",
    "    Support for string format is provided, but if both to_quarter and to_year are needed,\n",
    "    it is computationally more efficient to first convert string to datetime and then call\n",
    "    both to_quarter and to_year on the datetime object to avoid duplicated datetime conversion.\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        return None\n",
    "    if date_type == 'str':\n",
    "        date = re.sub(r\"\\([A-Z]{3,}\\)\", \"\", date)\n",
    "        date = date.strip()\n",
    "        dt = None\n",
    "        for fmt in ('%a, %d %b %Y %H:%M:%S %z', '%d %b %Y %H:%M:%S %z', '%a %d, %b %Y %H:%M:%S %z', '%a, %d %b %Y %H:%M:%S %Z'):\n",
    "            try:\n",
    "                dt = datetime.strptime(date, fmt)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if dt is None:\n",
    "            return None\n",
    "        return str(dt.year)\n",
    "    elif date_type == 'datetime':\n",
    "        return str(date.year)\n",
    "\n",
    "def to_halfyear(date, format):\n",
    "    \"\"\"\n",
    "    Returns quarter of date as str using date formats used in tech firm.\n",
    "    Support for string format is provided, but if both to_quarter and to_year are needed,\n",
    "    it is computationally more efficient to first convert string to datetime and then call\n",
    "    both to_quarter and to_year on the datetime object to avoid duplicated datetime conversion.\n",
    "    \"\"\"\n",
    "    if date is None:\n",
    "        return None\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        dt = str_to_datetime(date)\n",
    "        if dt is None:\n",
    "            return None\n",
    "        year = dt.year\n",
    "        month = dt.month\n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    halfyear = ((int(month)-1) // 6) + 1\n",
    "    timekey = str(year) + 'HY' + str(halfyear)\n",
    "    return timekey\n",
    "\n",
    "def to_thirdyear(date, format):\n",
    "    \"\"\"\n",
    "    Return third of year of date in string\n",
    "    \"\"\"\n",
    "    year, month = 0, 0\n",
    "    if date is None:\n",
    "        return None\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]\n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    thirdyear = ((int(month)-1) // 4) + 1\n",
    "    timekey = str(year) + 'TH' + str(thirdyear)\n",
    "    return timekey\n",
    "\n",
    "def word_similarity(model, w1, w2):\n",
    "    \"\"\"\n",
    "    This is an auxilary function that allows for comparing one word to another word or multiple words\n",
    "    If w1 and w2 are both single words, n_similarity returns their cosine similarity which is the same as \n",
    "    simply calling similarity(w1, w2)\n",
    "    If w1 or w2 is a set of words, n_similarity essentially takes the mean of the set of words and then computes\n",
    "    the cosine similarity between that vector mean and the other vector. This functionality is both reflected\n",
    "    in its source code and has been verified manually.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : KeyedVectors\n",
    "        The model that contains all the words and vectors\n",
    "    w1 : str or list\n",
    "        The first word or word list to be compared\n",
    "    w2 : str or list\n",
    "        The second word or word list to be compared\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between w1 and w2\n",
    "    \"\"\"\n",
    "    if not isinstance(w1, list):\n",
    "        w1 = [w1]\n",
    "    if not isinstance(w2, list):\n",
    "        w2 = [w2]\n",
    "    w1 = [w for w in w1 if w in model.key_to_index]\n",
    "    w2 = [w for w in w2 if w in model.key_to_index]\n",
    "    if len(w1) == 0 or len(w2) == 0:\n",
    "        return None\n",
    "    return model.n_similarity(w1, w2)\n",
    "\n",
    "#########################################################################\n",
    "############## Helper Functions for Working with Dataframes #############\n",
    "#########################################################################\n",
    "def dict_to_df(index2rows, cols, index_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    index2rows : dict\n",
    "        Dictionary mapping index to rows to be coverted\n",
    "    cols : list\n",
    "        List of column names of type str\n",
    "    index : list\n",
    "        List of index names\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Constructed dataframe\n",
    "    \"\"\"\n",
    "    if index2rows is None or len(index2rows) == 0:\n",
    "        return None\n",
    "    if len(index_name) == 1:\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df.index.name = index_name[0]\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df = pd.DataFrame(df, pd.MultiIndex.from_tuples(df.index, names=index_name))\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "def get_recipients(msg):\n",
    "    \"\"\"\n",
    "    Return a set of recipients of the current message.\n",
    "    self is removed from list of recipients if in recipients\n",
    "    Tech firm's from fields are all lists based on visual inspection\n",
    "    but we check for the type of the field just in case\n",
    "    \"\"\"\n",
    "    sender = msg['from'][0] if type(msg['from']) == list else msg['from']\n",
    "    return set(msg.get('to', []) + msg.get('cc', []) + msg.get('bcc', [])) - set([sender])\n",
    "\n",
    "def is_internal_msg(msg):\n",
    "    recipients = get_recipients(msg)\n",
    "    internal = True\n",
    "    for r in recipients:\n",
    "        if not re.match(r'\\d+', r):\n",
    "            internal = False\n",
    "            break\n",
    "    return internal\n",
    "    \n",
    "def extract_variables_from_file(file):\n",
    "    \"\"\" \n",
    "    Extract relevant information from name of embedding file, with format: {}(_{})?.txt\n",
    "    \"\"\"\n",
    "    file_chunks = file[0:-4].split('_')\n",
    "    usr = file_chunks[0]\n",
    "    time_key = file_chunks[1] if len(file_chunks) == 2 else None\n",
    "    return (usr, time_key)\n",
    "\n",
    "def month2timekey(month, time_key):\n",
    "    \"\"\"\n",
    "    Converts month numbers to actual year or quarter\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    if time_key == 'year':\n",
    "        year = 2006 + month // 12 \n",
    "        if month % 12 > 2:\n",
    "            year += 1\n",
    "        result = str(year)\n",
    "    elif time_key == 'quarter':\n",
    "        year = 2006 + month // 12 \n",
    "        remainder = month % 12\n",
    "        if remainder > 2:\n",
    "            year += 1\n",
    "        quarter = ''\n",
    "        if remainder <=2:\n",
    "            quarter = 'Q4'\n",
    "        elif remainder <= 5:\n",
    "            quarter = 'Q1'\n",
    "        elif remainder <= 8:\n",
    "            quarter = 'Q2'\n",
    "        else:\n",
    "            quarter = 'Q3'\n",
    "        result = str(year)+quarter\n",
    "    return result\n",
    "\n",
    "def extract_hr_df(hr_file, time_key=None):\n",
    "    \"\"\"\n",
    "    Extract and preprocess tech HR data\n",
    "    \"\"\"\n",
    "    hr = pd.read_csv(hr_file, index_col=0)\n",
    "    # spell is a counter of the number of observations, not tenure\n",
    "    hr['hire_month'] = hr.apply(lambda row : row['month'] if row['spell'] == 1 else np.nan, axis=1)\n",
    "    hr['hire_month'] = hr['hire_month'].fillna(method='ffill')\n",
    "    hr['tenure_months'] = hr.apply(lambda row : (row['month'] - row['hire_month'])+1, axis=1)\n",
    "    hr['tenure_days'] = hr.apply(lambda row: row['tenure_months'] * 365/12, axis=1)\n",
    "    hr['employeeid'] = hr['employeeid'].astype(str)\n",
    "    hr['bonus_eligible'] = hr['bonus_eligible'].astype(int)\n",
    "    if not time_key:\n",
    "        hr.drop_duplicates(subset='employeeid', keep='last', inplace=True)\n",
    "        hr = (hr[['employeeid', 'hire_month', 'sales', 'marketing', 'tech', 'vol_exit_event', 'invol_exit_event', 'manager', 'female', 'fav_rating', 'bonus', 'bonus_eligible', 'age', 'age2', 'cumbonus', 'tenure_months', 'tenure_days']])\n",
    "        hr.rename(columns={\"employeeid\": \"anon_id\"}, inplace=True)\n",
    "        hr.set_index('anon_id', inplace=True)\n",
    "    else:\n",
    "        hr[time_key] = hr['month'].apply(lambda row : month2timekey(row, time_key))\n",
    "        hr['bonus'] = hr.groupby(['employeeid', time_key])['bonus'].transform('sum')\n",
    "        hr['vol_exit_event'] = hr.groupby(['employeeid', time_key])['vol_exit_event'].transform('max')\n",
    "        hr['invol_exit_event'] = hr.groupby(['employeeid', time_key])['invol_exit_event'].transform('max')\n",
    "        hr['manager'] = hr.groupby(['employeeid', time_key])['manager'].transform('max')\n",
    "        hr['fav_rating'] = hr.groupby(['employeeid', time_key])['fav_rating'].transform('max')\n",
    "        hr['bonus_eligible'] = hr.groupby(['employeeid', time_key])['bonus_eligible'].transform('max')\n",
    "        hr.drop_duplicates(subset=['employeeid', time_key], keep='last', inplace=True)\n",
    "        hr = (hr[['employeeid', time_key, 'hire_month', 'sales', 'marketing', 'tech', 'vol_exit_event', 'invol_exit_event', 'manager', 'female', 'fav_rating', 'bonus', 'bonus_eligible', 'age', 'age2', 'cumbonus', 'tenure_months', 'tenure_days']])\n",
    "        hr.rename(columns={\"employeeid\": \"anon_id\"}, inplace=True)\n",
    "        hr.set_index(['anon_id', time_key], inplace=True)\n",
    "    return hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc0b147-614c-41bd-9883-7fda4028f3e1",
   "metadata": {},
   "source": [
    "Set the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f85156-97ad-4e8c-85e3-002f199364bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381e79b-4982-44b5-9604-a42e4e091a83",
   "metadata": {},
   "source": [
    "Set the input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa2269-85ac-4e44-a526-002eab8284fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 9\n",
    "build_threshold_network = False\n",
    "weighted_mode = 'unweighted'\n",
    "quarter_colname = 'quarter'\n",
    "home_dir = current_dir\n",
    "corpus_dir = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/tech/training/email_data_v2\"\n",
    "output_dir = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/downsampled_network_files/tech\"\n",
    "test_dir = os.path.join(home_dir, \"idtf_output_data_test\")\n",
    "hr_file = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/tech/cossim/tech_ph2_data_for_analysis.csv\"\n",
    "\n",
    "print(corpus_dir)\n",
    "print(output_dir)\n",
    "print(hr_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9231d4-b47b-43ed-8e61-ddf72eadc6fa",
   "metadata": {},
   "source": [
    "Run functions to generate the local clustering measure, along with other variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432e5a1-9ac2-4a38-8124-236400c9c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarterly_edges(test_mode):\n",
    "    \"\"\"\n",
    "    Reads all network connections from emails. Emails processed in this function are internal emails only, as mittens1_generate_corpus.py\n",
    "    have filtered out all emails addressed to an external party.\n",
    "    Parameter\n",
    "    ---------\n",
    "    test_mode : bool\n",
    "        If true, restrict nuber of emails read\n",
    "    Returns\n",
    "    -------\n",
    "    quarterly_edges : dict\n",
    "        A dictionary mapping quarters to edges, where quarters are represented by strings and each edge is represented as a 2-tuple (frm, to)\n",
    "    \"\"\"\n",
    "    usr_files = os.listdir(corpus_dir)\n",
    "    # Testing two percent of all users\n",
    "    if test_mode: usr_files = [usr_files[random.randint(0, len(usr_files)-1)] for _ in range(len(usr_files)//10)]\n",
    "    quarterly_edges = defaultdict(list)\n",
    "    \n",
    "    for filename in usr_files:\n",
    "        with open(os.path.join(corpus_dir, filename), encoding='utf-8') as f:\n",
    "            emails = json.load(f)\n",
    "            for e in emails:\n",
    "                if is_internal_msg(e):\n",
    "                    frm = e['from']\n",
    "                    if isinstance(frm, list): frm = frm[0]\n",
    "                    quarter = e['quarter']\n",
    "                    if quarter is None:\n",
    "                        continue\n",
    "                    curr_edges = []\n",
    "                    for r in get_recipients(e):\n",
    "                        assert frm != r, \"Creating self-loop!\"\n",
    "                        curr_edges.append((frm, r))\n",
    "                    quarterly_edges[quarter] += curr_edges\n",
    "    return quarterly_edges\n",
    "\n",
    "def generate_network_embeddedness(G_directed, weighted, edges2weights):\n",
    "    \"\"\"\n",
    "    Generate embeddedness of one's network\n",
    "    Parameter\n",
    "    ---------\n",
    "    G_directed : NetworkX graph\n",
    "    weighted : bool\n",
    "        Whether to engage in weighted computations\n",
    "    edges2weights : dict of {tuple : int}\n",
    "        Maps directed edges to weights\n",
    "    Returns\n",
    "    -------\n",
    "        Three different measures of network embeddedness at ego, alter, and alter's alter levels.\n",
    "    \"\"\"\n",
    "    node2embed_ego, node2embed_alter, node2embed_alter2 = defaultdict(lambda : None), defaultdict(lambda : None), defaultdict(lambda : None)\n",
    "    for u in G_directed:\n",
    "        local_network = set(G_directed.neighbors(u))\n",
    "        within_cluster, without_cluster = 0, 0\n",
    "        # As G is a directed network, u's network only includes those who u has sent an email to\n",
    "        for v in G_directed.neighbors(u):\n",
    "            for w in G_directed.neighbors(v):\n",
    "                add = edges2weights[v, w] if weighted else 1\n",
    "                if w in local_network:\n",
    "                    within_cluster += add\n",
    "                else:\n",
    "                    without_cluster += add\n",
    "\n",
    "        if (without_cluster+within_cluster) == 0:\n",
    "            node2embed_ego[u] = np.nan\n",
    "        else:\n",
    "            # if 1, no within_cluster ties, if -1, all within-cluster ties\n",
    "            node2embed_ego[u] = (without_cluster-within_cluster)/(without_cluster+within_cluster)\n",
    "\n",
    "    # alter level measure\n",
    "    for u in G_directed:\n",
    "        local_network = set(G_directed.neighbors(u))\n",
    "        v2ei, uv2weight = {}, {}\n",
    "        # As G is a directed network, u's network only includes those who u has sent an email to\n",
    "        for v in G_directed.neighbors(u):\n",
    "            uv2weight[v] = edges2weights[u, v]\n",
    "            # skip any alter who doesn't have an alter\n",
    "            if len(list(G_directed.neighbors(v))) == 0:\n",
    "                continue\n",
    "            within_cluster, without_cluster = 0, 0    \n",
    "            for w in G_directed.neighbors(v):\n",
    "                add = edges2weights[v, w] if weighted else 1\n",
    "                if w in local_network:\n",
    "                    within_cluster += add\n",
    "                else:\n",
    "                    without_cluster += add\n",
    "            v2ei[v] = (without_cluster-within_cluster)/(without_cluster+within_cluster)\n",
    "        \n",
    "        if len(v2ei) == 0:\n",
    "            node2embed_alter[u] = np.nan\n",
    "        else:\n",
    "            u_ei = 0\n",
    "            if weighted:\n",
    "                u_weight = sum(uv2weight.values())\n",
    "                uv2prop = {v : weight / u_weight for v, weight in uv2weight.items()}\n",
    "                for v, ei in v2ei.items():\n",
    "                    u_ei += (uv2prop[v] * ei)\n",
    "            else:\n",
    "                u_ei = sum(v2ei.values()) / len(v2ei)\n",
    "            node2embed_alter[u] = u_ei\n",
    "\n",
    "        \n",
    "    # alter alter level measure\n",
    "    for u in G_directed:\n",
    "        local_network = set(G_directed.neighbors(u))\n",
    "        v2ei, uv2weight = {}, {}\n",
    "        # As G is a directed network, u's network only includes those who u has sent an email to\n",
    "        for v in G_directed.neighbors(u):\n",
    "            uv2weight[v] = edges2weights[u, v]\n",
    "            # skip any alter who doesn't have an alter\n",
    "            if len(list(G_directed.neighbors(v))) == 0:\n",
    "                continue\n",
    "            w2ei, vw2weight = {}, {}\n",
    "            for w in G_directed.neighbors(v):\n",
    "                vw2weight[w] = edges2weights[v, w]\n",
    "                if len(list(G_directed.neighbors(w))) == 0:\n",
    "                    continue\n",
    "                within_cluster, without_cluster = 0, 0\n",
    "                for z in G_directed.neighbors(w):\n",
    "                    add = edges2weights[w, z] if weighted else 1\n",
    "                    if z in local_network:\n",
    "                        within_cluster += add\n",
    "                    else:\n",
    "                        without_cluster += add\n",
    "                w2ei[w] = (without_cluster-within_cluster)/(without_cluster+within_cluster)\n",
    "            \n",
    "            if len(w2ei) > 0:\n",
    "                v_ei = 0\n",
    "                if weighted:\n",
    "                    v_weight = sum(vw2weight.values())\n",
    "                    vw2prop = {w : weight/v_weight for w, weight in vw2weight.items()}\n",
    "                    for w, ei in w2ei.items():\n",
    "                        v_ei += (vw2prop[w] * ei)\n",
    "                else:\n",
    "                    v_ei = sum(w2ei.values()) / len(w2ei)\n",
    "                v2ei[v] = v_ei\n",
    "        \n",
    "        if len(v2ei) == 0:\n",
    "            node2embed_alter2[u] = np.nan\n",
    "        else:\n",
    "            u_ei = 0\n",
    "            if weighted:\n",
    "                u_weight = sum(uv2weight.values())\n",
    "                uv2prop = {u : weight / u_weight for u, weight in uv2weight.items()}\n",
    "                for v, ei in v2ei.items():\n",
    "                    u_ei += (uv2prop[v] * ei)\n",
    "            else:\n",
    "                u_ei = sum(v2ei.values()) / len(v2ei)\n",
    "            node2embed_alter2[u] = u_ei\n",
    "\n",
    "    return [node2embed_ego, node2embed_alter, node2embed_alter2]\n",
    "\n",
    "def generate_community_ei(G_directed, node2community, weighted, edges2weights):\n",
    "    \"\"\"\n",
    "    Calculate EI index based on community structure.\n",
    "    Parameter\n",
    "    ---------\n",
    "    G_directed : NetworkX graph\n",
    "    node2community : dict of {str : int}\n",
    "        A dictionary mapping user IDs to integers that represent distinct communities\n",
    "    weighted : bool\n",
    "        Whether to engage in weighted computations\n",
    "    edges2weights : dict of {tuple : int}\n",
    "        Maps directed edges to weights\n",
    "    Returns\n",
    "    -------\n",
    "        Six different measures of network embeddedness at ego, alter, and alter's alter levels, with optional\n",
    "        filtering.\n",
    "    \"\"\"\n",
    "    node2embed_ego, node2embed_ego_filter, node2embed_alter, node2embed_alter_filter, node2embed_alter2, node2embed_alter2_filter = [defaultdict(lambda : None) for _ in range(6)]\n",
    "    for u in G_directed:\n",
    "        local_community = node2community[u]\n",
    "        local_network = set(G_directed.neighbors(u))\n",
    "        within_cluster, within_cluster_filter, without_cluster, without_cluster_filter = 0, 0, 0, 0\n",
    "        for v in G_directed.neighbors(u):\n",
    "            filtered = node2community[v] == local_community\n",
    "            for w in G_directed.neighbors(v):\n",
    "                add = edges2weights[v, w] if weighted else 1\n",
    "                if w in local_network or node2community[w] == local_community:\n",
    "                    within_cluster += add\n",
    "                    if filtered:\n",
    "                        within_cluster_filter += add\n",
    "                else:\n",
    "                    without_cluster += add\n",
    "                    if filtered:\n",
    "                        without_cluster_filter += add\n",
    "\n",
    "        if (without_cluster+within_cluster) == 0:\n",
    "            node2embed_ego[u] = np.nan\n",
    "        else:\n",
    "            node2embed_ego[u] = (without_cluster-within_cluster)/(without_cluster+within_cluster)\n",
    "        \n",
    "        if (without_cluster_filter + within_cluster_filter) == 0:\n",
    "            node2embed_ego_filter[u] = np.nan\n",
    "        else:\n",
    "            node2embed_ego_filter[u] = (without_cluster_filter-within_cluster_filter)/(without_cluster_filter+within_cluster_filter) \n",
    "\n",
    "    for u in G_directed:\n",
    "        local_community = node2community[u]\n",
    "        local_network = set(G_directed.neighbors(u))\n",
    "        v2ei, v2ei_filtered, uv2weight = {}, {}, {}\n",
    "        # As G is a directed network, u's network only includes those who u has sent an email to\n",
    "        for v in G_directed.neighbors(u):\n",
    "            uv2weight[v] = edges2weights[u, v]\n",
    "            filtered = node2community[v] == local_community\n",
    "            # skip any alter who doesn't have an alter\n",
    "            if len(list(G_directed.neighbors(v))) == 0:\n",
    "                continue\n",
    "            within_cluster, without_cluster = 0, 0    \n",
    "            for w in G_directed.neighbors(v):\n",
    "                add = edges2weights[v, w] if weighted else 1\n",
    "                if w in local_network or node2community[w] == local_community:\n",
    "                    within_cluster += add\n",
    "                else:\n",
    "                    without_cluster += add\n",
    "            v_ei = (without_cluster-within_cluster)/(without_cluster+within_cluster)\n",
    "            v2ei[v] = v_ei\n",
    "            if node2community[v] == local_community:\n",
    "                v2ei_filtered[v] = v_ei\n",
    "        \n",
    "        if len(v2ei) == 0:\n",
    "            node2embed_alter[u] = np.nan\n",
    "        else:\n",
    "            u_ei = 0\n",
    "            if weighted:\n",
    "                u_weight = sum(uv2weight.values())\n",
    "                uv2prop = {u : weight / u_weight for u, weight in uv2weight.items()}\n",
    "                for v, ei in v2ei.items():\n",
    "                    u_ei += (uv2prop[v] * ei)\n",
    "            else:\n",
    "                u_ei = sum(v2ei.values()) / len(v2ei)\n",
    "            node2embed_alter[u] = u_ei\n",
    "        \n",
    "        if len(v2ei_filtered) == 0:\n",
    "            node2embed_alter_filter[u] = np.nan\n",
    "        else:\n",
    "            u_ei_filtered = 0 \n",
    "            # if weight by total traffic to same community alters only, need to create a new uv2prop\n",
    "            if weighted: \n",
    "                for v, ei in v2ei_filtered.items():\n",
    "                    u_ei_filtered += (uv2prop[v] * ei)\n",
    "            else:\n",
    "                u_ei_filtered = sum(v2ei_filtered.values()) / len(v2ei_filtered)\n",
    "            node2embed_alter_filter[u] = u_ei_filtered\n",
    "\n",
    "    # alter alter level measure\n",
    "    for u in G_directed:\n",
    "        local_network = set(G_directed.neighbors(u))\n",
    "        local_community = node2community[u]\n",
    "        v2ei, v2ei_filtered, uv2weight = {}, {}, {}\n",
    "        # As G is a directed network, u's network only includes those who u has sent an email to\n",
    "        for v in G_directed.neighbors(u):\n",
    "            uv2weight[v] = edges2weights[u, v]\n",
    "            # skip any alter who doesn't have an alter\n",
    "            if len(list(G_directed.neighbors(v))) == 0:\n",
    "                continue\n",
    "            w2ei, vw2weight = {}, {}\n",
    "            for w in G_directed.neighbors(v):\n",
    "                vw2weight[w] = edges2weights[v, w]\n",
    "                if len(list(G_directed.neighbors(w))) == 0:\n",
    "                    continue\n",
    "                within_cluster, without_cluster = 0, 0\n",
    "                for z in G_directed.neighbors(w):\n",
    "                    add = edges2weights[w, z] if weighted else 1\n",
    "                    if z in local_network or node2community[z] == local_community:\n",
    "                        within_cluster += add\n",
    "                    else:\n",
    "                        without_cluster += add\n",
    "                w2ei[w] = (without_cluster-within_cluster)/(without_cluster+within_cluster)\n",
    "            \n",
    "            if len(w2ei) > 0:\n",
    "                v_ei = 0\n",
    "                if weighted:\n",
    "                    v_weight = sum(vw2weight.values())\n",
    "                    vw2prop = {w : weight/v_weight for w, weight in vw2weight.items()}\n",
    "                    for w, ei in w2ei.items():\n",
    "                        v_ei += (vw2prop[w] * ei)\n",
    "                else:\n",
    "                    v_ei = sum(w2ei.values()) / len(w2ei)\n",
    "                v2ei[v] = v_ei\n",
    "                if node2community[v] == local_community:\n",
    "                    v2ei_filtered[v] = v_ei\n",
    "        if len(v2ei) == 0:\n",
    "            node2embed_alter2[u] = np.nan\n",
    "        else:\n",
    "            u_ei = 0\n",
    "            if weighted:\n",
    "                u_weight = sum(uv2weight.values())\n",
    "                uv2prop = {u : weight / u_weight for u, weight in uv2weight.items()}\n",
    "                for v, ei in v2ei.items():\n",
    "                    u_ei += (uv2prop[v] * ei)\n",
    "            else:\n",
    "                u_ei = sum(v2ei.values()) / len(v2ei)\n",
    "            node2embed_alter2[u] = u_ei\n",
    "\n",
    "        if len(v2ei_filtered) == 0:\n",
    "            node2embed_alter2[u] = np.nan\n",
    "        else:\n",
    "            u_ei_filtered = 0\n",
    "            if weighted:\n",
    "                for v, ei in v2ei_filtered.items():\n",
    "                    u_ei += (uv2prop[v] * ei)\n",
    "            else:\n",
    "                u_ei = sum(v2ei_filtered.values()) / len(v2ei)\n",
    "            node2embed_alter2_filter[u] = u_ei\n",
    "\n",
    "    return [node2embed_ego, node2embed_ego_filter, node2embed_alter, node2embed_alter_filter, node2embed_alter2, node2embed_alter2_filter]\n",
    "\n",
    "def generate_community_embeddedness(G_directed, community_algorithm, weighted, edges2weights):\n",
    "    \"\"\"\n",
    "    Generate the degree to which one's local network is embedded in one's community or outside of one's community\n",
    "    Parameter\n",
    "    ---------\n",
    "    G_directed : NetworkX DiGraph\n",
    "    community_algorithm : str\n",
    "        Indicates type of algorithm to use for community detection\n",
    "    weight : str\n",
    "        Either None or weight attribute\n",
    "    Returns\n",
    "    -------\n",
    "    measures : [dict, dict, dict, dict, dict, dict, dict, n_comm]\n",
    "        A list of possible community measures\n",
    "    \"\"\"\n",
    "    sys.stderr.write(\"Computing communities using {}'s algorithm at {}.\\n\".format(community_algorithm, datetime.now()))\n",
    "    communities = []\n",
    "    if community_algorithm == 'cnm':\n",
    "        communities = algorithms.greedy_modularity(G_directed, weight='weight').communities\n",
    "    elif community_algorithm == 'leiden':\n",
    "        try:\n",
    "            communities = algorithms.leiden(G_directed, weights='weight').communities\n",
    "        except nx.exception.AmbiguousSolution as e:\n",
    "            print('No community found using {} due to AmbiguousSolution error.'.format(community_algorithm))\n",
    "            return defaultdict(lambda : np.nan)\n",
    "    elif community_algorithm == 'surprise':\n",
    "        try:\n",
    "          communities = algorithms.surprise_communities(G_directed, weights='weight').communities\n",
    "        except nx.exception.AmbiguousSolution as e:\n",
    "            print('No community found using {} due to AmbiguousSolution error.'.format(community_algorithm))\n",
    "            return defaultdict(lambda : np.nan)\n",
    "    else:\n",
    "        print(\"Community detection algorithm {} not supported\".format(community_algorithm))\n",
    "        return defaultdict(lambda : np.nan)\n",
    "    \n",
    "    node2community = {node : i for i, c in enumerate(communities) for node in c}\n",
    "    measures = generate_community_ei(G_directed, node2community, weighted, edges2weights)\n",
    "    measures.append(len(communities))\n",
    "    return measures\n",
    "\n",
    "def compute_threshold(edges2weights):\n",
    "    \"\"\"\n",
    "    Computes the 20th percentage edge weight for all nodes\n",
    "    Parameters\n",
    "    ----------\n",
    "    edges2weights : dict of {tuple : list}\n",
    "        Maps all directed edges to the weight of the edge\n",
    "    Returns\n",
    "    -------\n",
    "    node2threshold : dict of {str : int}\n",
    "        Maps all nodes to the 20th percentile threshold\n",
    "    \"\"\"\n",
    "    node2weights = defaultdict(list)\n",
    "    node2threshold = defaultdict(lambda : None)\n",
    "    for edge, weight in edges2weights.items():\n",
    "        node2weights[edge[0]].append(weight)\n",
    "\n",
    "    for n, weights in node2weights.items():\n",
    "        node2threshold[n] = np.percentile(weights, 20)\n",
    "    return node2threshold\n",
    "\n",
    "def generate_network_measures(timekey, edges, test_mode):\n",
    "    \"\"\"\n",
    "    Generating network measures for a given time period using edges\n",
    "    Parameters\n",
    "    ----------\n",
    "    timekey : str\n",
    "        A string that represents the time period for which network measures are being computed\n",
    "    edges : list\n",
    "        A list of directd edges represented by 2-tuples\n",
    "    test_mode : bool\n",
    "        If true, restrict edges to a hundredth of all edges\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(edges) < 10:\n",
    "        sys.stderr.write('Returning empty network at %s with %d edges at %s.\\n' % (timekey, len(edges), datetime.now()))\n",
    "        return dict()\n",
    "\n",
    "    G_directed = nx.DiGraph()\n",
    "    sys.stderr.write('Generating weighted network measures for %s with %d edges at %s.\\n' % (timekey, len(edges), datetime.now()))\n",
    "    edges2weights = Counter(edges)\n",
    "    weighted_edges = [(edge[0], edge[1], weight) for edge, weight in edges2weights.items()]\n",
    "    G_directed.add_weighted_edges_from(weighted_edges)\n",
    "    usr_quarter2network_measures = defaultdict(list)\n",
    "\n",
    "    weighted_degree = G_directed.degree(weight='weight')\n",
    "    unweighted_degree = G_directed.degree(weight=None)\n",
    "    weighted_clustering = nx.clustering(G_directed, weight='weight')\n",
    "    unweighted_clustering = nx.clustering(G_directed, weight=None)\n",
    "\n",
    "    for n in G_directed:\n",
    "        row = ([weighted_degree[n], unweighted_degree[n], weighted_clustering[n], unweighted_clustering[n]])\n",
    "        usr_quarter2network_measures[(n, timekey)] = row\n",
    "    return dict(usr_quarter2network_measures)\n",
    "\n",
    "def time_edges_to_df(time_edges, test_mode=False):\n",
    "    \"\"\"\n",
    "    Calculates network measures using edge lists\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_edges : dict\n",
    "        A dictionary that maps quarters (quarters only) to a list of edges belonging to that time period\n",
    "    test_mode : bool, optional\n",
    "        If true, only generate one network\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        A dataframe of network measures with user id and timekey_type as index\n",
    "    \"\"\"\n",
    "    if test_mode:\n",
    "        time_edges = {quarter:edges for quarter, edges in time_edges.items() if len(edges) > 5}\n",
    "        test_timekey = random.choice(list(time_edges))\n",
    "        sys.stderr.write(\"Testing timekey %s out of %d time periods.\\n\" % (test_timekey, len(time_edges)))\n",
    "        network_measures = generate_network_measures(test_timekey, time_edges[test_timekey], test_mode)\n",
    "    else:\n",
    "        pool = multiprocessing.Pool(processes = num_cores)\n",
    "        results = [pool.apply_async(generate_network_measures, args=(timekey, edges, test_mode, )) for timekey, edges in time_edges.items()]\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        network_measures = defaultdict(list)\n",
    "        for r in results:\n",
    "            network_measures.update(r.get())\n",
    "\n",
    "    cols = (['weighted_degree', 'unweighted_degree', 'weighted_clustering', 'unweighted_clustering'])    \n",
    "    df = dict_to_df(network_measures, cols, index_name=['anon_id', 'quarter'])\n",
    "    return df.round(5)\n",
    "\n",
    "def extract_quarterly_network_measures(corpus_dir, test_mode=False):\n",
    "    \"\"\"\n",
    "    Main workhorse function for computing netwrork measures and writing them to file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_dir : str\n",
    "        Location of cleaned emails\n",
    "    test_mode : bool, optional\n",
    "        If testing, modify file_name to include flags for testing in final output file name\n",
    "    \"\"\"\n",
    "    if test_mode:\n",
    "        global output_dir\n",
    "        output_dir = test_dir\n",
    "    \n",
    "    edges_file = 'edges_corrected.txt'\n",
    "    edges_file = os.path.join(output_dir, edges_file)\n",
    "    \n",
    "    quarterly_edges = defaultdict(list)\n",
    "    if os.path.exists(edges_file):\n",
    "        sys.stderr.write(\"Reading edges from edge file at %s.\\n\" % str(datetime.now()))\n",
    "        with open(edges_file, 'r') as f:\n",
    "            for line in f:\n",
    "                tup = literal_eval(line)\n",
    "                quarterly_edges[tup[0]].append((tup[1], tup[2]))\n",
    "    else:\n",
    "        sys.stderr.write(\"Computing edges at %s.\\n\" % str(datetime.now()))\n",
    "        quarterly_edges = get_quarterly_edges(test_mode)\n",
    "        sys.stderr.write(\"Writing edges to edge file at %s.\\n\" % str(datetime.now()))\n",
    "        with open(edges_file, 'w') as f:\n",
    "            for quarter, edges in quarterly_edges.items():\n",
    "                for e in edges:\n",
    "                    f.write(str((quarter, e[0], e[1]))+'\\n')\n",
    "\n",
    "    network_df_quarterly = time_edges_to_df(quarterly_edges, test_mode)\n",
    "    network_quarterly_filename = os.path.join(output_dir, \"tech_network_embed.csv\")\n",
    "    network_df_quarterly.to_csv(network_quarterly_filename)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9762e22-d0f9-4f9d-a3ba-f936a3310655",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "test_mode = False\n",
    "try:\n",
    "    test_mode = sys.argv[1].lower() == 'test'\n",
    "except IndexError as error:\n",
    "    pass\n",
    "\n",
    "sys.stderr.write('Reading HR and survey data at %s.\\n' % datetime.now())\n",
    "hr_df = extract_hr_df(hr_file)\n",
    "usr2gender = {usr: 'female' if female == 1 else 'male' for usr, female in hr_df['female'].dropna().to_dict().items()}\n",
    "hr_df_quarterly = extract_hr_df(hr_file, 'quarter')\n",
    "hr_df_quarterly['department'] = np.select([(hr_df_quarterly['sales'] == 1), (hr_df_quarterly['tech'] == 1), (hr_df_quarterly['marketing'] == 1)], ['sales', 'tech', 'marketing'], default='other')\n",
    "usr_quarter2department = hr_df_quarterly['department'].dropna().to_dict()\n",
    "    \n",
    "sys.stderr.write('Generating network measures at %s.\\n' % datetime.now())\n",
    "extract_quarterly_network_measures(corpus_dir, test_mode)\n",
    "\n",
    "sys.stderr.write('Finished generating network measures at %s with a duration of %s.\\n' % (datetime.now(), str(datetime.now()-starttime)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
