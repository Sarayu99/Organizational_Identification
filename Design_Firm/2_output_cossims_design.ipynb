{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b17d3a-4f51-45c1-9d27-90e1384c3cf4",
   "metadata": {},
   "source": [
    "**Company** : <br>\n",
    "Design Firm\n",
    "\n",
    "**Notebook Function** : <br>\n",
    "    This notebook processes the identification measures \n",
    "\n",
    "**Output File(s)** : <br>\n",
    "    hash_embeddings_50_all_quarterly_internal.csv - A file containing the final identification measures for each person-quarter\n",
    "    \n",
    "**Author(s)** : <br>\n",
    "Lara Yang, Sarayu Anshuman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a77686-0d46-4932-979b-9e383ecf9ae4",
   "metadata": {},
   "source": [
    "Install packages and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccce858-4a7a-4b0b-8bd9-2d61ef77d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e202c-e57c-4eed-b061-1aa970f65823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88cc1a5-2eaf-42b5-80be-4f0598d95d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mittens import Mittens\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import json\n",
    "import ujson\n",
    "import re\n",
    "from gensim.matutils import cossim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from statistics import mean \n",
    "import multiprocessing\n",
    "from hashlib import md5\n",
    "from collections import Counter\n",
    "import random\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "import us\n",
    "import pytz\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "num_users_to_test = 12\n",
    "\n",
    "year_colname, quarter_colname, halfyear_colname, yearmonth_colname = 'year', 'quarter', 'halfyear', 'yearmonth'\n",
    "num_cores = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b46c41-396c-430f-a462-88a4f86c3aa8",
   "metadata": {},
   "source": [
    "Run helper function for calculating th eidentification measures for each person-quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4c587-a1bf-4db4-8225-fef09f089608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "########### Helper Functions for Generating Mittens Embeddings ##########\n",
    "#########################################################################\n",
    "\n",
    "def _window_based_iterator(toks, window_size, weighting_function):\n",
    "    for i, w in enumerate(toks):\n",
    "        yield w, w, 1\n",
    "        left = max([0, i-window_size])\n",
    "        for x in range(left, i):\n",
    "            yield w, toks[x],weighting_function(x)\n",
    "        right = min([i+1+window_size, len(toks)])\n",
    "        for x in range(i+1, right):\n",
    "            yield w, toks[x], weighting_function(x)\n",
    "    return\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        data = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader}\n",
    "    return data\n",
    "\n",
    "# Inspired by original build_weighted_matrix in utils.py in the Mittens paper source codebase\n",
    "def build_weighted_matrix(files,\n",
    "        mincount=300, vocab_size=None, window_size=10,\n",
    "        weighting_function=lambda x: 1 / (x + 1), load_to_mem=False, parallel=False, verbose=False,\n",
    "        internal_only=False):\n",
    "    \"\"\"\n",
    "    Builds a count matrix based on a co-occurrence window of\n",
    "    `window_size` elements before and `window_size` elements after the\n",
    "    focal word, where the counts are weighted based on proximity to the\n",
    "    focal word.\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list of strings\n",
    "        Filenames where documents in a corpus are located\n",
    "    corpus : iterable of str\n",
    "        Texts to tokenize.\n",
    "    mincount : int\n",
    "        Only words with at least this many tokens will be included.\n",
    "    vocab_size : int or None\n",
    "        If this is an int above 0, then, the top `vocab_size` words\n",
    "        by frequency are included in the matrix, and `mincount`\n",
    "        is ignored.\n",
    "    window_size : int\n",
    "        Size of the window before and after. (So the total window size\n",
    "        is 2 times this value, with the focal word at the center.)\n",
    "    weighting_function : function from ints to floats\n",
    "        How to weight counts based on distance. The default is 1/d\n",
    "        where d is the distance in words.\n",
    "    load_to_mem : bool, optional\n",
    "        Whether to load entire corpus to memory\n",
    "    parallel : bool, optional\n",
    "        Whether to process emails in parallel. Only makes sense if loading to memory.\n",
    "    internal_only : bool, optional\n",
    "        Whether to only include internal emails - emails whose recipients only include company employees\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        This is guaranteed to be a symmetric matrix, because of the\n",
    "        way the counts are collected.\n",
    "    \"\"\"\n",
    "    wc = defaultdict(int)\n",
    "    # This variable will be a list when load_to_mem is true and a generator when load_to_mem is false\n",
    "    corpus = None\n",
    "    if load_to_mem:\n",
    "        if parallel:\n",
    "            pool = multiprocessing.Pool(processes = num_cores)\n",
    "            results = [pool.apply_async(read_corpus_parallel, args=(f, internal_only, )) for f in files]\n",
    "            corpus = [r.get() for r in results if r.get()]\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        else:\n",
    "            # previously used split(' ') in 2yp\n",
    "            corpus = [text.split() for text in read_corpus(files, internal_only)]\n",
    "        for toks in corpus:\n",
    "            for tok in toks:\n",
    "                wc[tok] += 1\n",
    "        sys.stderr.write('\\n')\n",
    "    else:\n",
    "        corpus = read_corpus(files, internal_only)\n",
    "        for text in corpus:\n",
    "            for tok in text.split():\n",
    "                wc[tok] += 1\n",
    "    if verbose: sys.stderr.write('Finished counting %d unique words in corpus at %s.\\n' % (len(wc), datetime.now()))\n",
    "    if vocab_size:\n",
    "        srt = sorted(wc.items(), key=itemgetter(1), reverse=True)\n",
    "        vocab_set = {w for w, c in srt[: vocab_size]}\n",
    "    else:\n",
    "        vocab_set = {w for w, c in wc.items() if c >= mincount}\n",
    "    vocab = sorted(vocab_set)\n",
    "    n_words = len(vocab)\n",
    "    if verbose: sys.stderr.write('Finished generating vocab of %d words at %s.\\n' % (n_words, datetime.now()))\n",
    "    # Weighted counts:\n",
    "    # Generator function needs to be re-initated\n",
    "    if not load_to_mem: corpus = read_corpus(files, internal_only)\n",
    "    counts = defaultdict(float)\n",
    "    for toks in corpus:\n",
    "        if not load_to_mem: toks = toks.split()\n",
    "        window_iter = _window_based_iterator(toks, window_size, weighting_function)\n",
    "        for w, w_c, val in window_iter:\n",
    "            if w in vocab_set and w_c in vocab_set:\n",
    "                counts[(w, w_c)] += val\n",
    "    if verbose: sys.stderr.write('Finished counting co-occurrences across %d word pairs at %s.\\n' % (len(counts), datetime.now()))\n",
    "    X = np.zeros((n_words, n_words))\n",
    "    for i, w1 in enumerate(vocab):\n",
    "        for j, w2 in enumerate(vocab):\n",
    "            X[i, j] = counts[(w1, w2)]\n",
    "    if verbose: sys.stderr.write('Finished converting co-occurrences to sorted matrix of shape %s at %s.\\n' % (str(X.shape), datetime.now()))\n",
    "    X = pd.DataFrame(X, columns=vocab, index=pd.Index(vocab))\n",
    "    return X\n",
    "\n",
    "# this method is currently unused as it was used for proof-of-concept during macro lunch,\n",
    "# where only employees with the top 200 vocab counts are included\n",
    "def select_usr_corpus(inpath, num_usrs_to_process):\n",
    "    infile = pd.read_csv(inpath)\n",
    "    usr2num_tokens = infile.set_index([\"usr\"]).dropna().to_dict()[\"num_tokens\"]\n",
    "    usrs_to_analyze = [usr for usr, num_tokens in sorted(usr2num_tokens.items(), key=lambda item: item[1])]\n",
    "    usrs_to_analyze = usrs_to_analyze[-num_usrs_to_process:]\n",
    "    return usrs_to_analyze  \n",
    "\n",
    "# The function currently ignores sentence delimiters and considers co-occurrences across sentence boundaries to be\n",
    "# co-occurrences as well. If desired, we can vary yield behavior based on the sentence_delim boolean to yield every\n",
    "# sentence instead of every message as it currently does. However, if we were to yield every sentence, we need to make\n",
    "# sure to not simply output the body but to yield every new line in the body individually.\n",
    "def read_corpus(files, internal_only, sentence_delim=False):\n",
    "    for file in files:\n",
    "        with open(file, errors='ignore', encoding='utf-8') as infile:\n",
    "            try:\n",
    "                msg = ujson.load(infile)\n",
    "                if internal_only and not is_internal_msg(msg):\n",
    "                    continue\n",
    "                # This line ignores all sentence structures as we are using \\n to represent sentence splits\n",
    "                body = msg['hashed-body'].replace('\\n', ' ').strip()\n",
    "                if len(body) > 0:\n",
    "                    yield body\n",
    "                else:\n",
    "                    continue\n",
    "            except (ValueError, json.decoder.JSONDecodeError) as error:\n",
    "                continue\n",
    "    return\n",
    "\n",
    "def read_corpus_parallel(f, internal_only, sentence_delim=False):\n",
    "    with open(f, errors='ignore', encoding='utf-8') as infile:\n",
    "        try:\n",
    "            msg = ujson.load(infile)\n",
    "            if internal_only and not is_internal_msg(msg):\n",
    "                return None\n",
    "            # This line ignores all sentence structures as we are using \\n to represent sentence splits\n",
    "            body = msg['hashed-body'].replace('\\n', ' ').strip()\n",
    "            if len(body) > 0:\n",
    "                return body.split()\n",
    "            else:\n",
    "                return None\n",
    "        except (ValueError, json.decoder.JSONDecodeError) as error:\n",
    "            return None\n",
    "\n",
    "def output_embeddings(mittens_df, filename, compress=False):\n",
    "    \"\"\"\n",
    "    Writes embeddings in the format of pd.DataFrame to a text file, replacing any existing file with the\n",
    "    same name\n",
    "    \"\"\"\n",
    "    if compress:\n",
    "        mittens_df.to_csv(filename + '.gz', quoting=csv.QUOTE_NONE, header=False, sep=\" \", compression='gzip')\n",
    "    else:\n",
    "        mittens_df.to_csv(filename, quoting=csv.QUOTE_NONE, header=False, sep=\" \")\n",
    "    return\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with JSON Emails #############\n",
    "#########################################################################\n",
    "# Same as get_recipient function in Analysis/acculturaltion/lingdistance/jensen_shannon.py\n",
    "def get_recipients(msg):\n",
    "    sender = msg['from'][0] if type(msg['from']) == list else msg['from']\n",
    "    rec = set(msg['to'] + msg['cc'] + msg['bcc']) - set([sender])\n",
    "    return rec\n",
    "\n",
    "def is_internal_msg(msg):\n",
    "    \"\"\"\n",
    "    Determines whether msg is an internal message. An internal message is a message that is only sent to internal employees,\n",
    "    whose anon IDs start with 'U'. An individual who is not an internal employee does not have a domain associated with design firm and\n",
    "    their anon IDs start with 'E'.\n",
    "    \"\"\"\n",
    "    for r in get_recipients(msg):\n",
    "        if re.match('E---', r):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def slice_user_corpus(files, train_mode):\n",
    "    timekey2files = defaultdict(list)\n",
    "    #the function iterates through each email in the email list\n",
    "    for file in files:\n",
    "        with open(file, errors='ignore') as infile:\n",
    "            try:\n",
    "                msg = ujson.load(infile)\n",
    "                if train_mode == 'annual':\n",
    "                    timekey2files[to_year(msg['date'], format='str')].append(file)\n",
    "                elif train_mode == 'quarterly':\n",
    "                    timekey2files[to_quarter(msg['date'], format='str')].append(file)\n",
    "                elif train_mode == 'monthly':\n",
    "                    timekey2files[to_yearmonth(msg['date'], format='str')].append(file)\n",
    "                elif train_mode == 'halfyear':\n",
    "                    timekey2files[to_halfyear(msg['date'], format='str')].append(file)\n",
    "                elif train_mode == 'all':\n",
    "                    timekey2files[to_year(msg['date'], format='str')].append(file)\n",
    "                    timekey2files[to_quarter(msg['date'], format='str')].append(file)\n",
    "                    timekey2files[to_halfyear(msg['date'], format='str')].append(file)\n",
    "                    #timekey2files[to_yearmonth(msg['date'], format='str')].append(file) #removed as it leads to too many output files\n",
    "            except (ValueError, json.decoder.JSONDecodeError) as error:\n",
    "                continue\n",
    "    return timekey2files\n",
    "\n",
    "def read_message(files):\n",
    "    errors = 0\n",
    "    for file in files:\n",
    "        with open(file, errors='ignore') as infile:\n",
    "            try:\n",
    "                msg = ujson.load(infile)\n",
    "                yield msg\n",
    "            except (ValueError, json.decoder.JSONDecodeError) as error:\n",
    "                errors += 1\n",
    "                continue\n",
    "    sys.stderr.write(\"{} files produced errors out of a total of {} files.\\n\".format(errors, len(files)))\n",
    "    return\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with Date Objects ############\n",
    "#########################################################################\n",
    "\n",
    "def to_yearmonth(date, format):\n",
    "    if format == 'str':\n",
    "        return date[0:7]\n",
    "    elif format == 'datetime':\n",
    "        return date.strftime('%Y-%m')\n",
    "\n",
    "def to_quarter(date, format):\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]    \n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    quarter = ((int(month)-1) // 3) + 1\n",
    "    timekey = str(year) + 'Q' + str(quarter)\n",
    "    return timekey\n",
    "\n",
    "def to_halfyear(date, format):\n",
    "    \"\"\"\n",
    "    Return half year of date in string\n",
    "    \"\"\"\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]    \n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    halfyear = ((int(month)-1) // 6) + 1\n",
    "    timekey = str(year) + 'HY' + str(halfyear)\n",
    "    return timekey\n",
    "\n",
    "def to_year(date, format):\n",
    "    if format == 'str':\n",
    "        return date[0:4]\n",
    "    elif format == 'datetime':\n",
    "        return str(date.year)\n",
    "\n",
    "def datetime_to_timekey(date, time_key):\n",
    "    if time_key == 'year':\n",
    "        return to_year(date, format='datetime')\n",
    "    elif time_key == 'quarter':\n",
    "        return to_quarter(date, format='datetime')\n",
    "    elif time_key == 'halfyear': #added this new\n",
    "        return to_halfyear(date, format='datetime') #added this new\n",
    "    elif time_key == 'yearmonth':\n",
    "        return to_yearmonth(date, format='datetime')\n",
    "\n",
    "def is_month_before_equal(datetime1, datetime2):\n",
    "    if datetime1.year < datetime2.year:\n",
    "        return 1\n",
    "    elif (datetime1.year == datetime2.year) and (datetime1.month <= datetime2.month):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def num_months_between_dates(datetime1, datetime2):\n",
    "    return abs((datetime1.year - datetime2.year) * 12 + datetime1.month - datetime2.month)\n",
    "\n",
    "def num_quarters_between_dates(datetime1, datetime2):\n",
    "    return abs((datetime1.year - datetime2.year) * 12 + datetime1.month - datetime2.month) // 3\n",
    "\n",
    "def num_years_between_dates(datetime1, datetime2):\n",
    "    return abs(datetime1.year - datetime2.year)\n",
    "\n",
    "def time_between_dates(datetime1, datetime2, time_key):\n",
    "    if time_key == 'monthly':\n",
    "        return num_months_between_dates(datetime1, datetime2)\n",
    "    elif time_key == 'quarterly':\n",
    "        return num_quarters_between_dates(datetime1, datetime2)\n",
    "    elif time_key == 'annual':\n",
    "        return num_years_between_dates(datetime1, datetime2)\n",
    "\n",
    "#########################################################################\n",
    "############## Helper Functions for Working with Dataframes #############\n",
    "#########################################################################\n",
    "def dict_to_df(index2rows, cols, index_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    index2rows : dict\n",
    "        Dictionary mapping index to rows to be coverted\n",
    "    cols : list\n",
    "        List of column names of type str\n",
    "    index : list\n",
    "        List of index names\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Constructed dataframe\n",
    "    \"\"\"\n",
    "    if len(index_name) == 1:\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df.index.name = index_name[0]\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df = pd.DataFrame(df, pd.MultiIndex.from_tuples(df.index, names=index_name))\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "def generate_test_df():\n",
    "    \"\"\"\n",
    "    A function to generate a DataFrame for testing purposes.\n",
    "\n",
    "    This DataFrame uses Gender as the index and has a column for simulated word vectors for 'i'\n",
    "    and a column for simulated word vectors for 'we'. Each of these word vectors is a size-10 numpy vector.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['gender'] = ['F', 'M', 'M', 'F', 'M', 'F', 'F', 'M', 'M', 'F']\n",
    "    df['i'] = df.apply(lambda x: np.random.rand(10).round(5), axis=1)\n",
    "    df['we'] = df.apply(lambda x: np.random.rand(10).round(5), axis=1)\n",
    "    df.set_index('gender', inplace=True)\n",
    "    return df\n",
    "\n",
    "#########################################################################\n",
    "########### Helper Functions for Working with Embedding Output ##########\n",
    "#########################################################################\n",
    "\n",
    "def remove_empty_embeddings(embeddings_dir):\n",
    "    \"\"\"\n",
    "    Removes all empty files in embeddings_dir that were produced when vocab size was 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_dir : str\n",
    "        Full path to directory where embedding files are located\n",
    "    \"\"\"\n",
    "    for file in os.listdir(embeddings_dir):\n",
    "        mittens_file = os.path.join(embeddings_dir, file)\n",
    "        if os.path.getsize(mittens_file) == 0:\n",
    "            os.remove(mittens_file)\n",
    "    return\n",
    "\n",
    "def word_similarity(model, w1, w2):\n",
    "    \"\"\"\n",
    "    This is an auxilary function that allows for comparing one word to another word or multiple words\n",
    "    If w1 and w2 are both single words, n_similarity returns their cosine similarity which is the same as \n",
    "    simply calling similarity(w1, w2)\n",
    "    If w1 or w2 is a set of words, n_similarity essentially takes the mean of the set of words and then computes\n",
    "    the cosine similarity between that vector mean and the other vector. This functionality is both reflected\n",
    "    in its source code and has been verified manually.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : KeyedVectors\n",
    "        The model that contains all the words and vectors\n",
    "    w1 : str or list\n",
    "        The first word or word list to be compared\n",
    "    w2 : str or list\n",
    "        The second word or word list to be compared\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between w1 and w2\n",
    "    \"\"\"\n",
    "    if not isinstance(w1, list):\n",
    "        w1 = [w1]\n",
    "    if not isinstance(w2, list):\n",
    "        w2 = [w2]\n",
    "    #w1 = [w for w in w1 if w in model.vocab]\n",
    "    w1 = [w for w in w1 if w in model.key_to_index]\n",
    "    #w2 = [w for w in w2 if w in model.vocab]\n",
    "    w2 = [w for w in w2 if w in model.key_to_index]\n",
    "    if len(w1) == 0 or len(w2) == 0:\n",
    "        return None\n",
    "    return model.n_similarity(w1, w2)\n",
    "\n",
    "def extract_company_embedding(company_embeddings_filename, tmp_dir, words):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_embeddings_filename : str\n",
    "        File path of the company embeddings\n",
    "    tmp_dir : str\n",
    "        Path to the directory for gensim to output its tmp files in order to load embeddings into word2vec format\n",
    "    words : list\n",
    "        A list of strings to retrieve vectors for\n",
    "    Returns\n",
    "    -------\n",
    "    vecs : list\n",
    "        A list of vectors of type numpy.ndarray that correspond to the list of words given as parameters\n",
    "    \"\"\" \n",
    "    tmp_mittens = os.path.join(tmp_dir, \"mittens_embeddings_all_word2vec.txt\")\n",
    "    word2vec_mittens_file = get_tmpfile(tmp_mittens)\n",
    "    glove2word2vec(company_embeddings_filename, word2vec_mittens_file)\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_mittens_file)\n",
    "    vecs = []\n",
    "\n",
    "    '''\n",
    "    for w in words:\n",
    "        if w in model.vocab:\n",
    "            vecs.append(model.wv[w])\n",
    "        else:\n",
    "            print('%s not in company embeddings' % w)\n",
    "    '''\n",
    "    for w in words:\n",
    "     try:\n",
    "      index = model.key_to_index[w]\n",
    "      vecs.append(model.vectors[index])\n",
    "     except KeyError:\n",
    "      vecs.append(np.nan)\n",
    "      print('%s not in company embeddings' % w)\n",
    "\n",
    "    return vecs\n",
    "\n",
    "def isnull_wrapper(x):\n",
    "    r = pd.isnull(x)\n",
    "    if type(r) == bool:\n",
    "        return r\n",
    "    return r.any()\n",
    "\n",
    "def cossim_with_none(vec1, vec2, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Auxiliary function that calls cossim function to test if vectors are None to prevent erroring out.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : list of (int, float), gensim sparse vector format\n",
    "    vec2 : list of (int, float), gensim sparse vector format\n",
    "    format : str, optional\n",
    "        Either sparse or dense. If sparse, vec1 and vec2 are in gensim sparse vector format; use cossim function from gensim.\n",
    "        Otherwise, vec1 and vec2 are numpy arrays and cosine similarity is hand calculated\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between vec1 and vec2\n",
    "    \"\"\"\n",
    "    if not (isnull_wrapper(vec1) or isnull_wrapper(vec2)):\n",
    "        if vec_format == 'sparse':\n",
    "            return cossim(vec1, vec2)\n",
    "        elif vec_format == 'dense':\n",
    "            if len(vec1) == 0 or len(vec2) == 0:\n",
    "                return None\n",
    "            return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        else:\n",
    "            raise ValueError()\n",
    "    return None\n",
    "\n",
    "def calculate_pairwise_cossim(col1, col2=None, reference=False, reference_group=None, anon_ids=None):\n",
    "    \"\"\"\n",
    "    Calculates averaged cosine similarity of every row vector in col1 with every other row vector in col2.\n",
    "    If no col2 is provided, cosine similarity of every row vector with every other row vector in col1 is calculated.\n",
    "    The two columns should have equal length.\n",
    "    Parameters\n",
    "    ----------\n",
    "    col1 : pd.Series\n",
    "        A column where each row is a sparse word vector (BoW format that gensim code is written for).\n",
    "    col2 : pd.Series, optional\n",
    "        A column where each row is a sparse word vector (BoW format that gensim code is written for).\n",
    "    reference : bool, optional\n",
    "        Indicator variable for whether filtering for reference groups is needed\n",
    "    reference_group : pd.Series, optional\n",
    "        If filtering for reference groups, a list containing reference group members for every employee in col1\n",
    "    anon_ids : pd.Series, optional\n",
    "        If filtering for reference groups, a list containing anon_ids for every employee in col1\n",
    "    Returns\n",
    "    -------\n",
    "    results : list\n",
    "        A list where the ith element is the averaged cosine similarity between the ith vector in col1 and every vector\n",
    "        in col2 for which i != j. If no col2 is provided, a list where the ith element is the averaged cosine similarity\n",
    "        between the ith vector in col1 and every other vector in col1 is returned.\n",
    "    \"\"\"\n",
    "    vectors1 = col1.tolist()\n",
    "    vectors2 = col2.tolist() if col2 is not None else col1.tolist()\n",
    "    reference_group = reference_group.tolist() if reference else None\n",
    "    anon_ids = anon_ids.tolist() if anon_ids is not None else None\n",
    "    results = list()\n",
    "    for i in range(len(vectors1)):\n",
    "        total_sim = []\n",
    "        if vectors1[i]:\n",
    "            for j in range(len(vectors2)):\n",
    "                if i != j and vectors2[j]:\n",
    "                    # filter out any np.nans as our reference group\n",
    "                    if not reference or (type(reference_group[i]) == set and anon_ids[j] in reference_group[i]):\n",
    "                        total_sim.append(cossim(vectors1[i], vectors2[j]))\n",
    "            results.append(mean(total_sim) if len(total_sim) > 0 else None)\n",
    "        else:\n",
    "            results.append(None)\n",
    "    return results\n",
    "\n",
    "def vector_mean(col):\n",
    "    \"\"\"\n",
    "    Calculate vector means of row vectors\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : pd.Series\n",
    "        The column to be averaged\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        A vector that is the numerical average of all vectors in col\n",
    "    \"\"\"\n",
    "    return np.array(col[col.notna()].tolist()).mean(axis=0)\n",
    "\n",
    "def fix_department(hr_df):\n",
    "    \"\"\"\n",
    "    Correcting all typos in the department field. Mostly for generating a correct department\n",
    "    roster used by mittens3d_explore_alternative_embeddings.py\n",
    "    Parameters\n",
    "    ----------\n",
    "    hr_df : pd.DataFrame\n",
    "        A dataframe of HR data\n",
    "    Returns\n",
    "    -------\n",
    "    hr_df : pd.DataFrame\n",
    "        The original dataframe after typos are fixed in the department column\n",
    "    \"\"\"\n",
    "    hr_df['department'] = hr_df['department'].str.replace(\"Enviroments\", \"Environments\", regex=True)\n",
    "    hr_df['department'] = hr_df['department'].str.replace(\"Leanrnign\", \"Learning\", regex=True)\n",
    "    hr_df['department'] = hr_df['department'].str.replace(\" +$\", \"\", regex=True)\n",
    "    return hr_df\n",
    "\n",
    "def convert_to_datetime(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format='%m-%d-%y')\n",
    "    except ValueError:\n",
    "        return pd.to_datetime(date_str, format='%m-%d-%Y')\n",
    "\n",
    "def extract_hr_df(hr_filepath, time_key=None):\n",
    "    hr = pd.read_csv(hr_filepath)\n",
    "    cols = hr.columns\n",
    "    cols = cols.map(lambda x: x.replace(' ', '_') if isinstance(x, str) else x)\n",
    "    hr.columns = cols\n",
    "    #hr['snapshot_date'] = pd.to_datetime(hr['snapshot_date'],format='%m-%d-%Y')\n",
    "    hr['snapshot_date'] = hr['snapshot_date'].apply(convert_to_datetime)\n",
    "    hr = hr.sort_values(by=['anon_id', 'snapshot_date'])\n",
    "    # drop all rehires\n",
    "    hr_rehires = set(hr.loc[hr['rehire_date'].notna(), 'anon_id'])\n",
    "    hr['drop'] = hr.apply(lambda row : 1 if row.anon_id in hr_rehires else 0, axis=1)\n",
    "    hr = hr.loc[hr['drop'] == 0]\n",
    "    hr['hire_date'] = pd.to_datetime(hr['hire_date'], 'coerce')\n",
    "    hr['termination_date'] = pd.to_datetime(hr['termination_date'])\n",
    "    hr['termination_reason'] = hr['termination_reason'].str.lower()\n",
    "    hr.dropna(subset=['hire_date', 'snapshot_date'], axis=0, inplace=True)\n",
    "    hr['entry_age'] = hr.groupby('anon_id')['age'].transform('min')\n",
    "    hr['exit'] = hr.apply(lambda row : 1 if pd.notna(row['termination_reason']) else 0, axis=1)\n",
    "    hr['exit_vol'] = hr.apply(lambda row : 1 if (row['termination_reason'] == 'voluntary') else 0, axis=1)\n",
    "    hr['exit_invol'] = hr.apply(lambda row : 1 if (row['termination_reason'] == 'involuntary') else 0, axis=1)\n",
    "    hr = fix_department(hr)\n",
    "    if not time_key: #person level\n",
    "        supervisors = set(hr['supervisor_id'])\n",
    "        hr['is_supervisor'] = hr.apply(lambda row : 1 if row.anon_id in supervisors else 0, axis=1)\n",
    "        hr['initial_salary'] = hr.apply(lambda row: row.annual_salary if (row.snapshot_date.month == row.hire_date.month) else None, axis=1)\n",
    "        hr['initial_salary'].fillna(method = 'ffill', inplace=True)\n",
    "        hr['salary_delta'] = (hr['annual_salary'] - hr['initial_salary']) / hr['initial_salary']\n",
    "        hr['salary_avg'] = hr.groupby('anon_id')['annual_salary'].transform('mean')\n",
    "        hr['tenure'] = hr.apply(lambda row:\n",
    "            (row.termination_date - row.hire_date).days if pd.notna(row.termination_reason) else (row.snapshot_date - row.hire_date).days, axis=1)\n",
    "        hr.drop_duplicates(subset='anon_id', keep='last', inplace=True)\n",
    "        hr = (hr[['anon_id', 'gender', 'ethnicity', 'department', 'location', 'job_title',\n",
    "            'tenure', 'exit', 'exit_vol', 'exit_invol', 'is_supervisor', 'salary_delta', 'salary_avg', 'entry_age']])\n",
    "        hr.set_index('anon_id', inplace=True)\n",
    "        hr.index.name = 'usr'\n",
    "    else:\n",
    "        hr[time_key] = hr.apply(lambda row : datetime_to_timekey(row['snapshot_date'], time_key), axis=1)\n",
    "        hr['past_salary'] = hr.groupby('anon_id')['annual_salary'].shift(1)\n",
    "        hr['past_job_title'] = hr.groupby('anon_id')['job_title'].shift(1)\n",
    "        hr['promotion'] = hr.apply(lambda row : 1 if (row['annual_salary'] > row['past_salary'] and (row['past_job_title'] != row['job_title'])) else 0, axis=1)\n",
    "        monthly_supervisors = {k: set(sups['supervisor_id'].tolist()) for k, sups in hr[['snapshot_date', 'supervisor_id']].groupby(\"snapshot_date\")}\n",
    "        hr['is_supervisor'] = hr.apply(lambda row : 1 if row['anon_id'] in monthly_supervisors[row['snapshot_date']] else 0, axis=1)\n",
    "        if time_key == quarter_colname or time_key == year_colname or time_key == halfyear_colname:\n",
    "            hr['promotion'] = hr.groupby(['anon_id', time_key])['promotion'].transform('max')\n",
    "            hr['exit'] = hr.groupby(['anon_id', time_key])['exit'].transform('max')\n",
    "            hr['exit_vol'] = hr.groupby(['anon_id', time_key])['exit_vol'].transform('max')\n",
    "            hr['exit_invol'] = hr.groupby(['anon_id', time_key])['exit_invol'].transform('max')\n",
    "            hr['is_supervisor'] = hr.groupby(['anon_id', time_key])['is_supervisor'].transform('max')\n",
    "            hr.drop_duplicates(subset=['anon_id', time_key], keep='last', inplace=True)\n",
    "        hr = (hr[['anon_id', time_key, 'snapshot_date', 'gender', 'ethnicity', 'department', 'location', 'job_title', 'annual_salary', 'hire_date',\n",
    "                'is_supervisor', 'age', 'entry_age', 'promotion', 'exit', 'exit_vol', 'exit_invol']])\n",
    "        hr.set_index(['anon_id', time_key], inplace=True)\n",
    "    return hr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1cf7e4-cb8d-4253-a94d-f471100e3a32",
   "metadata": {},
   "source": [
    "Now run a function to calculate the hash for the design firm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d30fba-f943-45ca-87a3-ff07d58d3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheap_hash(to_hash, n=8): \n",
    "    \"\"\"\n",
    "    Computes a shortened version of the hash of an input string.\n",
    "    Reduces storage space; defense against lookup tables\n",
    "    Collision risk checked - no collision found using 20,000 most common English words\n",
    "    Parameters\n",
    "    ----------\n",
    "    to_hash : str\n",
    "        A string to be hashed\n",
    "    Returns\n",
    "    -------\n",
    "    hashed : str\n",
    "        A string that represents the hash of to_hash with length n\n",
    "    \"\"\"\n",
    "    hashed = md5(to_hash.encode('utf-8')).hexdigest()[:n] \n",
    "    return hashed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6420b661-dedf-4149-9aba-8080c9e9dec9",
   "metadata": {},
   "source": [
    "Identifify the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f0d97-e5b2-457f-b2b9-726f296dd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e11be-53d9-4324-a9d1-6f8e59945a7b",
   "metadata": {},
   "source": [
    "Set the hyperparameters and input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e4943-39fd-41e7-b74d-2fda9cf76ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_only = True\n",
    "first_batches = False\n",
    "num_cores = 14 \n",
    "num_users_to_test = 12\n",
    "mincount = 50\n",
    "\n",
    "#set directory paths\n",
    "home_dir = current_dir\n",
    "home_dir1 = \"/zfs/projects/faculty/amirgo-transfer/design/\"\n",
    "corpus_dirs = [os.path.join(home_dir1, \"hashed_corpus\"), os.path.join(home_dir1, \"hashed_corpus_batch3\")]\n",
    "utils_dir = os.path.join(home_dir, \"utils\")\n",
    "embeddings_dir=\"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/design/fine-tuning/hash_embeddings_50_all_internal\"\n",
    "tmp_dir = os.path.join(current_dir, \"tmp\")\n",
    "output_dir = os.path.join(current_dir, \"idtf_output_data\")\n",
    "\n",
    "#name final output file\n",
    "quarterly_output_filename = os.path.join(output_dir, \"hash_embeddings_{}_b12_quarterly\".format(mincount) if first_batches else \"hash_embeddings_{}_all_quarterly\".format(mincount))\n",
    "if internal_only: quarterly_output_filename += '_internal'\n",
    "quarterly_output_filename += '.csv'\n",
    "\n",
    "#read the hr file for the design firm\n",
    "hr_filepath = os.path.join(home_dir,\"anonymized_hr_data.csv\")\n",
    "quarter_colname = 'quarter'\n",
    "pronouns = ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves']\n",
    "single_pronouns = ['i', 'we']\n",
    "hash_pronouns = [cheap_hash(p) for p in pronouns]\n",
    "hash_single_pronouns = [cheap_hash(p) for p in single_pronouns]\n",
    "i_index, we_index = 0, 5\n",
    "\n",
    "print(corpus_dirs)\n",
    "print(embeddings_dir)\n",
    "print(tmp_dir)\n",
    "print(output_dir)\n",
    "print(quarterly_output_filename)\n",
    "print(hr_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a001764-54d0-43f1-80c3-44e5f53fee1e",
   "metadata": {},
   "source": [
    "Run helper functions to generate the identification measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915ef59-ee2f-44c3-b5db-3050df093a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "############### Functions for Calculating Frequency Counts ##############\n",
    "#########################################################################\n",
    "def scale(x):\n",
    "    \"\"\"\n",
    "    This scale function provides the same result as R's scale. sklearn's preprocessing.scale is slightly different as it uses N instead of N-1\n",
    "    in calculating standard deviation \n",
    "    \"\"\"\n",
    "    return (x-np.mean(x))/np.std(x, ddof=1)\n",
    "\n",
    "def reading_message_time(i, num_users, usr_dirs, usr):\n",
    "    msg2time = {}\n",
    "    files = [os.path.join(usr_dir, file) for usr_dir in usr_dirs for file in os.listdir(usr_dir)]    \n",
    "    for msg in read_message(files):\n",
    "        if msg['date'] is not None and msg['time'] is not None:\n",
    "            msg2time[msg['message-id']] = msg['date'] + ' ' + msg['time']\n",
    "    return msg2time\n",
    "\n",
    "def reading_usr_counts(i, num_users, usr_dirs, usr, msg2time, usr_quarter2timezone, slice_type):\n",
    "    \"\"\"\n",
    "    Reading raw counts for a specific user\n",
    "    Parameters\n",
    "    ----------\n",
    "    i : int\n",
    "        Index of current user in all users; used for keeping track of progress\n",
    "    num_users : int\n",
    "        Total number of users; used to keeping track of progress\n",
    "    usr_dirs : list of str\n",
    "        Contains full path directories that contain emails of the focal user\n",
    "    usr : str\n",
    "        Focal user whose emails we are currently processing; used for keeping track of progress\n",
    "    msg2time : dict\n",
    "        Maps unique message IDs to when they were sent; used for measuring average response time of emails\n",
    "    usr_quarter2timezone: dict\n",
    "        Maps user and quarter to timezone in which they work to provide additional precision to calculating effort variables\n",
    "        based on local timezones of user\n",
    "    slice_type : str\n",
    "        Either 'quarterly' or 'halfyearly'\n",
    "    Returns\n",
    "    -------\n",
    "    quarter2var2counts : dict\n",
    "        A dictionary mapping quarters to names of variables of interest to their values\n",
    "    \"\"\"\n",
    "\n",
    "    files = [os.path.join(usr_dir, file) for usr_dir in usr_dirs for file in os.listdir(usr_dir)]\n",
    "    sys.stderr.write(\"Processing \\t%d/%d -'%s' with %d files, at %s.\\n\" % (i, num_users, usr, len(files), datetime.now()))\n",
    "    quarterly2counts = defaultdict(lambda : [0, 0, 0, 0, set(), set(), []])\n",
    "    pacific = pytz.timezone('America/Los_Angeles')\n",
    "    for msg in read_message(files):\n",
    "        date = msg['date']\n",
    "        if date is None:\n",
    "            continue\n",
    "        if internal_only and not is_internal_msg(msg):\n",
    "            continue\n",
    "\n",
    "        if slice_type == 'quarterly':\n",
    "            quarter = to_quarter(date, format=\"str\")\n",
    "        else: #this is the 'halfyearly case'\n",
    "            quarter = to_halfyear(date, format=\"str\") #the variable quarter would actually contain the halfyear information in this case\n",
    "        \n",
    "        date = datetime.strptime(date + ' ' + msg['time'], '%Y-%m-%d %H:%M:%S')\n",
    "        # dates have all been converted to Pacific Timezone in clean.py\n",
    "        timezoned_date = pacific.localize(date).astimezone(pytz.timezone(usr_quarter2timezone[(usr, quarter)])) if (usr, quarter) in usr_quarter2timezone else date\n",
    "        weekday = timezoned_date.weekday()\n",
    "        hour = timezoned_date.hour\n",
    "\n",
    "        quarterly2counts[quarter][0] += len(msg['hashed-body'].replace('\\n', ' ').strip().split())\n",
    "        quarterly2counts[quarter][1] += 1\n",
    "        for r in get_recipients(msg):\n",
    "            assert r != usr, \"Created a self-loop\"\n",
    "            quarterly2counts[quarter][5].add(r)\n",
    "\n",
    "        if (weekday == 5) or (weekday == 6):\n",
    "            quarterly2counts[quarter][2] += 1\n",
    "            quarterly2counts[quarter][4].add(timezoned_date.strftime('%Y-%m-%d'))\n",
    "        elif (hour < 8) or (hour > 17):\n",
    "            quarterly2counts[quarter][3] += 1\n",
    "        if msg['in-reply-to'] is not None and msg['in-reply-to'] in msg2time:\n",
    "            diff = date - datetime.strptime(msg2time[msg['in-reply-to']], '%Y-%m-%d %H:%M:%S')\n",
    "            quarterly2counts[quarter][6].append(diff.total_seconds())\n",
    "    for quarter in quarterly2counts.keys():\n",
    "        quarterly2counts[quarter][4] = len(quarterly2counts[quarter][4])\n",
    "        if len(quarterly2counts[quarter][6]) > 0:\n",
    "            quarterly2counts[quarter][6] = sum(quarterly2counts[quarter][6])/len(quarterly2counts[quarter][6])\n",
    "        else:\n",
    "            quarterly2counts[quarter][6] = None\n",
    "    return dict(quarterly2counts)\n",
    "\n",
    "def reading_raw_counts(corpus_dirs, first_batches, hr_df_quarterly, slice_type='quarterly', test_mode=False):\n",
    "    \"\"\"\n",
    "    The main workhorse function for obtaining raw message and token counts as control variables.\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_dirs : list of str\n",
    "        Contains full path directories to both email corpora, batch 1 & 2 and batch 3\n",
    "    first_batches : bool\n",
    "        If true, only use emails in batch 1 and 2\n",
    "    hr_df_quarterly : pd.DataFrame\n",
    "        Used for performing departmental standardizing of effort variables, \n",
    "        but we can also pass a halfyearly file, so the local variable names in the function will still go by quarter in this case, but it will contain halfyearly data\n",
    "    slice_type :\n",
    "        Either 'quarterly' or 'halfyearly'\n",
    "    test_mode : bool, optional\n",
    "        If true, select a subset of user directories\n",
    "    Returns\n",
    "    -------\n",
    "    usr_quarter2counts_df : pd.DataFrame\n",
    "        Dataframe with user and quarter as multi-index and message and token counts as columns\n",
    "    \"\"\"\n",
    "    if first_batches:\n",
    "        corpus_dir = corpus_dirs[0]\n",
    "        usr_dirs = [os.path.join(corpus_dir, usr_dir) for usr_dir in os.listdir(corpus_dir) if os.path.isdir(os.path.join(corpus_dir, usr_dir))]\n",
    "    else:\n",
    "        usr_dirs = [os.path.join(corpus_dir, usr_dir) for corpus_dir in corpus_dirs for usr_dir in os.listdir(corpus_dir) if os.path.isdir(os.path.join(corpus_dir, usr_dir))]\n",
    "\n",
    "    if test_mode: usr_dirs = [usr_dirs[random.randint(0, len(usr_dirs)-1)] for _ in range(len(usr_dirs)//100)] \n",
    "    \n",
    "    usr2dirs = defaultdict(list)\n",
    "    for usr_dir in usr_dirs:\n",
    "        usr2dirs[os.path.basename(usr_dir)].append(usr_dir)\n",
    "    num_users = len(usr2dirs)\n",
    "\n",
    "    sys.stderr.write('Generating msg:time dictionary for %d directories at %s.\\n' % (len(usr_dirs), str(datetime.now())))\n",
    "    msg2time = {}\n",
    "    pool = multiprocessing.Pool(processes = num_cores)\n",
    "    results = []\n",
    "    for i, (usr, dirs) in enumerate(usr2dirs.items()):\n",
    "        results.append(pool.apply_async(reading_message_time, args=(i, num_users, dirs, usr, )))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for r in results:\n",
    "        d = r.get()\n",
    "        msg2time.update(d)\n",
    "    \n",
    "    usr_quarter2state = hr_df_quarterly['location'].to_dict()\n",
    "    states = hr_df_quarterly['location'].to_list()\n",
    "\n",
    "    #generating state timezone dictionary\n",
    "    state2timezone = {}\n",
    "    for state in set(states):\n",
    "        processed_state = state.strip()\n",
    "        if 'CA - ' in processed_state:\n",
    "            processed_state = 'CA'\n",
    "        elif 'Washington State' in processed_state:\n",
    "            processed_state = 'Washington'\n",
    "        us_state = us.states.lookup(processed_state)\n",
    "        if us_state:\n",
    "            # some states span more than one time zone; if so, choose the first one\n",
    "            state2timezone[state] = us_state.time_zones[0]\n",
    "        else:\n",
    "            print('{} is not a valid U.S. state.'.format(processed_state))\n",
    "    usr_quarter2timezone = {key:state2timezone[state] for key, state in usr_quarter2state.items()}\n",
    "    \n",
    "    sys.stderr.write('Reading all corpora to produce token and message counts at %s.\\n' % str(datetime.now()))\n",
    "    usr_quarter2counts = defaultdict(lambda : [0, 0, 0, 0, set(), set(), []])\n",
    "    pool = multiprocessing.Pool(processes = num_cores)\n",
    "    results = {}\n",
    "    for i, (usr, dirs) in enumerate(usr2dirs.items()):\n",
    "        results[usr] = pool.apply_async(reading_usr_counts, args=(i, num_users, dirs, usr, msg2time, usr_quarter2timezone, slice_type, ))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for usr, r in results.items():\n",
    "        quarterly2counts = r.get()\n",
    "        for quarter in sorted(quarterly2counts.keys()):\n",
    "            usr_quarter2counts[(usr, quarter)] = quarterly2counts[quarter]\n",
    "    for usr, quarter in list(usr_quarter2counts):\n",
    "        for peer in usr_quarter2counts[(usr, quarter)][5]:\n",
    "            usr_quarter2counts[(peer, quarter)][5].add(usr)\n",
    "\n",
    "    for usr, quarter in list(usr_quarter2counts):\n",
    "        peers = usr_quarter2counts[(usr, quarter)][5]\n",
    "        peer_num_messages_weekend = [usr_quarter2counts[(p, quarter)][2] for p in peers] + [usr_quarter2counts[(usr, quarter)][2]]\n",
    "        peer_num_messages_post_work = [usr_quarter2counts[(p, quarter)][3] for p in peers] + [usr_quarter2counts[(usr, quarter)][3]]\n",
    "        usr_working_weekends = usr_quarter2counts[(usr, quarter)][4] if type(usr_quarter2counts[(usr, quarter)][4]) == int else 0\n",
    "        peer_num_working_weekends = [usr_quarter2counts[(p, quarter)][4] if type(usr_quarter2counts[(p, quarter)][4]) == int else 0 for p in peers] + [usr_working_weekends]\n",
    "        usr_quarter2counts[(usr, quarter)].append(scale(peer_num_messages_weekend)[-1])\n",
    "        usr_quarter2counts[(usr, quarter)].append(scale(peer_num_messages_post_work)[-1])\n",
    "        usr_quarter2counts[(usr, quarter)].append(scale(peer_num_working_weekends)[-1])\n",
    "\n",
    "    cols = (['num_tokens', 'num_messages', 'num_messages_weekend', 'num_messages_post_work', 'num_working_weekends', 'network_peers', 'avg_response_time',\n",
    "    'peer_standardized_num_messages_weekend', 'peer_standardized_num_messages_post_work', 'peer_standardized_num_working_weekends'])\n",
    "    \n",
    "    if slice_type == 'quarterly':\n",
    "        usr_quarter2counts_df = dict_to_df(usr_quarter2counts, cols, index_name=['anon_id', quarter_colname])\n",
    "        usr_quarter2counts_df = usr_quarter2counts_df.join(hr_df_quarterly)\n",
    "    \n",
    "        #usr_quarter2counts_df['department_standardized_num_messages_weekend'] = usr_quarter2counts_df.groupby(['quarter', 'department'])['num_messages_weekend'].apply(scale)\n",
    "        usr_quarter2counts_df['department_standardized_num_messages_weekend'] = usr_quarter2counts_df.groupby(['quarter', 'department'])['num_messages_weekend'].transform(scale)\n",
    "        #usr_quarter2counts_df['department_standardized_num_messages_post_work'] = usr_quarter2counts_df.groupby(['quarter', 'department'])['num_messages_post_work'].apply(scale)\n",
    "        usr_quarter2counts_df['department_standardized_num_messages_post_work'] = usr_quarter2counts_df.groupby(['quarter', 'department'])['num_messages_post_work'].transform(scale)\n",
    "    \n",
    "        #usr_quarter2counts_df['num_working_weekends'] = usr_quarter2counts_df['num_working_weekends'].apply(lambda x : x if type(x) == int else 0)\n",
    "        usr_quarter2counts_df['num_working_weekends'] = usr_quarter2counts_df['num_working_weekends'].transform(lambda x: np.where(pd.api.types.is_integer_dtype(x), x, 0))\n",
    "    \n",
    "        #usr_quarter2counts_df['department_standardized_num_working_weekends'] = usr_quarter2counts_df.groupby(['quarter', 'department'])['num_working_weekends'].apply(scale)\n",
    "        usr_quarter2counts_df['department_standardized_num_working_weekends'] = usr_quarter2counts_df.groupby(['quarter', 'department'])['num_working_weekends'].transform(scale)\n",
    "    \n",
    "    else: #this is the case for halfyearly\n",
    "        usr_quarter2counts_df = dict_to_df(usr_quarter2counts, cols, index_name=['anon_id', halfyear_colname])\n",
    "        usr_quarter2counts_df = usr_quarter2counts_df.join(hr_df_quarterly)\n",
    "        usr_quarter2counts_df['department_standardized_num_messages_weekend'] = usr_quarter2counts_df.groupby(['halfyear', 'department'])['num_messages_weekend'].transform(scale)\n",
    "        usr_quarter2counts_df['department_standardized_num_messages_post_work'] = usr_quarter2counts_df.groupby(['halfyear', 'department'])['num_messages_post_work'].transform(scale)\n",
    "        usr_quarter2counts_df['num_working_weekends'] = usr_quarter2counts_df['num_working_weekends'].transform(lambda x: np.where(pd.api.types.is_integer_dtype(x), x, 0))\n",
    "        usr_quarter2counts_df['department_standardized_num_working_weekends'] = usr_quarter2counts_df.groupby(['halfyear', 'department'])['num_working_weekends'].transform(scale)\n",
    "    \n",
    "    return usr_quarter2counts_df\n",
    "\n",
    "#########################################################################\n",
    "#### Functions for Measuring Within-Person Similarities in Embeddings ###\n",
    "#########################################################################\n",
    "\n",
    "def embeddings_similarities(model):\n",
    "    \"\"\"\n",
    "    Returns the cosine similarity between i and we, and the cosine similarity between i-words and we-words\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : gensim.models.Word2Vec\n",
    "        Model that stores the embeddings for each word\n",
    "    Returns\n",
    "    -------\n",
    "    embeds : list\n",
    "        A list of cosine similarity between \"i\" and \"we\", i-words and we-words, and symmetric i-words and\n",
    "        we-words, in that order; an i-word is symmetric if and only if its equivalent we-word is in the model vocab\n",
    "    \"\"\"\n",
    "    embeds = ([word_similarity(model, hash_single_pronouns[0], hash_single_pronouns[1]),\n",
    "        word_similarity(model, hash_pronouns[i_index:we_index], hash_pronouns[we_index:])])\n",
    "    symmetric_i_words, symmetric_we_words = [], []\n",
    "    for i in range(len(hash_pronouns)-we_index):\n",
    "        #if hash_pronouns[i] in model.vocab and hash_pronouns[i+we_index] in model.vocab:\n",
    "        if hash_pronouns[i] in model.key_to_index and hash_pronouns[i+we_index] in model.key_to_index:\n",
    "            symmetric_i_words.append(hash_pronouns[i])\n",
    "            symmetric_we_words.append(hash_pronouns[i+we_index])\n",
    "    if len(symmetric_i_words) > 0:\n",
    "        embeds.append(model.n_similarity(symmetric_i_words, symmetric_we_words))\n",
    "    return embeds\n",
    "\n",
    "#load all the embeddings into the tmp directory with the appropriate extension\n",
    "def process_single_embedding_file(i, num_files, embeddings_dir, file):\n",
    "    \"\"\"\n",
    "    Reading from one embedding file\n",
    "    Parameters\n",
    "    ----------\n",
    "    i : int\n",
    "        Index used for progress tracking\n",
    "    num_files : int\n",
    "        Total number of files to process used for progress tracking\n",
    "    embedding_dir : str\n",
    "        Directory in which embedding files reside\n",
    "    file : str\n",
    "        Embedding file to open and process\n",
    "    Returns\n",
    "    -------\n",
    "    embeds : list\n",
    "        A list of cosine similarity between \"i\" and \"we\", i-words and we-words, and symmetric i-words and\n",
    "        we-words, in that order; an i-word is symmetric if and only if its equivalent we-word is in the model vocab\n",
    "    \"\"\"\n",
    "    mittens_file = os.path.join(embeddings_dir, file)\n",
    "    if i % 100 == 0:\n",
    "        sys.stderr.write(\"Processing \\t%d/%d -'%s', at %s.\\n\" % (i, num_files, mittens_file, datetime.now()))\n",
    "    # chopping off the file extension in filename\n",
    "    tmp_mittens = os.path.join(tmp_dir, file[0:-4] + \"_word2vec.txt\")\n",
    "    try:\n",
    "        word2vec_mittens_file = get_tmpfile(tmp_mittens)\n",
    "        glove2word2vec(mittens_file, word2vec_mittens_file)\n",
    "        model = KeyedVectors.load_word2vec_format(word2vec_mittens_file)\n",
    "        embeds = embeddings_similarities(model)\n",
    "        return embeds\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('File %s caused an error: %s.\\n' % (mittens_file, str(e)))\n",
    "\n",
    "def extract_usr_timekey(file):\n",
    "    \"\"\" \n",
    "    Extract timekey from file, with format hash_embedding_usr_timekey.txt\n",
    "    \"\"\"\n",
    "    file_chunks = file[0:-4].split('_') #obtain the name of the file until '.txt'\n",
    "    usr = file_chunks[2] #file is of the fome 'hash_embeddings_U0_2013HY1.txt'\n",
    "    time_key = file_chunks[3] if len(file_chunks) == 4 else None\n",
    "    return (usr, time_key)\n",
    "\n",
    "def self_similarities(files, num_files, embeddings_dir):\n",
    "    \"\"\"\n",
    "    Main workhorse function for calculating within-person similarities. Compares an individual's i embedding to we embedding, using both\n",
    "    i and we's embedding only and centroid of i-words and we-words. Indices used in this file relies on knowledge of the naming convention of underlying embedding files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list of str\n",
    "        Embedding files to process\n",
    "    num_files : int\n",
    "        Total number of files to process, used to keep track of progress\n",
    "    embeddings_dir : str\n",
    "        Directory in which embedding files reside\n",
    "    Returns\n",
    "    -------\n",
    "    usr_quarter2distances : dict\n",
    "        Dictionary mapping usr and quarter to within-person embedding similarities\n",
    "    \"\"\"\n",
    "    usr_quarter2distances = defaultdict(list)\n",
    "    usr_halfyear2distances = defaultdict(list)\n",
    "    pool = multiprocessing.Pool(processes = num_cores)\n",
    "    results = {}\n",
    "    for i, file in enumerate(files, 1):\n",
    "        usr, time_key = extract_usr_timekey(file) #usr is U0, U1 and so on, timekey is either 2013 or 2013Q1 or 2013HY1\n",
    "        if i % 100 == 0:\n",
    "            print('time_key')\n",
    "            print(time_key)\n",
    "        results[(usr, time_key)] = pool.apply_async(process_single_embedding_file, args=(i, num_files, embeddings_dir, file, ))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for key, r in results.items():\n",
    "        usr, time_key = key\n",
    "        curr_row = r.get()\n",
    "        # Empty if errored out\n",
    "        if curr_row:\n",
    "            if time_key and len(time_key) == 6: #ex. 2020Q1\n",
    "                    usr_quarter2distances[(usr, time_key)] = curr_row\n",
    "            elif time_key and len(time_key) == 7: #ex. 2020HY1\n",
    "                    usr_halfyear2distances[(usr, time_key)] = curr_row\n",
    "            else:\n",
    "                    sys.stderr.write('Embedding file format does not conform to expectations. Extracted time key %s for user %s.\\n' % (time_key, usr)) \n",
    "    return (usr_quarter2distances, usr_halfyear2distances)\n",
    "\n",
    "def reading_embeddings(embeddings_dir, test_mode=False):\n",
    "    \"\"\"\n",
    "    Calculates embedding similarities within-person and between-person\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_dir : str\n",
    "        Directory where all embedding files exist\n",
    "    test_mode : bool, optional\n",
    "        If testing, reduce number of files to process\n",
    "    Returns\n",
    "    -------\n",
    "    usr_quarter2distances_df : pd.DataFrame\n",
    "        Panel data at the quarter level that includes i vs we and i-words vs we-words cosine similarity \n",
    "    \"\"\"\n",
    "    files = os.listdir(embeddings_dir)\n",
    "    if test_mode: files = [files[random.randint(0, len(files)-1)] for _ in range(len(files)//50)]\n",
    "\n",
    "    num_files = len(files)\n",
    "\n",
    "    sys.stderr.write('Calculate within-person similarities for %d files at %s.\\n' % (num_files, str(datetime.now())))\n",
    "    usr_quarter2distances, usr_halfyear2distances = self_similarities(files, num_files, embeddings_dir)\n",
    "\n",
    "    cols = ['i_we', 'i_we_cluster', 'i_we_symmetric']\n",
    "    usr_quarter2distances_df = dict_to_df(usr_quarter2distances, cols, index_name=['anon_id', quarter_colname])\n",
    "    usr_halfyear2distances_df = dict_to_df(usr_halfyear2distances, cols, index_name=['anon_id', halfyear_colname])\n",
    "\n",
    "    return (usr_quarter2distances_df, usr_halfyear2distances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108ebeb-23a9-442e-aa90-91d28c7e3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "test = False\n",
    "for d in [output_dir, tmp_dir]:\n",
    "    if not os.path.exists(d):\n",
    "    \tos.mkdir(d)\n",
    "\n",
    "#for quarterly hr data\n",
    "sys.stderr.write('Reading quarterly HR data at %s.\\n' % datetime.now())           \n",
    "hr_df_quarterly = extract_hr_df(hr_filepath, quarter_colname)\n",
    "#for half yearly hr data \n",
    "sys.stderr.write('Reading halfyearly HR data at %s.\\n' % datetime.now())           \n",
    "hr_df_halfyearly = extract_hr_df(hr_filepath, halfyear_colname)\n",
    "sys.stderr.write('Reading corpus at %s.\\n' % str(datetime.now()))\n",
    "usr_quarter2counts_df = reading_raw_counts(corpus_dirs, first_batches, hr_df_quarterly, 'quarterly', test)\n",
    "sys.stderr.write('Reading corpus at %s.\\n' % str(datetime.now()))\n",
    "usr_halfyear2counts_df = reading_raw_counts(corpus_dirs, first_batches, hr_df_halfyearly, 'halfyearly', test)\n",
    "sys.stderr.write('Reading embeddings at %s.\\n' % datetime.now())\n",
    "usr_quarter2distances_df,usr_halfyear2distances_df = reading_embeddings(embeddings_dir, test)\n",
    "sys.stderr.write('Outputting dataframe at %s.\\n' % datetime.now())\n",
    "usr_quarter2counts_df.join(usr_quarter2distances_df).to_csv(quarterly_output_filename)\n",
    "usr_halfyear2counts_df.join(usr_halfyear2distances_df).to_csv(halfyearly_output_filename)\n",
    "\n",
    "sys.stderr.write(\"Finished outputting measures at %s, with a duration of %s.\\n\"\n",
    "    % (str(datetime.now()), str(datetime.now() - starttime)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
