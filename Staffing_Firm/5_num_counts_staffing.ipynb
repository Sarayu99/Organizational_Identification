{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8f2b86-ccaf-4cf2-a26a-7df7ee7efb4e",
   "metadata": {},
   "source": [
    "**Company** : <br>\n",
    "Staffing Firm\n",
    "\n",
    "**Notebook Function** : <br>\n",
    "    This notebook walks through the steps to calculate the number of 'i' and 'we' counts for the staffing firm corpus\n",
    "\n",
    "**Output File(s)** : <br>\n",
    "     - A folder containing the user embeddings for each time period\n",
    "\n",
    "**Author(s)** : <br>\n",
    "Lara Yang, Sarayu Anshuman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe12541-2ed7-4689-ba4f-338fc670599a",
   "metadata": {},
   "source": [
    "Install packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730badda-d357-4418-a060-bf15ed0e6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266c2b2-0546-4e35-8c09-521ced8a5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb6d3dd-3a70-4b0a-9eee-2d06dce66dce",
   "metadata": {},
   "source": [
    "Run helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce22263-68f8-491a-8175-bd210b0e23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "########### Helper Functions for Generating Mittens Embeddings ##########\n",
    "#########################################################################\n",
    "\n",
    "#this function returns a word, another word, and the value given in the cooccurance matrix based on the weighting function of the occurance of teh second word in the first word's context window\n",
    "def _window_based_iterator(toks, window_size, weighting_function):\n",
    "    for i, w in enumerate(toks):\n",
    "        yield w, w, 1\n",
    "        left = max([0, i-window_size])\n",
    "        for x in range(left, i):\n",
    "            yield w, toks[x],weighting_function(x)\n",
    "        right = min([i+1+window_size, len(toks)])\n",
    "        for x in range(i+1, right):\n",
    "            yield w, toks[x], weighting_function(x)\n",
    "    return\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    \"\"\"\n",
    "    Reads word vectors into a dictionary\n",
    "    Parameters\n",
    "    ----------\n",
    "    glove_filename : str\n",
    "        Name of file that contains vectors\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        A dictionary matching words to their vectors\n",
    "    \"\"\"\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE) #GloVe output files have the format one word per line, followed by its vector values separated by spaces\n",
    "        data = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader} #create a key (word): value (embedding) dict for every word\n",
    "    return data\n",
    "\n",
    "# Inspired by original build_weighted_matrix in utils.py in the Mittens paper source codebase\n",
    "def build_weighted_matrix(emails,\n",
    "        mincount=300, vocab_size=None, window_size=10,\n",
    "        weighting_function=lambda x: 1 / (x + 1),\n",
    "        email_type='internal'):\n",
    "    \"\"\"\n",
    "    Builds a count matrix based on a co-occurrence window of\n",
    "    `window_size` elements before and `window_size` elements after the\n",
    "    focal word, where the counts are weighted based on proximity to the\n",
    "    focal word.\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dicts\n",
    "        Emails converted from JSON formats\n",
    "    mincount : int\n",
    "        Only words with at least this many tokens will be included. #this means only words with atleast 300 occurances in teh document are included\n",
    "    vocab_size : int or None\n",
    "        If this is an int above 0, then, the top `vocab_size` words\n",
    "        by frequency are included in the matrix, and `mincount`\n",
    "        is ignored.\n",
    "    window_size : int\n",
    "        Size of the window before and after. (So the total window size\n",
    "        is 2 times this value, with the focal word at the center.)\n",
    "    weighting_function : function from ints to floats\n",
    "        How to weight counts based on distance. The default is 1/d\n",
    "        where d is the distance in words.\n",
    "    email_type : str, optional\n",
    "        Specifies which types of emails to include when building embeddings\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Cooccurence matrix guaranteed to be symmetric because of the way the counts are collected.\n",
    "    \"\"\"\n",
    "    wc = defaultdict(int)\n",
    "    #corpus contains a list  of emails, where each email is a dict\n",
    "    corpus = read_corpus(emails, email_type=email_type, sentence_delim=False)\n",
    "    if corpus is None:\n",
    "        print(\"These emails are empty\\t{}.\\nEmpty corpus returned for email type {}\".format(str(emails), email_type))\n",
    "        return pd.DataFrame()\n",
    "    for toks in corpus:\n",
    "        for tok in toks:\n",
    "            wc[tok] += 1 #word count\n",
    "    #now create the vocabulary\n",
    "    if vocab_size: #if a vocab size is defined then take the first top 'vocab_size' counts\n",
    "        srt = sorted(wc.items(), key=itemgetter(1), reverse=True)\n",
    "        vocab_set = {w for w, c in srt[: vocab_size]} #sort all the words in the vocabulary according to count\n",
    "    else: #define a vocab based on all those words which have a count greater than 'mincount'\n",
    "        vocab_set = {w for w, c in wc.items() if c >= mincount}\n",
    "    vocab = sorted(vocab_set)\n",
    "    n_words = len(vocab) #length of vocab\n",
    "    # Weighted counts:\n",
    "    counts = defaultdict(float)\n",
    "    for toks in corpus: #for each email\n",
    "        window_iter = _window_based_iterator(toks, window_size, weighting_function)\n",
    "        for w, w_c, val in window_iter:\n",
    "            if w in vocab_set and w_c in vocab_set:\n",
    "                counts[(w, w_c)] += val\n",
    "\n",
    "    '''\n",
    "    For each sentence (list of words), it uses a helper function _window_based_iterator (not provided) to generate a sequence of tuples:\n",
    "    The first element is the focal word.\n",
    "    The second element is a co-occurring word within the window.\n",
    "    The third element is the weight for the co-occurrence based on the distance between the words (using the provided weighting_function).\n",
    "    It checks if both the focal word and co-occurring word are in the vocabulary (vocab_set). If so, it updates the counts dictionary with the weighted co-occurrence for that word pair.\n",
    "    '''\n",
    "    X = np.zeros((n_words, n_words))\n",
    "    for i, w1 in enumerate(vocab):\n",
    "        for j, w2 in enumerate(vocab):\n",
    "            X[i, j] = counts[(w1, w2)]\n",
    "    X = pd.DataFrame(X, columns=vocab, index=pd.Index(vocab))\n",
    "    return X\n",
    "\n",
    "def read_corpus(emails, email_type, sentence_delim=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dict\n",
    "        A list of emails converted from JSON formats\n",
    "    email_type : str\n",
    "        Specifies which types of emails to include when building embeddings\n",
    "        'internal' filters for internal emails, 'external' filters for external and mixed emails, \n",
    "        and anything else does not filter\n",
    "    sentence_delim : bool, optional\n",
    "        If true, co-occurrences across sentence boundaries are ignored.\n",
    "    Returns\n",
    "    -------\n",
    "    list of list of str\n",
    "        Corpus converted from emails.\n",
    "        If sentence_delim is false, returns a list of emails, which are represented as lists of tokens\n",
    "        If sentence_delim is true, returns a list of sentences, which are represented as lists of tokens\n",
    "    \"\"\"\n",
    "    # split with no argument splits on whitespaces and newlines\n",
    "    if not sentence_delim:\n",
    "        if email_type == 'internal':\n",
    "            return [email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails if (\n",
    "                email['email_type'] == 'int')]\n",
    "        elif email_type == 'external':\n",
    "            return [email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails if (\n",
    "                email['email_type'] == 'ext' or email['email_type'] == 'mixed')]\n",
    "        # agnostic to email type\n",
    "        else:\n",
    "            return [email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails]\n",
    "    else:\n",
    "        if email_type == 'internal':\n",
    "            return [sent.strip().split() for email in emails for line in email['hb'].split('\\n') for sent in line.split('SENT_END') if (\n",
    "                len(sent) > 0 and email_type == 'int')]\n",
    "        elif email_type == 'external':\n",
    "            return [sent.strip().split() for email in emails for line in email['hb'].split('\\n') for sent in line.split('SENT_END') if (\n",
    "                len(sent) > 0 and (email_type == 'int' or email['email_type'] == 'mixed'))]\n",
    "        else:\n",
    "            return [sent.strip().split() for email in emails for line in email['hb'].split('\\n') for sent in line.split('SENT_END') if len(sent) > 0]\n",
    "\n",
    "def output_embeddings(mittens_df, filename, compress=False):\n",
    "    if compress:\n",
    "        mittens_df.to_csv(filename + '.gz', quoting=csv.QUOTE_NONE, header=False, sep=\" \", compression='gzip')\n",
    "    else:\n",
    "        mittens_df.to_csv(filename, quoting=csv.QUOTE_NONE, header=False, sep=\" \")\n",
    "    return\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with JSON Emails #############\n",
    "#########################################################################\n",
    "# Slightly different from get_recipients for spacespace emails\n",
    "def get_recipients(msg):\n",
    "    \"\"\"\n",
    "    Return a set of recipients of the current message.\n",
    "    self is removed from list of recipients if in recipients #(-set([sender]))\n",
    "    All fields contain email addresses, not user IDs. From fields are visually just strings\n",
    "    but checking just in case\n",
    "    \"\"\"\n",
    "    sender = msg['From'][0] if type(msg['From']) == list else msg['From']\n",
    "    return set(msg.get('To', []) + msg.get('Cc', []) + msg.get('Bcc', [])) - set([sender]) #basically return a set of recipients to the current email\n",
    "\n",
    "def slice_user_corpus(emails, train_mode): #slices a list of emails into chunks based on time periods specified by the train_mode parameter\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dict\n",
    "        A list of emails converted from JSON formats to dictionaries \n",
    "    train_mode : str\n",
    "        One of 'annual', 'quarterly', 'all'\n",
    "        Indicates how to chunk up emails - into quarters, years, or both\n",
    "    Returns\n",
    "    -------\n",
    "    timekey2emails : dict\n",
    "        Matches quarters or years to respective emails\n",
    "    \"\"\"\n",
    "    #'ActivityCreatedAt' contains a timestamp for the list of emails\n",
    "    timekey2emails = defaultdict(list)\n",
    "    #the function iterates through each email in the email list\n",
    "    for email in emails:\n",
    "        if train_mode == 'annual':\n",
    "            timekey2emails[to_year(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "        elif train_mode == 'quarterly':\n",
    "            timekey2emails[to_quarter(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "        elif train_mode == 'halfyear':\n",
    "            timekey2emails[to_halfyear(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "        elif train_mode == 'all':\n",
    "            timekey2emails[to_year(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "            timekey2emails[to_quarter(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "            timekey2emails[to_halfyear(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "    return timekey2emails\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with Date Objects ############\n",
    "#########################################################################\n",
    "\n",
    "def to_quarter(date, format):\n",
    "    \"\"\"\n",
    "    Return quarter of date in string\n",
    "    \"\"\"\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]    \n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    quarter = ((int(month)-1) // 3) + 1\n",
    "    timekey = str(year) + 'Q' + str(quarter)\n",
    "    return timekey\n",
    "\n",
    "def to_halfyear(date, format):\n",
    "    \"\"\"\n",
    "    Return half year of date in string\n",
    "    \"\"\"\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]    \n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    halfyear = ((int(month)-1) // 6) + 1\n",
    "    timekey = str(year) + 'HY' + str(halfyear)\n",
    "    return timekey\n",
    "\n",
    "def to_year(date, format):\n",
    "    \"\"\"\n",
    "    Return year of date in string\n",
    "    \"\"\"\n",
    "    if format == 'str':\n",
    "        return date[0:4]\n",
    "    elif format == 'datetime':\n",
    "        return str(date.year)\n",
    "\n",
    "def datetime_to_timekey(date, time_key):\n",
    "    if time_key == 'year':\n",
    "        return to_year(date, format='datetime')\n",
    "    elif time_key == 'quarter':\n",
    "        return to_quarter(date, format='datetime')\n",
    "\n",
    "def is_month_before_equal(datetime1, datetime2):\n",
    "    if datetime1.year < datetime2.year:\n",
    "        return 1\n",
    "    elif (datetime1.year == datetime2.year) and (datetime1.month <= datetime2.month):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def num_months_between_dates(datetime1, datetime2):\n",
    "    return abs((datetime1.year - datetime2.year) * 12 + datetime1.month - datetime2.month)\n",
    "\n",
    "def num_quarters_between_dates(datetime1, datetime2):\n",
    "    return abs((datetime1.year - datetime2.year) * 12 + datetime1.month - datetime2.month) // 3\n",
    "\n",
    "def num_years_between_dates(datetime1, datetime2):\n",
    "    return abs(datetime1.year - datetime2.year)\n",
    "\n",
    "def time_between_dates(datetime1, datetime2, time_key):\n",
    "    if time_key == 'monthly':\n",
    "        return num_months_between_dates(datetime1, datetime2)\n",
    "    elif time_key == 'quarterly':\n",
    "        return num_quarters_between_dates(datetime1, datetime2)\n",
    "    elif time_key == 'annual':\n",
    "        return num_years_between_dates(datetime1, datetime2)\n",
    "\n",
    "#########################################################################\n",
    "############## Helper Functions for Working with Dataframes #############\n",
    "#########################################################################\n",
    "\n",
    "#function to convert a dictionary to a dataframe\n",
    "def dict_to_df(index2rows, cols, index_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    index2rows : dict\n",
    "        Dictionary mapping index to rows to be coverted\n",
    "    cols : list\n",
    "        List of column names of type str\n",
    "    index : list\n",
    "        List of index names\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Constructed dataframe\n",
    "    \"\"\"\n",
    "    if index2rows is None or len(index2rows) == 0:\n",
    "        return None\n",
    "    #case where the dataframe is formed across entire time span of user emails\n",
    "    if len(index_name) == 1:\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df.index.name = index_name[0]\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "    else: #case where the user token (word) counts and email counts are formed for ecah year or each quarter\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df = pd.DataFrame(df, pd.MultiIndex.from_tuples(df.index, names=index_name))\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "#########################################################################\n",
    "########### Helper Functions for Working with Embedding Output ##########\n",
    "#########################################################################\n",
    "\n",
    "def remove_empty_embeddings(embeddings_dir):\n",
    "    \"\"\"\n",
    "    Removes all empty files in embeddings_dir that were produced when vocab size was 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_dir : str\n",
    "        Full path to directory where embedding files are located\n",
    "    \"\"\"\n",
    "    for file in os.listdir(embeddings_dir):\n",
    "        mittens_file = os.path.join(embeddings_dir, file)\n",
    "        if os.path.getsize(mittens_file) == 0:\n",
    "            os.remove(mittens_file)\n",
    "    return\n",
    "\n",
    "def extract_company_embedding(company_embeddings_filename, tmp_dir, words):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_embeddings_filename : str\n",
    "        File path of the company embeddings\n",
    "    tmp_dir : str\n",
    "        Path to the directory for gensim to output its tmp files in order to load embeddings into word2vec format\n",
    "    words : list\n",
    "        A list of strings for which to retrieve vectors\n",
    "    Returns\n",
    "    -------\n",
    "    vecs : list\n",
    "        A list of vectors of type numpy.ndarray that correspond to the list of words given as parameters\n",
    "    \"\"\" \n",
    "    tmp_mittens = os.path.join(tmp_dir, \"mittens_embeddings_all_word2vec.txt\")\n",
    "    word2vec_mittens_file = get_tmpfile(tmp_mittens)\n",
    "    glove2word2vec(company_embeddings_filename, word2vec_mittens_file) #load the embeddings in the glove2word2vec format\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_mittens_file) #this model represents the glove embddings space using the default company embeddings\n",
    "    vecs = []\n",
    "    '''\n",
    "    for w in words:\n",
    "        if w in model.vocab:\n",
    "            vecs.append(model.wv[w]) #default vectors for the words in the vocab\n",
    "        else:\n",
    "            vecs.append(np.nan)\n",
    "            print('%s not in company embeddings' % w)\n",
    "    '''\n",
    "    for w in words:\n",
    "     try:\n",
    "      index = model.key_to_index[w]\n",
    "      vecs.append(model.vectors[index])\n",
    "     except KeyError:\n",
    "      vecs.append(np.nan)\n",
    "      print('%s not in company embeddings' % w)\n",
    "    return vecs\n",
    "\n",
    "def word_similarity(model, w1, w2):\n",
    "    \"\"\"\n",
    "    This is an auxilary function that allows for comparing one word to another word or multiple words\n",
    "    If w1 and w2 are both single words, n_similarity returns their cosine similarity which is the same as \n",
    "    simply calling similarity(w1, w2)\n",
    "    If w1 or w2 is a set of words, n_similarity essentially takes the mean of the set of words and then computes\n",
    "    the cosine similarity between that vector mean and the other vector. This functionality is both reflected\n",
    "    in its source code and has been verified manually.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : KeyedVectors\n",
    "        The model that contains all the words and vectors\n",
    "    w1 : str or list\n",
    "        The first word or word list to be compared\n",
    "    w2 : str or list\n",
    "        The second word or word list to be compared\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between w1 and w2\n",
    "    \"\"\"\n",
    "    #first fo the case where w1 and w2 parameters are single words\n",
    "    if not isinstance(w1, list): \n",
    "        w1 = [w1]\n",
    "    if not isinstance(w2, list):\n",
    "        w2 = [w2]\n",
    "    #create a copy of the input lists\n",
    "    '''\n",
    "    w1 = [w for w in w1 if w in model.vocab]\n",
    "    w2 = [w for w in w2 if w in model.vocab]\n",
    "    '''\n",
    "    w1 = [w for w in w1 if w in model.key_to_index]\n",
    "    w2 = [w for w in w2 if w in model.key_to_index]\n",
    "    if len(w1) == 0 or len(w2) == 0:\n",
    "        return None\n",
    "    return model.n_similarity(w1, w2) #inbuilt word2vec model that calculates the cosine similarity between two lists; takes the mean of the set of words and then computes\n",
    "    #the cosine similarity between that vector mean and the other vector\n",
    "\n",
    "def cossim_with_none(vec1, vec2, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Auxiliary function that calls cossim function to test if vectors are None to prevent erroring out.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : list of (int, float), gensim sparse vector format\n",
    "    vec2 : list of (int, float), gensim sparse vector format\n",
    "    format : str, optional\n",
    "        Either sparse or dense. If sparse, vec1 and vec2 are in gensim sparse vector format; use cossim function from gensim.\n",
    "        Otherwise, vec1 and vec2 are numpy arrays and cosine similarity is hand calculated\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between vec1 and vec2\n",
    "    \"\"\"\n",
    "    if not (isnull_wrapper(vec1) or isnull_wrapper(vec2)):\n",
    "        if vec_format == 'sparse':\n",
    "            return cossim(vec1, vec2)\n",
    "        elif vec_format == 'dense':\n",
    "            if len(vec1) == 0 or len(vec2) == 0:\n",
    "                return None\n",
    "            return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        else:\n",
    "            raise ValueError()\n",
    "    return None\n",
    "\n",
    "def calculate_pairwise_cossim(col1, col2=None, reference=False, reference_group=None, anon_ids=None, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Calculates averaged cosine similarity of every row vector in col1 with every other row vector in col2.\n",
    "    If no col2 is provided, cosine similarity of every row vector with every other row vector in col1 is calculated.\n",
    "    The two columns should have equal length.\n",
    "    Parameters\n",
    "    ----------\n",
    "    col1 : pd.Series\n",
    "        A column where each row is a sparse word vector (BoW format that gensim code is written for).\n",
    "    col2 : pd.Series, optional\n",
    "        A column where each row is a sparse word vector (BoW format that gensim code is written for).\n",
    "    reference : bool, optional\n",
    "        Indicator variable for whether filtering for reference groups is needed\n",
    "    reference_group : pd.Series, optional\n",
    "        If filtering for reference groups, a list containing reference group members for every employee in col1\n",
    "    anon_ids : pd.Series, optional\n",
    "        If filtering for reference groups, a list containing anon_ids for every employee in col1\n",
    "    Returns\n",
    "    -------\n",
    "    results : list\n",
    "        A list where the ith element is the averaged cosine similarity between the ith vector in col1 and every vector\n",
    "        in col2 for which i != j. If no col2 is provided, a list where the ith element is the averaged cosine similarity\n",
    "        between the ith vector in col1 and every other vector in col1 is returned.\n",
    "    \"\"\"\n",
    "    vectors1 = col1.tolist()\n",
    "    vectors2 = col2.tolist() if col2 is not None else col1.tolist()\n",
    "    reference_group = reference_group.tolist() if reference else None\n",
    "    anon_ids = anon_ids.tolist() if anon_ids is not None else None\n",
    "    results = list()\n",
    "    for i in range(len(vectors1)):\n",
    "        total_sim = []\n",
    "        if not isnull_wrapper(vectors1[i]):\n",
    "            for j in range(len(vectors2)):\n",
    "                if i != j and not isnull_wrapper(vectors2[j]):\n",
    "                    # filter out any np.nans as our reference group\n",
    "                    if not reference or (type(reference_group[i]) == set and anon_ids[j] in reference_group[i]):\n",
    "                        if vec_format == 'sparse':\n",
    "                            total_sim.append(cossim(vectors1[i], vectors2[j]))\n",
    "                        elif vec_format == 'dense':\n",
    "                            total_sim.append(np.dot(vectors1[i], vectors2[j])/(np.linalg.norm(vectors1[i]) * np.linalg.norm(vectors2[j])))\n",
    "                        else:\n",
    "                            raise ValueError()\n",
    "            results.append(mean(total_sim) if len(total_sim) > 0 else None)\n",
    "        else:\n",
    "            results.append(None)\n",
    "    return results\n",
    "\n",
    "def isnull_wrapper(x):\n",
    "    r = pd.isnull(x)\n",
    "    if type(r) == bool:\n",
    "        return r\n",
    "    return r.any()\n",
    "\n",
    "def vector_mean(col):\n",
    "    \"\"\"\n",
    "    Calculate vector means of row vectors\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : pd.Series\n",
    "        The column to be averaged\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        A vector that is the numerical average of all vectors in col\n",
    "    \"\"\"\n",
    "    return np.array(col[col.notna()].tolist()).mean(axis=0)\n",
    "\n",
    "def extract_hr_survey_df(survey_hr_file, user_qualtrics_file, users_file, perf_likert_file, perf_percentage_file):\n",
    "    \"\"\"\n",
    "    Loads various datasets to prepare survey and hr data for merging with embeddings df.\n",
    "    Returns\n",
    "    -------\n",
    "    survey_hr_df : pd.DataFrame\n",
    "        Dataframe indexed by user_id\n",
    "    \"\"\"\n",
    "    perf_likert_df = pd.read_csv(perf_likert_file)\n",
    "    perf_percentage_df = pd.read_csv(perf_percentage_file)\n",
    "    perf_likert_df = perf_likert_df[['UID', '2019 Perf_Type(Rating)', '2020 Perf_Type(Rating)']]\n",
    "    perf_percentage_df = perf_percentage_df[['UID', '2019_Perf_Type(Percentage)', '2020_Perf_Type(Percentage)']]\n",
    "    perf_likert_df.columns = [\"UID\", \"perf_rating_2019\", \"perf_rating_2020\"]\n",
    "    perf_percentage_df.columns = [\"UID\", \"perf_percentage_2019\", \"perf_percentage_2020\"]\n",
    "    \n",
    "    user_qualtrics_df = pd.read_csv(user_qualtrics_file)\n",
    "    user_qualtrics_df = user_qualtrics_df.merge(perf_likert_df, on='UID', how='left').merge(perf_percentage_df, on=\"UID\", how='left')\n",
    "\n",
    "    survey_hr_df = pd.read_csv(survey_hr_file)\n",
    "    survey_hr_df = survey_hr_df.merge(user_qualtrics_df, left_on='uid', right_on='UID', how='left')\n",
    "    # we lose two employees whose emails are not included in the crawled email data\n",
    "    email2uid = {}\n",
    "    with open(users_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            user = json.loads(line)\n",
    "            for e in user['Emails']:\n",
    "                email2uid[e] = user['UserId']\n",
    "\n",
    "    survey_hr_df = survey_hr_df[survey_hr_df['Email'].isin(email2uid.keys())]\n",
    "    survey_hr_df['user_id'] = survey_hr_df['Email'].apply(lambda e : email2uid[e])\n",
    "    survey_hr_df.set_index('user_id', inplace=True)\n",
    "    return survey_hr_df\n",
    "\n",
    "def extract_variables_from_file(file):\n",
    "    \"\"\" \n",
    "    Extract relevant information from name of embedding file, with format: {}(_{})?_(internal|external).txt\n",
    "    \"\"\"\n",
    "    file_chunks = file[0:-4].split('_') #split the file name into 3 parts\n",
    "    usr = file_chunks[0] #the first part contains the userID\n",
    "    time_key = file_chunks[1] if len(file_chunks) == 3 else None #this part contains the year - like 2020, or 2020Q1, or 2020HY1\n",
    "    return (usr, time_key)\n",
    "\n",
    "def project(word, dimension):\n",
    "    \"\"\"\n",
    "    Returns the scalar projection of word on dimension. Word and dimension are both assumed to be vectors\n",
    "    \"\"\"\n",
    "    return np.dot(word, dimension)/ np.linalg.norm(dimension)\n",
    "\n",
    "def drop(u, v):\n",
    "    \"\"\"\n",
    "    Returns the component in u that is orthogonal to v, also known as the vector rejection of u from v\n",
    "    \"\"\"\n",
    "    return u - v * u.dot(v) / v.dot(v)\n",
    "\n",
    "def remove_frequency(company_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Remove frequency dimension from company embeddings (assumed to be the top dimension)\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_embeddings : dict\n",
    "        A dictionary mapping words to their embeddings\n",
    "    embedding_dim : int\n",
    "        The dimension of word vectors. Used to determine the number of components that go into PCA.\n",
    "    Returns\n",
    "    -------\n",
    "    new_company_embeddings : dict\n",
    "        A dictionary mapping words to their embeddings, where embeddings are demeaned and first PCA\n",
    "        dimension hypothesized to represent frequency dimension is removed\n",
    "    \"\"\"\n",
    "    vectors = np.array([v for k, v in company_embeddings.items()])\n",
    "    miu = np.mean(vectors, axis=0)\n",
    "    demeaned_vectors = vectors - miu\n",
    "    pca = PCA(n_components = embedding_dim)\n",
    "    pca.fit(demeaned_vectors)\n",
    "    frequency_dim = pca.components_[0]\n",
    "    new_company_embeddings = {k : drop(v-miu, frequency_dim) for k, v in company_embeddings.items()}\n",
    "    return new_company_embeddings\n",
    "\n",
    "def doPCA(words_start, words_end):\n",
    "    \"\"\"\n",
    "    Performs PCA on differences between pairs of words and returns the first component\n",
    "    Based on function doPCA in Bolukbasi et al. (2016) source code at https://github.com/tolga-b/debiaswe/blob/master/debiaswe/we.py\n",
    "    Parameter\n",
    "    ---------\n",
    "    words_start : list\n",
    "        List of hashed words at one end of interested dimension\n",
    "    words_end: list\n",
    "        List of hashed words at the other end of dimension\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        First component of PCA of differences between pairs of words\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    for i in range(len(words_start)):\n",
    "        center = (words_start[i] + words_end[i])/2\n",
    "        matrix.append(words_end[i] - center)\n",
    "        matrix.append(words_start[i] - center)\n",
    "    matrix = np.array(matrix)\n",
    "    # cannot have more components than the number of samples\n",
    "    num_components = len(words_start)*2\n",
    "    pca = PCA(n_components = num_components)\n",
    "    pca.fit(matrix)\n",
    "    return pca.components_[0]\n",
    "\n",
    "def build_dimension(words_start, words_end):\n",
    "    \"\"\"\n",
    "    This method builds a dimension defined by words at separate end of a dimension.\n",
    "    Multiple methods exist in previous literature when building such a dimension.\n",
    "    1) Kozlowski et al. (2019) averages across differences between different word pairs, noted to be interchangeable with averaging words on each side of the dimension and\n",
    "    then taking the difference between averages. They are empirically verified to be identical.\n",
    "    2) Bolukbasi et al. (2016) defines gender direction using a simple difference between man and woman in the corresponding tutorial. In the same tutorial, \n",
    "    racial direction is defined as difference between two clusters of words that are each sum of the embeddings of its corresponding dimensions\n",
    "    normalized by the L2 norm. Wang et al. (2020) note that normalization is unnecessary. If unnormalized, this method should be equivalent to #3.\n",
    "    3) Bolukbasi et al. (2016) defines gender direction also by taking the differences across multiple pairs, doing PCA on these differences, and \n",
    "    taking the first component as the gender direction.\n",
    "    Parameter\n",
    "    ---------\n",
    "    words_start : list\n",
    "        List of hashed words at the positive end of the dimension, where positive implies more likely to affect identification positively\n",
    "    words_end: list\n",
    "        List of hashed words at the other end of dimension\n",
    "    Returns\n",
    "    -------\n",
    "    (mean_dim, pca_dimension) : 2-tuple of numpy vector\n",
    "        Two vector that represents the dimension of interest calculated using method #1 and #3.\n",
    "    \"\"\"\n",
    "    assert len(words_start) == len(words_end)\n",
    "    differences = [(np.array(words_start[i]) - np.array(words_end[i])) for i in range(len(words_start)) if not np.isnan(words_start[i]).any() and not np.isnan(words_end[i]).any()]\n",
    "    mean_dim = np.array(differences).mean(axis=0)\n",
    "    pca_dim = doPCA(words_start, words_end)\n",
    "    if project(words_start[0], pca_dim) < 0:\n",
    "        # convention used in the current script is that words_start should represent the positive dimension\n",
    "        pca_dim = pca_dim * -1\n",
    "    return (mean_dim, pca_dim)\n",
    "\n",
    "#ending here-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861ef27-e2c2-45b5-a894-8070444bbad2",
   "metadata": {},
   "source": [
    "Print current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e918587-39cb-4433-8e55-53b29f22e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c2220-79db-4471-a868-5da72d0ad91e",
   "metadata": {},
   "source": [
    "Set hyperparameters and input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e7828-849a-43a6-821b-5c4f9c7870da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters\n",
    "embedding_dim = 50\n",
    "mincount = 50\n",
    "\n",
    "#set file paths\n",
    "home_dir = \"/zfs/projects/faculty/amirgo-identification/\"\n",
    "email_dir = os.path.join(home_dir, \"email_data/\")\n",
    "mittens_dir = os.path.join(home_dir, \"mittens\")\n",
    "utils_dir = os.path.join(mittens_dir, \"utils\")\n",
    "embeddings_dir = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/coco/fine-tuning/embeddings_high_prob_eng_08_50d_mincount50\"\n",
    "\n",
    "#set input files\n",
    "email_file = os.path.join(email_dir, 'MessagesHashed.jsonl')\n",
    "users_file = os.path.join(email_dir, 'Users.json')\n",
    "activity_file = os.path.join(email_dir, 'Activities.json')\n",
    "survey_dir = os.path.join(home_dir, \"survey_hr_data\")\n",
    "user_qualtrics_file = os.path.join(survey_dir, \"UsersQualtrics.csv\")\n",
    "perf_percentage = os.path.join(survey_dir, \"perf_rating_percentages.csv\")\n",
    "perf_likert = os.path.join(survey_dir, \"perf_rating_likert.csv\")\n",
    "\n",
    "analyses_data_dir = \"/zfs/projects/faculty/amirgo-transfer/spacespace/spacespace/staffing/analyses_data/\"\n",
    "survey_filename = os.path.join(analyses_data_dir, \"preprocessed_survey_hr.csv\")\n",
    "company_embeddings_filename = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/staffing/training/vectors_high_prob_eng_08_50d.txt\"\n",
    "\n",
    "tmp_dir = os.path.join(current_dir, \"tmp_num_counts\")\n",
    "output_dir = os.path.join(current_dir, \"coco_num_we_counts\")\n",
    "output_filename = os.path.join(output_dir, \"coco_num_counts.csv\")\n",
    "\n",
    "print(embeddings_dir)\n",
    "print(company_embeddings_filename)\n",
    "print(tmp_dir)\n",
    "print(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74649fd-77e5-46bd-a3e3-88266504aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hashes\n",
    "year_colname, quarter_colname = 'year', 'quarter'\n",
    "hash2word = {\n",
    "    '09f83385': 'mine', '20019fa4': 'i', '20b60145': 'us', '28969cb1': 'them', '3828d3d2': 'me', '4dd6d391': 'their', '5b4e27db': 'my',\n",
    "    '64a505fc': 'ourselves', '6935bb23': 'ours', '6f75419e': 'myself', '86df0c8d': 'themselves', 'a7383e72': 'we', 'a9193217': 'theirs', 'b72a9dd7': 'our', 'fd0ccf1c': 'they', \n",
    "    'ce696289': 'home', 'b95eb14b': 'attached', '267430a0': 'good', '294fa7d1': 'collabera', '974811d0': 'pay', 'edbf568e': 'work', 'b71be0e8': 'team', '4c088971': 'great',\n",
    "    'c74560f9': 'best', 'f18e6868': 'different', '1f4d7738': 'group', '255ddfcd': 'glad', 'aa829423': 'included', '17e1378b': 'money', '454ea538': 'salary', '311b8ad0': 'community',\n",
    "    '3b75b927': 'happy', '9324aa22': 'organized', '63b8b7ea': 'bad', '643ce56f': 'responsive', 'f4732b84': 'enthusiastic', '2e32c475': 'competitive', 'b9625ccf': 'family',\n",
    "    '900c73ff': 'unresponsive', 'cfe1bd08': 'income', '223deabb': 'worst', 'fa81b32a': 'pride', '1455e3bd': 'passionate', '9582e03b': 'awful', 'd9f0fe6c': 'promotion',\n",
    "    'c40b5da1': 'excluded', 'cf9cb85a': 'ambitious', 'a0cb3a2b': 'sad', '8a4e04bd': 'honor', 'cafaa726': 'belong', '24cb6fe3': 'shame', 'b92208fc': 'disciplined', '68e0c9c9': 'undisciplined',\n",
    "    '81bcf2f5': 'receptive', '8ca67680': 'disorganized', 'd22e4710': 'bitter', 'bf4db4c4': 'unenthusiastic', '8602bd25': 'dignity', '822f792d': 'detached', 'a7ca40f1': 'humiliation',\n",
    "    '7911da73': 'noncompetitive', '627fcac3': 'dishonor', '84cadff4': 'unreceptive', '07ca39d6': 'lazy', '95a160e0': 'indifferent', '10a4d7ee': 'apathetic'}\n",
    "word2hash = {v:k for k, v in hash2word.items()}\n",
    "pronouns = ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves']\n",
    "single_pronouns = ['i', 'we']\n",
    "i_index, we_index = 0, 5\n",
    "hash_pronouns = [word2hash[p] for p in pronouns]\n",
    "hash_single_pronouns = [word2hash[p] for p in single_pronouns]\n",
    "\n",
    "file_name_re = re.compile(\"5([a-z0-9]+)_(2020(Q3)?_)?internal.txt\")\n",
    "num_cores = 12\n",
    "\n",
    "domain_hash = {\n",
    "    'collabera.com':                     '509c8f6b1127bceefd418c023533d653',\n",
    "    'collaberainc.mail.onmicrosoft.com': 'ec5b67548b6ec06f1234d198efec741e',\n",
    "    'collaberainc.onmicrosoft.com':      '86160680578ee9258f097a67a5f25af9',\n",
    "    'collaberasp.com':                   '6bf3934d19f1acf5b9295b63e0e7f66e',\n",
    "    'g-c-i.com':                         '3444d1f7d5e46443080f2d069e41a10c'}\n",
    "collabera_hashes = set([v for k, v in domain_hash.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109fd696-cf52-40fe-8bbc-fdfd02424966",
   "metadata": {},
   "source": [
    "Run a function to obatin the counts of the words 'i' and 'we'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fbdc5-8229-402b-8255-d4fff21bb3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_email(activity_file, email_file, name2dim={}, test_mode=False):\n",
    "    \"\"\"\n",
    "    The main workhorse function for obtaining word frequency counts that might relevant for identification\n",
    "    as well as computing average projection score of global vectors in raw emails.\n",
    "    Parameters\n",
    "    ----------\n",
    "    activity_file : str\n",
    "        The full filepath that contains all email metadata, where each line is a JSON object that represents one email\n",
    "    email_file : str\n",
    "        The full filepath that contains all email content, where each line is a JSON object that represents one email\n",
    "    name2dims : dict\n",
    "        Dictionary matching dimension names to actual 2-tuples of dimensions\n",
    "    test_mode: bool\n",
    "        If true, run a small number of files\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of user-level, annual, and quarterly dataframes\n",
    "    \"\"\"\n",
    "    usr2counts, usr_year2counts, usr_quarter2counts = defaultdict(lambda : [0] * num_cols), defaultdict(lambda : [0] * num_cols), defaultdict(lambda :[0] * num_cols)\n",
    "    sid2activity = {}\n",
    "    num_cols = 2 \n",
    "    #dim_i_word2proj = {}\n",
    "    with open(activity_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            activity = json.loads(line)\n",
    "            sid2activity[activity['MailSummarySid']] = activity\n",
    "\n",
    "    with open(email_file, encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if test_mode and i > 10000: break\n",
    "            if (i+1) % 100000 == 0:\n",
    "                sys.stderr.write('Processing email #%d.\\n' % (i+1))\n",
    "            email = json.loads(line)\n",
    "            lang = email['l']\n",
    "            if len(email['hb']) > 0 and lang[0] == \"__label__en\" and lang[1] > 0.8:\n",
    "                activity = sid2activity[email['sid']]\n",
    "                recipients = get_recipients(activity)\n",
    "                pure_internal = True\n",
    "                for r in recipients:\n",
    "                    domain = r.split('@')[1]\n",
    "                    if domain not in collabera_hashes:\n",
    "                        pure_internal = False\n",
    "                        break\n",
    "                if not pure_internal:\n",
    "                    continue\n",
    "                user = activity['UserId']\n",
    "                year = to_year(activity['ActivityCreatedAt'], format='str')\n",
    "                quarter = to_quarter(activity['ActivityCreatedAt'], format='str')\n",
    "                toks = email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split()\n",
    "                tok2count = Counter(email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split())\n",
    "                i_counts = sum([tok2count[h] for h in hash_pronouns[i_index:we_index]])\n",
    "                we_counts = sum([tok2count[h] for h in hash_pronouns[we_index:]])\n",
    "\n",
    "                row = [i_counts, we_counts]\n",
    "                for col in range(num_cols):\n",
    "                    usr2counts[user][col] += row[col]\n",
    "                    usr_year2counts[(user, year)][col] += row[col]\n",
    "                    usr_quarter2counts[(user, quarter)][col] += row[col]\n",
    "    cols = ['num_i_words', 'num_we_words']\n",
    "\n",
    "    usr2counts_df = dict_to_df(usr2counts, cols, index_name=['user_id'])\n",
    "    usr_year2counts_df = dict_to_df(usr_year2counts, cols, index_name=['user_id', year_colname])\n",
    "    usr_quarter2counts_df = dict_to_df(usr_quarter2counts, cols, index_name=['user_id', quarter_colname])\n",
    "    return (usr2counts_df, usr_year2counts_df, usr_quarter2counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d742ad-f28c-493e-9a8c-5e8edb5a457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.now()\n",
    "test = False\n",
    "for d in [output_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)\n",
    "\n",
    "sys.stderr.write('Loading corpus counts at %s.\\n' % datetime.now())\n",
    "usr2counts, usr2annual_counts, usr2quarterly_counts = read_raw_email(activity_file, email_file, name2dim={}, test_mode=False)\n",
    "\n",
    "# different embedding files should be matched with the same hr file as hr data is not in panel format\n",
    "sys.stderr.write('Reading HR and Survey data at %s.\\n' % datetime.now())\n",
    "hr_df = extract_hr_survey_df(survey_filename, user_qualtrics_file, users_file, perf_likert, perf_percentage)\n",
    "\n",
    "sys.stderr.write('Outputting dataframe at %s.\\n' % datetime.now())\n",
    "if usr2quarterly_measures is not None: hr_df.join(usr2quarterly_measures).join(usr2quarterly_counts).to_csv(output_filename)\n",
    "    \n",
    "sys.stderr.write(\"Finished outputting measures at %s, with a duration of %s.\\n\"\n",
    "    % (str(datetime.now()), str(datetime.now() - starttime)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
