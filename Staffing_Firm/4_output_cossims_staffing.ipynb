{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ca54fd-c361-43e2-9fd2-1aed9818d4bb",
   "metadata": {},
   "source": [
    "**Company** : <br>\n",
    "Staffing Firm\n",
    "\n",
    "**Notebook Function** : <br>\n",
    "    This notebook processes the identification measures\n",
    "\n",
    "**Output File(s)** : <br>\n",
    "    embeddings_high_prob_eng_50_quarterly_50d_mincount{}_v2.csv - A file containing the final identification measures for each person-quarter\n",
    "\n",
    "**Author(s)** : <br>\n",
    "Lara Yang, Sarayu Anshuman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a1cd1e-4dda-4639-b5a3-7d9c5fd99470",
   "metadata": {},
   "source": [
    "Install packages and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cacb93-3c98-4f1a-8a3f-730035419963",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec299f6-6994-415d-90e0-e8175aa8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd34988-3154-43af-b159-c3fd40a7bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mittens import Mittens\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "import ujson as json\n",
    "import re\n",
    "from gensim.matutils import cossim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from statistics import mean \n",
    "from sklearn.decomposition import PCA\n",
    "import multiprocessing\n",
    "from collections import defaultdict\n",
    "from gensim.matutils import cossim, any2sparse\n",
    "import random\n",
    "from statistics import mean \n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2f70d-0525-46c5-8a59-e282855dd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "########### Helper Functions for Generating Mittens Embeddings ##########\n",
    "#########################################################################\n",
    "\n",
    "#this function returns a word, another word, and the value given in the cooccurance matrix based on the weighting function of the occurance of teh second word in the first word's context window\n",
    "def _window_based_iterator(toks, window_size, weighting_function):\n",
    "    for i, w in enumerate(toks):\n",
    "        yield w, w, 1\n",
    "        left = max([0, i-window_size])\n",
    "        for x in range(left, i):\n",
    "            yield w, toks[x],weighting_function(x)\n",
    "        right = min([i+1+window_size, len(toks)])\n",
    "        for x in range(i+1, right):\n",
    "            yield w, toks[x], weighting_function(x)\n",
    "    return\n",
    "\n",
    "def glove2dict(glove_filename):\n",
    "    \"\"\"\n",
    "    Reads word vectors into a dictionary\n",
    "    Parameters\n",
    "    ----------\n",
    "    glove_filename : str\n",
    "        Name of file that contains vectors\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        A dictionary matching words to their vectors\n",
    "    \"\"\"\n",
    "    with open(glove_filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE) #GloVe output files have the format one word per line, followed by its vector values separated by spaces\n",
    "        data = {line[0]: np.array(list(map(float, line[1: ]))) for line in reader} #create a key (word): value (embedding) dict for every word\n",
    "    return data\n",
    "\n",
    "# Inspired by original build_weighted_matrix in utils.py in the Mittens paper source codebase\n",
    "def build_weighted_matrix(emails,\n",
    "        mincount=300, vocab_size=None, window_size=10,\n",
    "        weighting_function=lambda x: 1 / (x + 1),\n",
    "        email_type='internal'):\n",
    "    \"\"\"\n",
    "    Builds a count matrix based on a co-occurrence window of\n",
    "    `window_size` elements before and `window_size` elements after the\n",
    "    focal word, where the counts are weighted based on proximity to the\n",
    "    focal word.\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dicts\n",
    "        Emails converted from JSON formats\n",
    "    mincount : int\n",
    "        Only words with at least this many tokens will be included. #this means only words with atleast 300 occurances in teh document are included\n",
    "    vocab_size : int or None\n",
    "        If this is an int above 0, then, the top `vocab_size` words\n",
    "        by frequency are included in the matrix, and `mincount`\n",
    "        is ignored.\n",
    "    window_size : int\n",
    "        Size of the window before and after. (So the total window size\n",
    "        is 2 times this value, with the focal word at the center.)\n",
    "    weighting_function : function from ints to floats\n",
    "        How to weight counts based on distance. The default is 1/d\n",
    "        where d is the distance in words.\n",
    "    email_type : str, optional\n",
    "        Specifies which types of emails to include when building embeddings\n",
    "    Returns\n",
    "    -------\n",
    "    X : pd.DataFrame\n",
    "        Cooccurence matrix guaranteed to be symmetric because of the way the counts are collected.\n",
    "    \"\"\"\n",
    "    wc = defaultdict(int)\n",
    "    #corpus contains a list  of emails, where each email is a dict\n",
    "    corpus = read_corpus(emails, email_type=email_type, sentence_delim=False)\n",
    "    if corpus is None:\n",
    "        print(\"These emails are empty\\t{}.\\nEmpty corpus returned for email type {}\".format(str(emails), email_type))\n",
    "        return pd.DataFrame()\n",
    "    for toks in corpus:\n",
    "        for tok in toks:\n",
    "            wc[tok] += 1 #word count\n",
    "    #now create the vocabulary\n",
    "    if vocab_size: #if a vocab size is defined then take the first top 'vocab_size' counts\n",
    "        srt = sorted(wc.items(), key=itemgetter(1), reverse=True)\n",
    "        vocab_set = {w for w, c in srt[: vocab_size]} #sort all the words in the vocabulary according to count\n",
    "    else: #define a vocab based on all those words which have a count greater than 'mincount'\n",
    "        vocab_set = {w for w, c in wc.items() if c >= mincount}\n",
    "    vocab = sorted(vocab_set)\n",
    "    n_words = len(vocab) #length of vocab\n",
    "    # Weighted counts:\n",
    "    counts = defaultdict(float)\n",
    "    for toks in corpus: #for each email\n",
    "        window_iter = _window_based_iterator(toks, window_size, weighting_function)\n",
    "        for w, w_c, val in window_iter:\n",
    "            if w in vocab_set and w_c in vocab_set:\n",
    "                counts[(w, w_c)] += val\n",
    "\n",
    "    '''\n",
    "    For each sentence (list of words), it uses a helper function _window_based_iterator (not provided) to generate a sequence of tuples:\n",
    "    The first element is the focal word.\n",
    "    The second element is a co-occurring word within the window.\n",
    "    The third element is the weight for the co-occurrence based on the distance between the words (using the provided weighting_function).\n",
    "    It checks if both the focal word and co-occurring word are in the vocabulary (vocab_set). If so, it updates the counts dictionary with the weighted co-occurrence for that word pair.\n",
    "    '''\n",
    "    X = np.zeros((n_words, n_words))\n",
    "    for i, w1 in enumerate(vocab):\n",
    "        for j, w2 in enumerate(vocab):\n",
    "            X[i, j] = counts[(w1, w2)]\n",
    "    X = pd.DataFrame(X, columns=vocab, index=pd.Index(vocab))\n",
    "    return X\n",
    "\n",
    "def read_corpus(emails, email_type, sentence_delim=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dict\n",
    "        A list of emails converted from JSON formats\n",
    "    email_type : str\n",
    "        Specifies which types of emails to include when building embeddings\n",
    "        'internal' filters for internal emails, 'external' filters for external and mixed emails, \n",
    "        and anything else does not filter\n",
    "    sentence_delim : bool, optional\n",
    "        If true, co-occurrences across sentence boundaries are ignored.\n",
    "    Returns\n",
    "    -------\n",
    "    list of list of str\n",
    "        Corpus converted from emails.\n",
    "        If sentence_delim is false, returns a list of emails, which are represented as lists of tokens\n",
    "        If sentence_delim is true, returns a list of sentences, which are represented as lists of tokens\n",
    "    \"\"\"\n",
    "    # split with no argument splits on whitespaces and newlines\n",
    "    if not sentence_delim:\n",
    "        if email_type == 'internal':\n",
    "            return [email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails if (\n",
    "                email['email_type'] == 'int')]\n",
    "        elif email_type == 'external':\n",
    "            return [email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails if (\n",
    "                email['email_type'] == 'ext' or email['email_type'] == 'mixed')]\n",
    "        # agnostic to email type\n",
    "        else:\n",
    "            return [email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split() for email in emails]\n",
    "    else:\n",
    "        if email_type == 'internal':\n",
    "            return [sent.strip().split() for email in emails for line in email['hb'].split('\\n') for sent in line.split('SENT_END') if (\n",
    "                len(sent) > 0 and email_type == 'int')]\n",
    "        elif email_type == 'external':\n",
    "            return [sent.strip().split() for email in emails for line in email['hb'].split('\\n') for sent in line.split('SENT_END') if (\n",
    "                len(sent) > 0 and (email_type == 'int' or email['email_type'] == 'mixed'))]\n",
    "        else:\n",
    "            return [sent.strip().split() for email in emails for line in email['hb'].split('\\n') for sent in line.split('SENT_END') if len(sent) > 0]\n",
    "\n",
    "def output_embeddings(mittens_df, filename, compress=False):\n",
    "    if compress:\n",
    "        mittens_df.to_csv(filename + '.gz', quoting=csv.QUOTE_NONE, header=False, sep=\" \", compression='gzip')\n",
    "    else:\n",
    "        mittens_df.to_csv(filename, quoting=csv.QUOTE_NONE, header=False, sep=\" \")\n",
    "    return\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with JSON Emails #############\n",
    "#########################################################################\n",
    "# Slightly different from get_recipients for spacespace emails\n",
    "def get_recipients(msg):\n",
    "    \"\"\"\n",
    "    Return a set of recipients of the current message.\n",
    "    self is removed from list of recipients if in recipients #(-set([sender]))\n",
    "    All fields contain email addresses, not user IDs. From fields are visually just strings\n",
    "    but checking just in case\n",
    "    \"\"\"\n",
    "    sender = msg['From'][0] if type(msg['From']) == list else msg['From']\n",
    "    return set(msg.get('To', []) + msg.get('Cc', []) + msg.get('Bcc', [])) - set([sender]) #basically return a set of recipients to the current email\n",
    "\n",
    "def slice_user_corpus(emails, train_mode): #slices a list of emails into chunks based on time periods specified by the train_mode parameter\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    emails : list of dict\n",
    "        A list of emails converted from JSON formats to dictionaries \n",
    "    train_mode : str\n",
    "        One of 'annual', 'quarterly', 'all'\n",
    "        Indicates how to chunk up emails - into quarters, years, or both\n",
    "    Returns\n",
    "    -------\n",
    "    timekey2emails : dict\n",
    "        Matches quarters or years to respective emails\n",
    "    \"\"\"\n",
    "    #'ActivityCreatedAt' contains a timestamp for the list of emails\n",
    "    timekey2emails = defaultdict(list)\n",
    "    #the function iterates through each email in the email list\n",
    "    for email in emails:\n",
    "        if train_mode == 'annual':\n",
    "            timekey2emails[to_year(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "        elif train_mode == 'quarterly':\n",
    "            timekey2emails[to_quarter(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "        elif train_mode == 'halfyear':\n",
    "            timekey2emails[to_halfyear(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "        elif train_mode == 'all':\n",
    "            timekey2emails[to_year(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "            timekey2emails[to_quarter(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "            timekey2emails[to_halfyear(email['ActivityCreatedAt'], format='str')].append(email)\n",
    "    return timekey2emails\n",
    "\n",
    "#########################################################################\n",
    "############# Helper Functions for Working with Date Objects ############\n",
    "#########################################################################\n",
    "\n",
    "def to_quarter(date, format):\n",
    "    \"\"\"\n",
    "    Return quarter of date in string\n",
    "    \"\"\"\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]    \n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    quarter = ((int(month)-1) // 3) + 1\n",
    "    timekey = str(year) + 'Q' + str(quarter)\n",
    "    return timekey\n",
    "\n",
    "def to_halfyear(date, format):\n",
    "    \"\"\"\n",
    "    Return half year of date in string\n",
    "    \"\"\"\n",
    "    year, month = 0, 0\n",
    "    if format == 'str':\n",
    "        year = date[0:4]\n",
    "        month = date[5:7]    \n",
    "    elif format == 'datetime':\n",
    "        year = date.year\n",
    "        month = date.month\n",
    "    halfyear = ((int(month)-1) // 6) + 1\n",
    "    timekey = str(year) + 'HY' + str(halfyear)\n",
    "    return timekey\n",
    "\n",
    "def to_year(date, format):\n",
    "    \"\"\"\n",
    "    Return year of date in string\n",
    "    \"\"\"\n",
    "    if format == 'str':\n",
    "        return date[0:4]\n",
    "    elif format == 'datetime':\n",
    "        return str(date.year)\n",
    "\n",
    "def datetime_to_timekey(date, time_key):\n",
    "    if time_key == 'year':\n",
    "        return to_year(date, format='datetime')\n",
    "    elif time_key == 'quarter':\n",
    "        return to_quarter(date, format='datetime')\n",
    "\n",
    "def is_month_before_equal(datetime1, datetime2):\n",
    "    if datetime1.year < datetime2.year:\n",
    "        return 1\n",
    "    elif (datetime1.year == datetime2.year) and (datetime1.month <= datetime2.month):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def num_months_between_dates(datetime1, datetime2):\n",
    "    return abs((datetime1.year - datetime2.year) * 12 + datetime1.month - datetime2.month)\n",
    "\n",
    "def num_quarters_between_dates(datetime1, datetime2):\n",
    "    return abs((datetime1.year - datetime2.year) * 12 + datetime1.month - datetime2.month) // 3\n",
    "\n",
    "def num_years_between_dates(datetime1, datetime2):\n",
    "    return abs(datetime1.year - datetime2.year)\n",
    "\n",
    "def time_between_dates(datetime1, datetime2, time_key):\n",
    "    if time_key == 'monthly':\n",
    "        return num_months_between_dates(datetime1, datetime2)\n",
    "    elif time_key == 'quarterly':\n",
    "        return num_quarters_between_dates(datetime1, datetime2)\n",
    "    elif time_key == 'annual':\n",
    "        return num_years_between_dates(datetime1, datetime2)\n",
    "\n",
    "#########################################################################\n",
    "############## Helper Functions for Working with Dataframes #############\n",
    "#########################################################################\n",
    "\n",
    "#function to convert a dictionary to a dataframe\n",
    "def dict_to_df(index2rows, cols, index_name):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    index2rows : dict\n",
    "        Dictionary mapping index to rows to be coverted\n",
    "    cols : list\n",
    "        List of column names of type str\n",
    "    index : list\n",
    "        List of index names\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Constructed dataframe\n",
    "    \"\"\"\n",
    "    if index2rows is None or len(index2rows) == 0:\n",
    "        return None\n",
    "    #case where the dataframe is formed across entire time span of user emails\n",
    "    if len(index_name) == 1:\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df.index.name = index_name[0]\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "    else: #case where the user token (word) counts and email counts are formed for ecah year or each quarter\n",
    "        df = pd.DataFrame.from_dict(index2rows, orient='index', columns=cols)\n",
    "        df = pd.DataFrame(df, pd.MultiIndex.from_tuples(df.index, names=index_name))\n",
    "        df.sort_index(axis=0, inplace=True)\n",
    "        return df\n",
    "\n",
    "#########################################################################\n",
    "########### Helper Functions for Working with Embedding Output ##########\n",
    "#########################################################################\n",
    "\n",
    "def remove_empty_embeddings(embeddings_dir):\n",
    "    \"\"\"\n",
    "    Removes all empty files in embeddings_dir that were produced when vocab size was 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_dir : str\n",
    "        Full path to directory where embedding files are located\n",
    "    \"\"\"\n",
    "    for file in os.listdir(embeddings_dir):\n",
    "        mittens_file = os.path.join(embeddings_dir, file)\n",
    "        if os.path.getsize(mittens_file) == 0:\n",
    "            os.remove(mittens_file)\n",
    "    return\n",
    "\n",
    "def extract_company_embedding(company_embeddings_filename, tmp_dir, words):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_embeddings_filename : str\n",
    "        File path of the company embeddings\n",
    "    tmp_dir : str\n",
    "        Path to the directory for gensim to output its tmp files in order to load embeddings into word2vec format\n",
    "    words : list\n",
    "        A list of strings for which to retrieve vectors\n",
    "    Returns\n",
    "    -------\n",
    "    vecs : list\n",
    "        A list of vectors of type numpy.ndarray that correspond to the list of words given as parameters\n",
    "    \"\"\" \n",
    "    tmp_mittens = os.path.join(tmp_dir, \"mittens_embeddings_all_word2vec.txt\")\n",
    "    word2vec_mittens_file = get_tmpfile(tmp_mittens)\n",
    "    glove2word2vec(company_embeddings_filename, word2vec_mittens_file) #load the embeddings in the glove2word2vec format\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_mittens_file) #this model represents the glove embddings space using the default company embeddings\n",
    "    vecs = []\n",
    "    '''\n",
    "    for w in words:\n",
    "        if w in model.vocab:\n",
    "            vecs.append(model.wv[w]) #default vectors for the words in the vocab\n",
    "        else:\n",
    "            vecs.append(np.nan)\n",
    "            print('%s not in company embeddings' % w)\n",
    "    '''\n",
    "    for w in words:\n",
    "     try:\n",
    "      index = model.key_to_index[w]\n",
    "      vecs.append(model.vectors[index])\n",
    "     except KeyError:\n",
    "      vecs.append(np.nan)\n",
    "      print('%s not in company embeddings' % w)\n",
    "    return vecs\n",
    "\n",
    "def word_similarity(model, w1, w2):\n",
    "    \"\"\"\n",
    "    This is an auxilary function that allows for comparing one word to another word or multiple words\n",
    "    If w1 and w2 are both single words, n_similarity returns their cosine similarity which is the same as \n",
    "    simply calling similarity(w1, w2)\n",
    "    If w1 or w2 is a set of words, n_similarity essentially takes the mean of the set of words and then computes\n",
    "    the cosine similarity between that vector mean and the other vector. This functionality is both reflected\n",
    "    in its source code and has been verified manually.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : KeyedVectors\n",
    "        The model that contains all the words and vectors\n",
    "    w1 : str or list\n",
    "        The first word or word list to be compared\n",
    "    w2 : str or list\n",
    "        The second word or word list to be compared\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between w1 and w2\n",
    "    \"\"\"\n",
    "    #first fo the case where w1 and w2 parameters are single words\n",
    "    if not isinstance(w1, list): \n",
    "        w1 = [w1]\n",
    "    if not isinstance(w2, list):\n",
    "        w2 = [w2]\n",
    "    #create a copy of the input lists\n",
    "    '''\n",
    "    w1 = [w for w in w1 if w in model.vocab]\n",
    "    w2 = [w for w in w2 if w in model.vocab]\n",
    "    '''\n",
    "    w1 = [w for w in w1 if w in model.key_to_index]\n",
    "    w2 = [w for w in w2 if w in model.key_to_index]\n",
    "    if len(w1) == 0 or len(w2) == 0:\n",
    "        return None\n",
    "    return model.n_similarity(w1, w2) #inbuilt word2vec model that calculates the cosine similarity between two lists; takes the mean of the set of words and then computes\n",
    "    #the cosine similarity between that vector mean and the other vector\n",
    "\n",
    "def cossim_with_none(vec1, vec2, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Auxiliary function that calls cossim function to test if vectors are None to prevent erroring out.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vec1 : list of (int, float), gensim sparse vector format\n",
    "    vec2 : list of (int, float), gensim sparse vector format\n",
    "    format : str, optional\n",
    "        Either sparse or dense. If sparse, vec1 and vec2 are in gensim sparse vector format; use cossim function from gensim.\n",
    "        Otherwise, vec1 and vec2 are numpy arrays and cosine similarity is hand calculated\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine similarity between vec1 and vec2\n",
    "    \"\"\"\n",
    "    if not (isnull_wrapper(vec1) or isnull_wrapper(vec2)):\n",
    "        if vec_format == 'sparse':\n",
    "            return cossim(vec1, vec2)\n",
    "        elif vec_format == 'dense':\n",
    "            if len(vec1) == 0 or len(vec2) == 0:\n",
    "                return None\n",
    "            return np.dot(vec1, vec2)/(np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        else:\n",
    "            raise ValueError()\n",
    "    return None\n",
    "\n",
    "def calculate_pairwise_cossim(col1, col2=None, reference=False, reference_group=None, anon_ids=None, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Calculates averaged cosine similarity of every row vector in col1 with every other row vector in col2.\n",
    "    If no col2 is provided, cosine similarity of every row vector with every other row vector in col1 is calculated.\n",
    "    The two columns should have equal length.\n",
    "    Parameters\n",
    "    ----------\n",
    "    col1 : pd.Series\n",
    "        A column where each row is a sparse word vector (BoW format that gensim code is written for).\n",
    "    col2 : pd.Series, optional\n",
    "        A column where each row is a sparse word vector (BoW format that gensim code is written for).\n",
    "    reference : bool, optional\n",
    "        Indicator variable for whether filtering for reference groups is needed\n",
    "    reference_group : pd.Series, optional\n",
    "        If filtering for reference groups, a list containing reference group members for every employee in col1\n",
    "    anon_ids : pd.Series, optional\n",
    "        If filtering for reference groups, a list containing anon_ids for every employee in col1\n",
    "    Returns\n",
    "    -------\n",
    "    results : list\n",
    "        A list where the ith element is the averaged cosine similarity between the ith vector in col1 and every vector\n",
    "        in col2 for which i != j. If no col2 is provided, a list where the ith element is the averaged cosine similarity\n",
    "        between the ith vector in col1 and every other vector in col1 is returned.\n",
    "    \"\"\"\n",
    "    vectors1 = col1.tolist()\n",
    "    vectors2 = col2.tolist() if col2 is not None else col1.tolist()\n",
    "    reference_group = reference_group.tolist() if reference else None\n",
    "    anon_ids = anon_ids.tolist() if anon_ids is not None else None\n",
    "    results = list()\n",
    "    for i in range(len(vectors1)):\n",
    "        total_sim = []\n",
    "        if not isnull_wrapper(vectors1[i]):\n",
    "            for j in range(len(vectors2)):\n",
    "                if i != j and not isnull_wrapper(vectors2[j]):\n",
    "                    # filter out any np.nans as our reference group\n",
    "                    if not reference or (type(reference_group[i]) == set and anon_ids[j] in reference_group[i]):\n",
    "                        if vec_format == 'sparse':\n",
    "                            total_sim.append(cossim(vectors1[i], vectors2[j]))\n",
    "                        elif vec_format == 'dense':\n",
    "                            total_sim.append(np.dot(vectors1[i], vectors2[j])/(np.linalg.norm(vectors1[i]) * np.linalg.norm(vectors2[j])))\n",
    "                        else:\n",
    "                            raise ValueError()\n",
    "            results.append(mean(total_sim) if len(total_sim) > 0 else None)\n",
    "        else:\n",
    "            results.append(None)\n",
    "    return results\n",
    "\n",
    "def isnull_wrapper(x):\n",
    "    r = pd.isnull(x)\n",
    "    if type(r) == bool:\n",
    "        return r\n",
    "    return r.any()\n",
    "\n",
    "def vector_mean(col):\n",
    "    \"\"\"\n",
    "    Calculate vector means of row vectors\n",
    "    Parameters\n",
    "    ----------\n",
    "    col : pd.Series\n",
    "        The column to be averaged\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        A vector that is the numerical average of all vectors in col\n",
    "    \"\"\"\n",
    "    return np.array(col[col.notna()].tolist()).mean(axis=0)\n",
    "\n",
    "def extract_hr_survey_df(survey_hr_file, user_qualtrics_file, users_file, perf_likert_file, perf_percentage_file):\n",
    "    \"\"\"\n",
    "    Loads various datasets to prepare survey and hr data for merging with embeddings df.\n",
    "    Returns\n",
    "    -------\n",
    "    survey_hr_df : pd.DataFrame\n",
    "        Dataframe indexed by user_id\n",
    "    \"\"\"\n",
    "    perf_likert_df = pd.read_csv(perf_likert_file)\n",
    "    perf_percentage_df = pd.read_csv(perf_percentage_file)\n",
    "    perf_likert_df = perf_likert_df[['UID', '2019 Perf_Type(Rating)', '2020 Perf_Type(Rating)']]\n",
    "    perf_percentage_df = perf_percentage_df[['UID', '2019_Perf_Type(Percentage)', '2020_Perf_Type(Percentage)']]\n",
    "    perf_likert_df.columns = [\"UID\", \"perf_rating_2019\", \"perf_rating_2020\"]\n",
    "    perf_percentage_df.columns = [\"UID\", \"perf_percentage_2019\", \"perf_percentage_2020\"]\n",
    "    \n",
    "    user_qualtrics_df = pd.read_csv(user_qualtrics_file)\n",
    "    user_qualtrics_df = user_qualtrics_df.merge(perf_likert_df, on='UID', how='left').merge(perf_percentage_df, on=\"UID\", how='left')\n",
    "\n",
    "    survey_hr_df = pd.read_csv(survey_hr_file)\n",
    "    survey_hr_df = survey_hr_df.merge(user_qualtrics_df, left_on='uid', right_on='UID', how='left')\n",
    "    # we lose two employees whose emails are not included in the crawled email data\n",
    "    email2uid = {}\n",
    "    with open(users_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            user = json.loads(line)\n",
    "            for e in user['Emails']:\n",
    "                email2uid[e] = user['UserId']\n",
    "\n",
    "    survey_hr_df = survey_hr_df[survey_hr_df['Email'].isin(email2uid.keys())]\n",
    "    survey_hr_df['user_id'] = survey_hr_df['Email'].apply(lambda e : email2uid[e])\n",
    "    survey_hr_df.set_index('user_id', inplace=True)\n",
    "    return survey_hr_df\n",
    "\n",
    "def extract_variables_from_file(file):\n",
    "    \"\"\" \n",
    "    Extract relevant information from name of embedding file, with format: {}(_{})?_(internal|external).txt\n",
    "    \"\"\"\n",
    "    file_chunks = file[0:-4].split('_') #split the file name into 3 parts\n",
    "    usr = file_chunks[0] #the first part contains the userID\n",
    "    time_key = file_chunks[1] if len(file_chunks) == 3 else None #this part contains the year - like 2020, or 2020Q1, or 2020HY1\n",
    "    return (usr, time_key)\n",
    "\n",
    "def project(word, dimension):\n",
    "    \"\"\"\n",
    "    Returns the scalar projection of word on dimension. Word and dimension are both assumed to be vectors\n",
    "    \"\"\"\n",
    "    return np.dot(word, dimension)/ np.linalg.norm(dimension)\n",
    "\n",
    "def drop(u, v):\n",
    "    \"\"\"\n",
    "    Returns the component in u that is orthogonal to v, also known as the vector rejection of u from v\n",
    "    \"\"\"\n",
    "    return u - v * u.dot(v) / v.dot(v)\n",
    "\n",
    "def remove_frequency(company_embeddings, embedding_dim):\n",
    "    \"\"\"\n",
    "    Remove frequency dimension from company embeddings (assumed to be the top dimension)\n",
    "    Parameters\n",
    "    ----------\n",
    "    company_embeddings : dict\n",
    "        A dictionary mapping words to their embeddings\n",
    "    embedding_dim : int\n",
    "        The dimension of word vectors. Used to determine the number of components that go into PCA.\n",
    "    Returns\n",
    "    -------\n",
    "    new_company_embeddings : dict\n",
    "        A dictionary mapping words to their embeddings, where embeddings are demeaned and first PCA\n",
    "        dimension hypothesized to represent frequency dimension is removed\n",
    "    \"\"\"\n",
    "    vectors = np.array([v for k, v in company_embeddings.items()])\n",
    "    miu = np.mean(vectors, axis=0)\n",
    "    demeaned_vectors = vectors - miu\n",
    "    pca = PCA(n_components = embedding_dim)\n",
    "    pca.fit(demeaned_vectors)\n",
    "    frequency_dim = pca.components_[0]\n",
    "    new_company_embeddings = {k : drop(v-miu, frequency_dim) for k, v in company_embeddings.items()}\n",
    "    return new_company_embeddings\n",
    "\n",
    "def doPCA(words_start, words_end):\n",
    "    \"\"\"\n",
    "    Performs PCA on differences between pairs of words and returns the first component\n",
    "    Based on function doPCA in Bolukbasi et al. (2016) source code at https://github.com/tolga-b/debiaswe/blob/master/debiaswe/we.py\n",
    "    Parameter\n",
    "    ---------\n",
    "    words_start : list\n",
    "        List of hashed words at one end of interested dimension\n",
    "    words_end: list\n",
    "        List of hashed words at the other end of dimension\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        First component of PCA of differences between pairs of words\n",
    "    \"\"\"\n",
    "    matrix = []\n",
    "    for i in range(len(words_start)):\n",
    "        center = (words_start[i] + words_end[i])/2\n",
    "        matrix.append(words_end[i] - center)\n",
    "        matrix.append(words_start[i] - center)\n",
    "    matrix = np.array(matrix)\n",
    "    # cannot have more components than the number of samples\n",
    "    num_components = len(words_start)*2\n",
    "    pca = PCA(n_components = num_components)\n",
    "    pca.fit(matrix)\n",
    "    return pca.components_[0]\n",
    "\n",
    "def build_dimension(words_start, words_end):\n",
    "    \"\"\"\n",
    "    This method builds a dimension defined by words at separate end of a dimension.\n",
    "    Multiple methods exist in previous literature when building such a dimension.\n",
    "    1) Kozlowski et al. (2019) averages across differences between different word pairs, noted to be interchangeable with averaging words on each side of the dimension and\n",
    "    then taking the difference between averages. They are empirically verified to be identical.\n",
    "    2) Bolukbasi et al. (2016) defines gender direction using a simple difference between man and woman in the corresponding tutorial. In the same tutorial, \n",
    "    racial direction is defined as difference between two clusters of words that are each sum of the embeddings of its corresponding dimensions\n",
    "    normalized by the L2 norm. Wang et al. (2020) note that normalization is unnecessary. If unnormalized, this method should be equivalent to #3.\n",
    "    3) Bolukbasi et al. (2016) defines gender direction also by taking the differences across multiple pairs, doing PCA on these differences, and \n",
    "    taking the first component as the gender direction.\n",
    "    Parameter\n",
    "    ---------\n",
    "    words_start : list\n",
    "        List of hashed words at the positive end of the dimension, where positive implies more likely to affect identification positively\n",
    "    words_end: list\n",
    "        List of hashed words at the other end of dimension\n",
    "    Returns\n",
    "    -------\n",
    "    (mean_dim, pca_dimension) : 2-tuple of numpy vector\n",
    "        Two vector that represents the dimension of interest calculated using method #1 and #3.\n",
    "    \"\"\"\n",
    "    assert len(words_start) == len(words_end)\n",
    "    differences = [(np.array(words_start[i]) - np.array(words_end[i])) for i in range(len(words_start)) if not np.isnan(words_start[i]).any() and not np.isnan(words_end[i]).any()]\n",
    "    mean_dim = np.array(differences).mean(axis=0)\n",
    "    pca_dim = doPCA(words_start, words_end)\n",
    "    if project(words_start[0], pca_dim) < 0:\n",
    "        # convention used in the current script is that words_start should represent the positive dimension\n",
    "        pca_dim = pca_dim * -1\n",
    "    return (mean_dim, pca_dim)\n",
    "\n",
    "#ending here-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e636d-eec5-4323-8e6c-533097ac06c6",
   "metadata": {},
   "source": [
    "Print current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5370f7-8c15-490a-b208-d782c54b15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26e6bd-66a1-4703-92b0-e60ee2d7ff1e",
   "metadata": {},
   "source": [
    "Set hyperparameters and input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4c39e3-2056-4545-935d-fac849386930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters\n",
    "embedding_dim = 50\n",
    "mincount = 50\n",
    "ling_thres = 0.8 \n",
    "\n",
    "#set file paths\n",
    "home_dir = \"/zfs/projects/faculty/amirgo-identification/\"\n",
    "email_dir = os.path.join(home_dir, \"email_data/\")\n",
    "mittens_dir = os.path.join(home_dir, \"mittens\")\n",
    "utils_dir = os.path.join(mittens_dir, \"utils\")\n",
    "#note: created two copies of the embeddings folder, use this copy - embeddings_high_prob_eng_08_50d_mincount50_v2 instead of embeddings_high_prob_eng_08_50d_mincount50\n",
    "embeddings_dir = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/staffing/fine-tuning/embeddings_high_prob_eng_08_50d_mincount50_v2\"\n",
    "\n",
    "email_file = os.path.join(email_dir, 'MessagesHashed.jsonl')\n",
    "users_file = os.path.join(email_dir, 'Users.json')\n",
    "activity_file = os.path.join(email_dir, 'Activities.json')\n",
    "survey_dir = os.path.join(home_dir, \"survey_hr_data\")\n",
    "user_qualtrics_file = os.path.join(survey_dir, \"UsersQualtrics.csv\")\n",
    "#other additional survey files\n",
    "perf_percentage = os.path.join(survey_dir, \"perf_rating_percentages.csv\")\n",
    "perf_likert = os.path.join(survey_dir, \"perf_rating_likert.csv\")\n",
    "\n",
    "analyses_data_dir = \"/zfs/projects/faculty/amirgo-transfer/spacespace/spacespace/staffing/analyses_data/\"\n",
    "survey_filename = os.path.join(analyses_data_dir, \"preprocessed_survey_hr.csv\")\n",
    "company_embeddings_filename = \"/zfs/projects/faculty/amirgo-identification/identification-Sarayu/staffing/training/vectors_high_prob_eng_08_50d.txt\"\n",
    "\n",
    "#change the mincount directory as needed by tmp based on mincount value\n",
    "tmp_dir = os.path.join(current_dir, \"tmp_mincount50_v2\")\n",
    "output_dir = os.path.join(current_dir, \"staffing_email_idtf_data_mincount50_v2\")\n",
    "\n",
    "#the main output files with the similarity measures, the measures are created at the person, person-quarter, person-halfyear, and person-year levels\n",
    "user_output_filename = os.path.join(output_dir, \"embeddings_high_prob_eng_{}_users_{}d_mincount{}_v2.csv\".format(str(ling_thres).replace(\".\", \"\"), embedding_dim, mincount))\n",
    "annual_output_filename = os.path.join(output_dir, \"embeddings_high_prob_eng_{}_annual_{}d_mincount{}_v2.csv\".format(str(ling_thres).replace(\".\", \"\"), embedding_dim, mincount))\n",
    "quarterly_output_filename = os.path.join(output_dir, \"embeddings_high_prob_eng_{}_quarterly_{}d_mincount{}_v2.csv\".format(str(ling_thres).replace(\".\", \"\"), embedding_dim, mincount))\n",
    "halfyearly_output_filename = os.path.join(output_dir, \"embeddings_high_prob_eng_{}_halfyearly_{}d_mincount{}_v2.csv\".format(str(ling_thres).replace(\".\", \"\"), embedding_dim, mincount))\n",
    "\n",
    "print(embeddings_dir)\n",
    "print(company_embeddings_filename)\n",
    "print(tmp_dir)\n",
    "print(user_output_filename)\n",
    "print(annual_output_filename)\n",
    "print(quarterly_output_filename)\n",
    "print(halfyearly_output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4f026-33a1-43c0-baae-58a503a13d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hashes\n",
    "\n",
    "year_colname, quarter_colname, halfyear_colname = 'year', 'quarter', 'halfyear'\n",
    "hash2word = {\n",
    "    '09f83385': 'mine',\n",
    "    '20019fa4': 'i',\n",
    "    '20b60145': 'us',\n",
    "    '28969cb1': 'them',\n",
    "    '3828d3d2': 'me',\n",
    "    '4dd6d391': 'their',\n",
    "    '5b4e27db': 'my',\n",
    "    '64a505fc': 'ourselves',\n",
    "    '6935bb23': 'ours',\n",
    "    '6f75419e': 'myself',\n",
    "    '86df0c8d': 'themselves',\n",
    "    'a7383e72': 'we',\n",
    "    'a9193217': 'theirs',\n",
    "    'b72a9dd7': 'our',\n",
    "     'fd0ccf1c': 'they'}\n",
    "word2hash = {v:k for k, v in hash2word.items()}\n",
    "pronouns = ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves']\n",
    "single_pronouns = ['i', 'we']\n",
    "i_index, we_index = 0, 5 #index in the 'pronouns' list defined above\n",
    "hash_pronouns = [word2hash[p] for p in pronouns]\n",
    "hash_single_pronouns = [word2hash[p] for p in single_pronouns]\n",
    "\n",
    "file_name_re = re.compile(\"5([a-z0-9]+)_(2020(Q3)?_)?internal.txt\") #all the generated email slices\n",
    "num_cores = 10 #note yen1 has only 12 cores\n",
    "\n",
    "domain_hash = {\n",
    "    'collabera.com':                     '509c8f6b1127bceefd418c023533d653',\n",
    "    'collaberainc.mail.onmicrosoft.com': 'ec5b67548b6ec06f1234d198efec741e',\n",
    "    'collaberainc.onmicrosoft.com':      '86160680578ee9258f097a67a5f25af9',\n",
    "    'collaberasp.com':                   '6bf3934d19f1acf5b9295b63e0e7f66e',\n",
    "    'g-c-i.com':                         '3444d1f7d5e46443080f2d069e41a10c'}\n",
    "collabera_hashes = set([v for k, v in domain_hash.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9e2c1-0000-4e9f-ac9b-7c8c8ca8cc02",
   "metadata": {},
   "source": [
    "Run the functions to generate the identification measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b252d-2e45-4ff5-98c5-e598b99b91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "######### Functions for Loading Raw Counts as Control Variables #########\n",
    "#########################################################################\n",
    "\n",
    "def read_raw_counts(activity_file, email_file): #function to create the two control variables for each user: token (word) count across all emails made by the user, and number of emails made by the user\n",
    "    \"\"\"\n",
    "    The main workhorse function for obtaining raw message and token counts (word count) as control variables.\n",
    "    Parameters\n",
    "    ----------\n",
    "    activity_file : str\n",
    "        The full filepath that contains all email metadata, where each line is a JSON object that represents one email\n",
    "    email_file : str\n",
    "        The full filepath that contains all email content, where each line is a JSON object that represents one email\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of user-level, annual, and quarterly dataframes\n",
    "    \"\"\"\n",
    "    usr2counts, usr_year2counts, usr_quarter2counts, usr_halfyear2counts = defaultdict(lambda : [0, 0]), defaultdict(lambda : [0, 0]), defaultdict(lambda :[0, 0]), defaultdict(lambda :[0, 0])\n",
    "    sid2activity = {}\n",
    "    cols = ['num_tokens', 'num_messages']\n",
    "    tok_count_index, msg_count_index = 0, 1\n",
    "    with open(activity_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            activity = json.loads(line)\n",
    "            sid2activity[activity['MailSummarySid']] = activity\n",
    "\n",
    "    with open(email_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            email = json.loads(line)\n",
    "            lang = email['l']\n",
    "            if len(email['hb']) > 0 and lang[0] == \"__label__en\" and lang[1] > ling_thres:\n",
    "                activity = sid2activity[email['sid']]\n",
    "                user = activity['UserId']\n",
    "                year = to_year(activity['ActivityCreatedAt'], format='str')\n",
    "                quarter = to_quarter(activity['ActivityCreatedAt'], format='str')\n",
    "                halfyear = to_halfyear(activity['ActivityCreatedAt'], format='str')\n",
    "                num_toks = len(email['hb'].replace('\\n', ' ').replace(\"SENT_END\", \"\").strip().split()) #number of tokens or words in the email\n",
    "                activity = sid2activity[email['sid']]\n",
    "                recipients = get_recipients(activity)\n",
    "                pure_internal = True\n",
    "                for r in recipients:\n",
    "                    domain = r.split('@')[1]\n",
    "                    if domain not in collabera_hashes:\n",
    "                        pure_internal = False\n",
    "                        break\n",
    "                if not pure_internal:\n",
    "                    continue\n",
    "                #update these counts only for the internal messages\n",
    "                usr2counts[user][tok_count_index] += num_toks #keep track of each user's token count across all his emails\n",
    "                usr2counts[user][msg_count_index] += 1 #keep a count of each user's token count\n",
    "                #repeat for the year and quarter dicts. The dictionary above should be an overall dictionary, whereas the dictionaries below are for a user in a specific year or quarter\n",
    "                usr_year2counts[(user, year)][tok_count_index] += num_toks\n",
    "                usr_year2counts[(user, year)][msg_count_index] += 1\n",
    "                usr_quarter2counts[(user, quarter)][tok_count_index] += num_toks\n",
    "                usr_quarter2counts[(user, quarter)][msg_count_index] += 1\n",
    "                usr_halfyear2counts[(user, halfyear)][tok_count_index] += num_toks\n",
    "                usr_halfyear2counts[(user, halfyear)][msg_count_index] += 1\n",
    "\n",
    "    #creates three different dataframes based on each dictionary (now four because including halfyear too)\n",
    "    usr2counts_df = dict_to_df(usr2counts, cols, index_name=['user_id']) #calls util.py function\n",
    "    usr_year2counts_df = dict_to_df(usr_year2counts, cols, index_name=['user_id', year_colname])\n",
    "    usr_quarter2counts_df = dict_to_df(usr_quarter2counts, cols, index_name=['user_id', quarter_colname])\n",
    "    usr_halfyear2counts_df = dict_to_df(usr_halfyear2counts, cols, index_name=['user_id', halfyear_colname])\n",
    "    # return the dataframe that contain the control variables for each user along with the token (word) count and email count\n",
    "    return (usr2counts_df, usr_year2counts_df, usr_quarter2counts_df, usr_halfyear2counts_df)\n",
    "\n",
    "#########################################################################\n",
    "#### Functions for Measuring Within-Person Similarities in Embeddings ###\n",
    "#########################################################################\n",
    "\n",
    "def embeddings_similarities(model):\n",
    "    \"\"\"\n",
    "    Returns the embeddings of i, we, centroid of i-words, centroid of we-words, and their respective cosine similarities\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : gensim.models.Word2Vec\n",
    "        Model that stores the embeddings for each word\n",
    "    Returns\n",
    "    -------\n",
    "    embeds : list\n",
    "        A list of embeddings (vectors) and similarities (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    '''\n",
    "    i = model.wv[hash_single_pronouns[0]] if hash_single_pronouns[0] in model.vocab else None #obtain the embedding of 'i' by passing the hash\n",
    "    we = model.wv[hash_single_pronouns[1]] if hash_single_pronouns[1] in model.vocab else None\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "     #i = model.wv[model.key_to_index[hash_single_pronouns[0]]]\n",
    "     i = model[model.key_to_index[hash_single_pronouns[0]]]\n",
    "    except KeyError:\n",
    "     i = None\n",
    "    try:\n",
    "     #we = model.wv[model.key_to_index[hash_single_pronouns[1]]]\n",
    "     we = model[model.key_to_index[hash_single_pronouns[1]]]\n",
    "    except KeyError:\n",
    "     we = None\n",
    "    \n",
    "    #i_cluster represents all the words that are lose to i\n",
    "    #i_cluster = [model.wv[word] for word in hash_pronouns[i_index:we_index] if word in model.vocab]\n",
    "    #i_cluster = [model.wv[word] for word in hash_pronouns[i_index:we_index] if word in model.key_to_index]\n",
    "    i_cluster = [model[word] for word in hash_pronouns[i_index:we_index] if word in model.key_to_index]\n",
    "    i_cluster = None if len(i_cluster) == 0 else np.mean(i_cluster, axis=0) #store the centroid or mean value of all the embddings that represent i-words\n",
    "    #we_cluster represents all the words that are close to we\n",
    "    #we_cluster = [model.wv[word] for word in hash_pronouns[we_index:] if word in model.vocab]\n",
    "    #we_cluster = [model.wv[word] for word in hash_pronouns[we_index:] if word in model.key_to_index]\n",
    "    we_cluster = [model[word] for word in hash_pronouns[we_index:] if word in model.key_to_index]\n",
    "    we_cluster = None if len(we_cluster) == 0 else np.mean(we_cluster, axis=0) #store the centroid or mean value of all the embddings that represent we-words\n",
    "    embeds = ([i, we] + [word_similarity(model, hash_pronouns[i_index+i], hash_pronouns[we_index+i]) for i in range(5)] +\n",
    "        [i_cluster, we_cluster, word_similarity(model, hash_pronouns[i_index:we_index], hash_pronouns[we_index:])]) #call util.py\n",
    "    \n",
    "    symmetric_i_words, symmetric_we_words = [], [] #create two lists to store the hashes of the i-words and we-words\n",
    "    '''\n",
    "    for i in range(len(hash_pronouns)-we_index): #for all the i-words\n",
    "        if hash_pronouns[i] in model.vocab and hash_pronouns[i+we_index] in model.vocab:\n",
    "            symmetric_i_words.append(hash_pronouns[i])\n",
    "            symmetric_we_words.append(hash_pronouns[i+we_index])\n",
    "    if len(symmetric_i_words) > 0:\n",
    "        embeds.append(model.n_similarity(symmetric_i_words, symmetric_we_words))\n",
    "    return embeds #embeds is a list that contains the embeddings of (i, we, centroid of i-words, centroid of we-words), and cosine similarities between all the i-words and all the we-words'''\n",
    "    \n",
    "    symmetric_i_words = []\n",
    "    symmetric_we_words = []\n",
    "    for i in range(len(hash_pronouns)-we_index):\n",
    "        if hash_pronouns[i] in model.key_to_index and hash_pronouns[i+we_index] in model.key_to_index:\n",
    "            symmetric_i_words.append(hash_pronouns[i])\n",
    "            symmetric_we_words.append(hash_pronouns[i+we_index])\n",
    "    if len(symmetric_i_words) > 0:\n",
    "        embeds.append(model.n_similarity(symmetric_i_words, symmetric_we_words))\n",
    "    return embeds\n",
    "\n",
    "\n",
    "def process_single_embedding_file(i, num_files, embeddings_dir, file):\n",
    "    \"\"\"\n",
    "    Reading from one embedding file\n",
    "    Parameters\n",
    "    ----------\n",
    "    i : int\n",
    "        Index used for progress tracking\n",
    "    num_files : int\n",
    "        Total number of files to process used for progress tracking\n",
    "    embeddings_dir : str\n",
    "        Directory in which embedding files reside\n",
    "    file : str\n",
    "        Embedding file to open and process\n",
    "    Returns\n",
    "    -------\n",
    "    embeds : list\n",
    "        A list of embeddings and similarities. Embeddings are used for calculating between-person similarities in downstream functions.\n",
    "    \"\"\"\n",
    "    mittens_file = os.path.join(embeddings_dir, file)\n",
    "    if i%100 == 0:\n",
    "        sys.stderr.write(\"Processing \\t%d/%d -'%s', at %s.\\n\" % (i, num_files, mittens_file, datetime.now()))\n",
    "        print('searching the directory')\n",
    "        print(embeddings_dir)\n",
    "    # chopping off the file extension in filename\n",
    "    tmp_mittens = os.path.join(tmp_dir, file[0:-4] + \"_word2vec.txt\")\n",
    "    try:\n",
    "        word2vec_mittens_file = get_tmpfile(tmp_mittens) #this is a function in utils, the embeddings files are converted to word2vec format and stored in a location called tmp_dir\n",
    "        glove2word2vec(mittens_file, word2vec_mittens_file)\n",
    "        model = KeyedVectors.load_word2vec_format(word2vec_mittens_file)\n",
    "        embeds = embeddings_similarities(model)\n",
    "        return embeds\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('File %s caused an error: %s.\\n' % (mittens_file, str(e)))\n",
    "   \n",
    "#within-person similarities: individual's i embedding to we embedding\n",
    "def self_similarities(files, num_files, embeddings_dir):\n",
    "    \"\"\"\n",
    "    Main workhorse function for calculating within-person similarities. Compares an individual's i embedding to we embedding, using both\n",
    "    i and we's embedding only and centroid of i-words and we-words. Indices used in this file relies on knowledge of the naming convention of underlying embedding files.\n",
    "    Parameters\n",
    "    ----------\n",
    "    files : list of str\n",
    "        Embedding files to process\n",
    "    num_files : int\n",
    "        Total number of files to process, used to keep track of progress\n",
    "    embeddings_dir : str\n",
    "        Directory in which embedding files reside\n",
    "    Return\n",
    "    ------\n",
    "    tuple\n",
    "        3-tuple of dictionaries mapping usr and optional timekeys to within-person embedding similarities\n",
    "    \"\"\"\n",
    "    usr2distances, usr_year2distances, usr_quarter2distances, usr_halfyear2distances = defaultdict(list), defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "    pool = multiprocessing.Pool(processes = num_cores) #muliprocessing uisng pythons inbuilt modeule depending on the number of cpu cores available\n",
    "    results = {}\n",
    "    for i, file in enumerate(files, 1): #start indexing from 1\n",
    "        usr, time_key = extract_variables_from_file(file) #utils\n",
    "        #process_single_embedding_file defined above is the function that will be executed by a worker process \n",
    "        results[(usr, time_key)] = pool.apply_async(process_single_embedding_file, args=(i, num_files, embeddings_dir, file, ))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    for key, r in results.items():\n",
    "        usr, time_key = key\n",
    "        curr_row = r.get()\n",
    "        # Empty if errored out\n",
    "        if curr_row:\n",
    "            if time_key:\n",
    "                if len(time_key) == 4: #ex. 2020\n",
    "                    usr_year2distances[(usr, time_key)] = curr_row\n",
    "                elif len(time_key) == 6: #ex. 2020Q1\n",
    "                    usr_quarter2distances[(usr, time_key)] = curr_row\n",
    "                elif len(time_key) == 7: #ex. 2020HY1\n",
    "                    usr_halfyear2distances[(usr, time_key)] = curr_row\n",
    "                else:\n",
    "                    sys.stderr.write('Embedding file format does not conform to expectations. Extracted time key %s for user %s.\\n' % (time_key, usr)) \n",
    "            else:\n",
    "                usr2distances[(usr)] = curr_row\n",
    "    return (usr2distances, usr_year2distances, usr_quarter2distances,usr_halfyear2distances)\n",
    "\n",
    "#########################################################################\n",
    "### Functions for Measuring Between-Person Similarities in Embeddings ###\n",
    "#########################################################################\n",
    "def pairwise_cossim(df, i_col, we_col, reference_tag='', reference=False, reference_group=None, anon_ids=None, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Calculating pairwise cosine similarities between every i-embedding to every other i-embedding, and every we-embedding to every other we-embedding\n",
    "    \"\"\"\n",
    "    col1, col2 = 'avg_i_i' + reference_tag, 'avg_we_we' + reference_tag\n",
    "    df[col1] = calculate_pairwise_cossim(df[i_col], reference=reference, reference_group=reference_group, anon_ids=anon_ids, vec_format=vec_format)\n",
    "    df[col2] = calculate_pairwise_cossim(df[we_col], reference=reference, reference_group=reference_group, anon_ids=anon_ids, vec_format=vec_format)\n",
    "    return df\n",
    "\n",
    "def sparsify(df, to_sparse, sparse):\n",
    "    assert len(to_sparse) == len(sparse)\n",
    "    for i in range(len(to_sparse)):\n",
    "        df[sparse[i]] = df[to_sparse[i]].apply(lambda x : any2sparse(x) if (x is not None and np.isfinite(x).all()) else None)\n",
    "    return df\n",
    "    \n",
    "def cossimify(df, names, vec_format='sparse'):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "    names : list\n",
    "        A list of tuples, where the first element is the first column name, second element is the second column name, and third element is the result column name\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        The original dataframe with the cosine similarity columns appended\n",
    "    \"\"\"\n",
    "    for tup in names:\n",
    "        df[tup[2]] = df.apply(lambda row : cossim_with_none(row[tup[0]], row[tup[1]], vec_format), axis=1)\n",
    "    return df\n",
    "\n",
    "#between-person similarities: individual's embeddings to the average embeddings of everyone else in the company\n",
    "def self_other_similarities(df, company_embeddings, company_cluster_embeddings, panel_data):\n",
    "    \"\"\"\n",
    "    Main workhorse function for calculating between-person similarities. Compares an individual's embeddings to\n",
    "    the average embeddings of everyone else in the company, as well as GloVe embeddings built on the entire company's corpus.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame produced by self_similarities that includes both embeddings and embedding similarities at the individual level\n",
    "    company_embeddings : 2-tuple of numpy array\n",
    "        Company embeddings of \"i\" and \"we\"\n",
    "    company_cluster_embeddings : 2-tuple of numpy array\n",
    "        Company embeddings of centroid of \"i\" words and centroid of \"we\" words\n",
    "    panel_data : bool\n",
    "        Indicates whether df is in a panel data format or a cross-sectional format. If in panel format,\n",
    "        grouping by timekey is needed before averaging.\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe that includes both within- and between-person similarities\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    sparse_company_embeddings = [any2sparse(company_embeddings[0]), any2sparse(company_embeddings[1])]\n",
    "    sparse_company_cluster_embeddings = [any2sparse(company_cluster_embeddings[0]), any2sparse(company_cluster_embeddings[1])]\n",
    "\n",
    "    # converts np.array or scipy array to a sparse vector format used by gensim\n",
    "    # any doesn't mean anything -- Python lists are not accepted by this function\n",
    "    # sparse vectors are lists whose first elements are indices and second elements are numbers\n",
    "    # the indices correspond to indices of the original vector fed into any2sparse, where zeroes were retained\n",
    "    # sparse vectors through out all the zeroes in the actual vector to save space\n",
    "    # are vector indices of the dense vector \n",
    "    df['i_sparse'] = df['i_embed'].apply(lambda x : any2sparse(x) if not isnull_wrapper(x) else None)\n",
    "    df['we_sparse'] = df['we_embed'].apply(lambda x: any2sparse(x) if not isnull_wrapper(x) else None)\n",
    "    df['i_cluster_sparse'] = df['i_cluster'].apply(lambda x : any2sparse(x) if not isnull_wrapper(x) else None)\n",
    "    df['we_cluster_sparse'] = df['we_cluster'].apply(lambda x : any2sparse(x) if not isnull_wrapper(x) else None)\n",
    "\n",
    "    if not panel_data:\n",
    "        i_mean = vector_mean(df['i_embed'])\n",
    "        df['i_embed_avg'] = df.apply(lambda x : i_mean, axis=1)\n",
    "        we_mean = vector_mean(df['we_embed'])\n",
    "        df['we_embed_avg'] = df.apply(lambda x : we_mean, axis=1)\n",
    "        i_mean = vector_mean(df['i_cluster'])\n",
    "        df['i_cluster_avg'] = df.apply(lambda x : i_mean, axis=1)\n",
    "        we_mean = vector_mean(df['we_cluster'])\n",
    "        df['we_cluster_avg'] = df.apply(lambda x : we_mean, axis=1)\n",
    "        df = pairwise_cossim(df, 'i_sparse', 'we_sparse', vec_format='sparse')\n",
    "        df = pairwise_cossim(df, 'i_cluster_sparse', 'we_cluster_sparse', reference_tag='_cluster', vec_format='sparse')\n",
    "    else:\n",
    "        #  If i_embed or we_embed is not defined for any anon_id during this period,\n",
    "        # then vector_mean will return np.nan\n",
    "        df = df.join(df['i_embed'].groupby(level=1).apply(vector_mean), rsuffix='_avg') \n",
    "        df = df.join(df['we_embed'].groupby(level=1).apply(vector_mean), rsuffix='_avg')\n",
    "        df = df.join(df['i_cluster'].groupby(level=1).apply(vector_mean), rsuffix='_avg') \n",
    "        df = df.join(df['we_cluster'].groupby(level=1).apply(vector_mean), rsuffix='_avg')\n",
    "        new_df = pd.DataFrame()\n",
    "        for time_chunk, time_df in df.groupby(level=1):\n",
    "            time_df = pairwise_cossim(time_df, 'i_sparse', 'we_sparse', vec_format='sparse')\n",
    "            time_df = pairwise_cossim(time_df, 'i_cluster_sparse', 'we_cluster_sparse', reference_tag='_cluster', vec_format='sparse')\n",
    "            new_df = new_df._append(time_df)\n",
    "        df = new_df\n",
    "    # Filtering out Nones and np.nan (who has class float)\n",
    "    df = sparsify(df, ['i_embed_avg', 'we_embed_avg', 'i_cluster_avg', 'we_cluster_avg'],\n",
    "        ['i_avg_sparse', 'we_avg_sparse', 'i_cluster_avg_sparse', 'we_cluster_avg_sparse'])\n",
    "    \n",
    "    # These averages are already defined based on time periods when we are running using panel data due to the inherent structure of the data\n",
    "    df = cossimify(df,\n",
    "        [('i_sparse', 'i_avg_sparse', 'i_avg_i'), ('we_sparse', 'we_avg_sparse', 'we_avg_we'),\n",
    "        ('i_cluster_sparse', 'i_cluster_avg_sparse', 'i_avg_i_cluster'), ('we_cluster_sparse', 'we_cluster_avg_sparse', 'we_avg_we_cluster'),\n",
    "        ('i_sparse', 'we_avg_sparse', 'i_avg_we'), ('i_cluster_sparse', 'we_cluster_avg_sparse', 'i_avg_we_cluster')], vec_format='sparse')\n",
    "\n",
    "    df['i_company_i'] = df.apply(lambda row : cossim_with_none(row['i_sparse'], sparse_company_embeddings[0], vec_format='sparse'), axis=1) \n",
    "    df['we_company_we'] = df.apply(lambda row : cossim_with_none(row['we_sparse'], sparse_company_embeddings[1], vec_format='sparse'), axis=1) \n",
    "    df['i_company_i_cluster'] = df.apply(lambda row : cossim_with_none(row['i_cluster_sparse'], sparse_company_cluster_embeddings[0], vec_format='sparse'), axis=1) \n",
    "    df['we_company_we_cluster'] = df.apply(lambda row : cossim_with_none(row['we_cluster_sparse'], sparse_company_cluster_embeddings[1], vec_format='sparse'), axis=1) \n",
    "    df['i_company_we'] = df.apply(lambda row : cossim_with_none(row['i_sparse'], sparse_company_embeddings[1], vec_format='sparse'), axis=1) \n",
    "    df['i_company_we_cluster'] = df.apply(lambda row : cossim_with_none(row['i_cluster_sparse'], sparse_company_cluster_embeddings[1], vec_format='sparse'), axis=1) \n",
    "    return df.round(5)\n",
    "\n",
    "def compare_internal_external(df):\n",
    "    df = cossimify(df,\n",
    "        [('i_sparse_internal', 'we_sparse_external', 'i_int_we_ext'),\n",
    "        ('i_cluster_sparse_internal', 'we_cluster_sparse_external', 'i_int_we_ext_cluster'),\n",
    "        ('i_sparse_internal', 'we_avg_sparse_external', 'i_int_we_avg_ext'),\n",
    "        ('i_cluster_sparse_internal', 'we_cluster_avg_sparse_external', 'i_int_we_avg_ext_cluster')], vec_format='sparse')\n",
    "\n",
    "    for post in ['_internal', '_external']:\n",
    "        cols = ['i_embed', 'we_embed', 'i_sparse', 'we_sparse', 'i_embed_avg', 'we_embed_avg', 'i_avg_sparse', 'we_avg_sparse',\n",
    "        'i_cluster', 'we_cluster', 'i_cluster_sparse', 'we_cluster_sparse', 'i_cluster_avg', 'we_cluster_avg', 'i_cluster_avg_sparse', 'we_cluster_avg_sparse']\n",
    "        cols = [c+post for c in cols]\n",
    "        df.drop(cols, axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "#most important function to read the finally generated embeddings\n",
    "def reading_embeddings(embeddings_dir, company_embeddings, company_cluster_embeddings, test_mode=False):\n",
    "    \"\"\"\n",
    "    Calculates embedding similarities within-person and between-person\n",
    "    Parameters\n",
    "    ----------\n",
    "    embeddings_dir : str\n",
    "        Directory where all embedding files exist\n",
    "    company_embeddings : tuple of numpy array\n",
    "        Embeddings of \"i\" and \"we\" in the whole company email corpus\n",
    "    company_cluster_embeddings : tuple of numpy array\n",
    "        Embeddings of average of all \"i\" words and average of all \"we\" words in the whole company email corpus\n",
    "    test_mode : bool, optional\n",
    "        If testing, reduce number of files to process\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        User, annual, and quarter level dataframes that include both within- and between-person embedding similarities\n",
    "    \"\"\"\n",
    "    #note: embeddings_dir = os.path.join(mittens_dir, \"embeddings_high_prob_eng_{}_{}d_mincount{}\".format(str(ling_thres).replace(\".\", \"\"), embedding_dim, mincount))\n",
    "    #thus, embeddings_dir is the embeddings file itself which is in glove embeddings  format \n",
    "    all_files = os.listdir(embeddings_dir) #specifies list of files in the directory\n",
    "    if test_mode: all_files = [all_files[random.randint(0, len(all_files)-1)] for _ in range(len(all_files)//50)]\n",
    "\n",
    "    internal_re = re.compile(\".+_internal.txt\")\n",
    "    external_re = re.compile(\".+_external.txt\")\n",
    "\n",
    "#create a list of internal and external email exchanges files\n",
    "    internal_files, external_files = [], []\n",
    "    for f in all_files:\n",
    "        if re.match(internal_re, f):\n",
    "            internal_files.append(f)\n",
    "        elif re.match(external_re, f):\n",
    "            external_files.append(f)\n",
    "    print(len(internal_files))\n",
    "    print(len(external_files))\n",
    "    result_dfs = []\n",
    "    #when enumerating over two lists, the value of i, and files first refers to internal_files, then in the next iteration refers to extrenal_files\n",
    "    for i, files in enumerate([internal_files, external_files], 1): #once for internal files, once for external files\n",
    "    #for i, files in enumerate([internal_files], 1):   #for only internal files\n",
    "        num_files = len(files)\n",
    "        sys.stderr.write('Iteration %d: Calculate within-person similarities for %d files at %s.\\n' % (i, num_files, str(datetime.now())))\n",
    "        usr2distances, usr_year2distances, usr_quarter2distances, usr_halfyear2distances = self_similarities(files, num_files, embeddings_dir)\n",
    "        print('reached here')\n",
    "        cols = ['i_embed', 'we_embed', 'i_we', 'me_us', 'my_our', 'mine_ours', 'myself_ourselves', 'i_cluster', 'we_cluster', 'i_we_cluster', 'i_we_symmetric']\n",
    "        usr2distances_df = dict_to_df(usr2distances, cols, index_name=['user_id'])\n",
    "        usr_year2distances_df = dict_to_df(usr_year2distances, cols, index_name=['user_id', year_colname])\n",
    "        usr_quarter2distances_df = dict_to_df(usr_quarter2distances, cols, index_name=['user_id', quarter_colname])\n",
    "        usr_halfyear2distances_df = dict_to_df(usr_halfyear2distances, cols, index_name=['user_id', halfyear_colname])\n",
    "\n",
    "        sys.stderr.write('Iteration %d: Calculate between-person similarities for %d files at %s.\\n' % (i, num_files, str(datetime.now())))\n",
    "        pool = multiprocessing.Pool(processes = num_cores)\n",
    "        results = ([pool.apply_async(self_other_similarities, args=(df, company_embeddings, company_cluster_embeddings, panel,))\n",
    "            for df, panel in [(usr2distances_df, False), (usr_year2distances_df, True), (usr_quarter2distances_df, True), (usr_halfyear2distances_df, True)]])\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        result_dfs.append([r.get() for r in results])\n",
    "        sys.stderr.write('Iteration %d: Successfully read and computed cosine similarities for %d embedding files at %s.\\n' % (i, num_files, str(datetime.now())))  \n",
    "    \n",
    "    print('dimensions of result------')\n",
    "    if isinstance(result_dfs, list):\n",
    "        if all(isinstance(sublist, list) for sublist in result_dfs):\n",
    "          # 2D array\n",
    "          rows = len(result_dfs)\n",
    "          cols = len(result_dfs[0])\n",
    "          print(\"Dimensions: {} rows, {} columns\".format(rows, cols))\n",
    "        else:\n",
    "          # 1D array\n",
    "          print(\"Dimensions: 1D array with {} elements\".format(len(result_dfs)))\n",
    "    else:\n",
    "        print(\"Input is not a list.\")\n",
    "    \n",
    "    #if the first for statement including both internal and external email's are considered\n",
    "    usr_df = result_dfs[0][0].join(result_dfs[1][0], lsuffix='_internal', rsuffix='_external', how='outer')\n",
    "    usr_year_df = result_dfs[0][1].join(result_dfs[1][1], lsuffix='_internal', rsuffix='_external', how='outer')\n",
    "    usr_quarter_df = result_dfs[0][2].join(result_dfs[1][2], lsuffix='_internal', rsuffix='_external', how='outer')\n",
    "    usr_halfyear_df = result_dfs[0][3].join(result_dfs[1][3], lsuffix='_internal', rsuffix='_external', how='outer')\n",
    "\n",
    "    '''\n",
    "    #if only internal emails are considered\n",
    "    usr_df = result_dfs[0][0]\n",
    "    usr_year_df = result_dfs[0][1]\n",
    "    usr_quarter_df = result_dfs[0][2]\n",
    "    usr_halfyear_df = result_dfs[0][3]\n",
    "    '''\n",
    "\n",
    "    #comparing internal and external emails\n",
    "    usr_df = compare_internal_external(usr_df)\n",
    "    usr_year_df = compare_internal_external(usr_year_df)\n",
    "    usr_quarter_df = compare_internal_external(usr_quarter_df)\n",
    "    usr_halfyear_df = compare_internal_external(usr_halfyear_df)\n",
    "    return (usr_df, usr_year_df, usr_quarter_df, usr_halfyear_df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starttime = datetime.now()\n",
    "    test = False\n",
    "    try:\n",
    "        test = sys.argv[1].lower() == 'test'\n",
    "    except IndexError as error:\n",
    "        pass\n",
    "    if test:\n",
    "        user_output_filename = os.path.join(output_dir, \"test_embeddings_users.csv\")\n",
    "        annual_output_filename = os.path.join(output_dir, \"test_embeddings_annual.csv\")\n",
    "        quarterly_output_filename = os.path.join(output_dir, \"test_embeddings_quarterly.csv\")\n",
    "    for d in [output_dir, tmp_dir]:\n",
    "        if not os.path.exists(d):\n",
    "            os.mkdir(d)\n",
    "\n",
    "    #note the start time\n",
    "    sys.stderr.write('Loading corpus counts at %s.\\n' % datetime.now())\n",
    "    #create three dataframes (now four because including halfyearly counts) that contain the two control variable values for each user: token (word) count and email count\n",
    "    usr2counts, usr2annual_counts, usr2quarterly_counts, usr2halfyearly_counts = read_raw_counts(activity_file, email_file) \n",
    "    #read the default embeds by calling a function from utils.py. How are the default vallues of the embddings set?\n",
    "    sys.stderr.write('Reading embeddings at %s.\\n' % datetime.now())\n",
    "    company_embeddings = extract_company_embedding(company_embeddings_filename, tmp_dir, hash_pronouns) #loads the company embeddings are converts them to the appropriate format and stores them in the tmp_dir location\n",
    "    #call utils.py to create the initial i-words mean embedding and the initial we-word embedding and store these two in a list\n",
    "    company_cluster_embeddings = (vector_mean(pd.Series(company_embeddings[i_index:we_index])), vector_mean(pd.Series(company_embeddings[we_index:])))\n",
    "    #the read embeddings function is the main function\n",
    "    usr2measures, usr2annual_measures, usr2quarterly_measures, usr2halfyearly_measures = reading_embeddings(embeddings_dir, company_embeddings, company_cluster_embeddings, test)\n",
    "    \n",
    "    # different embedding files should be matched with the same hr file as hr data is not in panel format\n",
    "    sys.stderr.write('Reading HR and Survey data at %s.\\n' % datetime.now())\n",
    "    hr_df = extract_hr_survey_df(survey_filename, user_qualtrics_file, users_file, perf_likert, perf_percentage)\n",
    "\n",
    "    # could just merge and write to csv without calling another function\n",
    "    sys.stderr.write('Outputting dataframe at %s.\\n' % datetime.now())\n",
    "    if usr2measures is not None: hr_df.join(usr2measures).join(usr2counts).to_csv(user_output_filename)\n",
    "    if usr2annual_measures is not None: hr_df.join(usr2annual_measures).join(usr2annual_counts).to_csv(annual_output_filename)\n",
    "    if usr2quarterly_measures is not None: hr_df.join(usr2quarterly_measures).join(usr2quarterly_counts).to_csv(quarterly_output_filename)\n",
    "    if usr2halfyearly_measures is not None: hr_df.join(usr2halfyearly_measures).join(usr2halfyearly_counts).to_csv(halfyearly_output_filename)\n",
    "    \n",
    "    sys.stderr.write(\"Finished outputting measures at %s, with a duration of %s.\\n\"\n",
    "        % (str(datetime.now()), str(datetime.now() - starttime)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
